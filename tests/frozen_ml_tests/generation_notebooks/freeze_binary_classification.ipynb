{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Binary Classifier\n",
                "This notebook builds a binary classification model and performs data, performance, fairness, security, and privacy assessments for it.\n",
                "\n",
                "The model predicts whether a loan applicant is unqualified or qualified based on their income, credit, etc.\n",
                "\n",
                "Dataset preparation notebook available [here](https://github.com/credo-ai/customer_demos/blob/prod/prod/d3_loan_approval/data_preparation.ipynb)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Loading Libraries\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "import pandas as pd\n",
                "\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import GradientBoostingClassifier\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "import pickle"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "sensitive_feature_keys = ['Gender', 'Race']\n",
                "label_key = 'loan_approved'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_pickle('../frozen_data/loan_binary/loan_processed.pkl')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "sf = df[sensitive_feature_keys]\n",
                "target = df[label_key]\n",
                "features = df.drop(sensitive_feature_keys + [label_key], axis=1)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train, X_test, y_train, y_test, sf_train, sf_test = train_test_split(\n",
                "    features, target, sf, random_state=0, test_size=0.3\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=0)</pre></div></div></div></div></div>"
                        ],
                        "text/plain": [
                            "LogisticRegression(random_state=0)"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "clf = LogisticRegression(random_state=0)\n",
                "clf.fit(X_train, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Accuracy on training set: 0.81\n",
                        "Accuracy on test set: 0.82\n"
                    ]
                }
            ],
            "source": [
                "print('Accuracy on training set: {:.2f}'.format(clf.score(X_train, y_train)))\n",
                "print('Accuracy on test set: {:.2f}'.format(clf.score(X_test, y_test)))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_training = pd.concat([sf_train, X_train, y_train], axis=1)\n",
                "df_validation = pd.concat([sf_test, X_test, y_test], axis=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "from credoai.lens import Lens\n",
                "from credoai.artifacts import TabularData, ClassificationModel\n",
                "from credoai.evaluators import *\n",
                "from credoai.governance import Governance\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "credo_model = ClassificationModel(\n",
                "    'loan_default_classifier',\n",
                "    clf\n",
                ")\n",
                "test_data = TabularData(\n",
                "    name='loan_val',\n",
                "    X=X_test,\n",
                "    y=y_test,\n",
                "    sensitive_features=sf_test\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": [
                "validation_data = {\n",
                "    'name': 'loan_val', \n",
                "    'val_features': X_test, \n",
                "    'val_labels': y_test, \n",
                "    'label_name': 'loan_approved', \n",
                "    'sensitive_features': sf_test\n",
                "}\n",
                "\n",
                "with open('../frozen_data/loan_binary/loan_validation.pkl', 'wb') as f:\n",
                "    pickle.dump(validation_data, f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": [
                "# pipeline scan be specifed using a sklearn-like style\n",
                "metrics = [\"false_negative_rate\", \"average_precision_score\"]\n",
                "pipeline = [\n",
                "    (Performance(metrics), 'Performance Assessment'),\n",
                "    (ModelFairness(metrics), \"ModelFairness Assessment\"),\n",
                "]\n",
                "\n",
                "pipeline_info = {'metrics': metrics, 'assessments': ['Performance', 'ModelFairness']}\n",
                "\n",
                "with open('../frozen_results/pipeline_info.pkl', 'wb') as f:\n",
                "    pickle.dump(pipeline_info, f)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2022-10-20 12:29:24,522 - lens - INFO - Evaluator Performance added to pipeline. \n",
                        "2022-10-20 12:29:24,528 - lens - INFO - Evaluator ModelFairness added to pipeline. Dataset used: assessment_data. Sensitive feature: Gender\n",
                        "2022-10-20 12:29:24,534 - lens - INFO - Evaluator ModelFairness added to pipeline. Dataset used: assessment_data. Sensitive feature: Race\n",
                        "2022-10-20 12:29:24,535 - lens - INFO - Running evaluation for step: Performance Assessment\n",
                        "2022-10-20 12:29:24,536 - lens - INFO - Running evaluation for step: ModelFairness Assessment\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "<credoai.lens.lens.Lens at 0x28d5e7a60>"
                        ]
                    },
                    "execution_count": 31,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "lens = Lens(\n",
                "    model=credo_model,\n",
                "    assessment_data=test_data,\n",
                "    pipeline=pipeline\n",
                ")\n",
                "\n",
                "lens.run()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": [
                "results = lens.get_results()\n",
                "with open('../frozen_results/binary_clf_results.pkl', 'wb') as f:\n",
                "    pickle.dump(results, f)\n",
                "# results['Performance Assessment'][0].to_pickle('../frozen_results/loan_perf.pkl')\n",
                "# results['Fairness Assessment'][0].to_pickle('../frozen_results/loan_parity.pkl')\n",
                "# results['Fairness Assessment'][1].to_pickle('../frozen_results/loan_disag.pkl')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>Race</th>\n",
                            "      <th>type</th>\n",
                            "      <th>value</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>Black</td>\n",
                            "      <td>false_negative_rate</td>\n",
                            "      <td>0.039216</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>NHPI</td>\n",
                            "      <td>false_negative_rate</td>\n",
                            "      <td>0.032258</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>White</td>\n",
                            "      <td>false_negative_rate</td>\n",
                            "      <td>0.033333</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>Black</td>\n",
                            "      <td>average_precision_score</td>\n",
                            "      <td>0.860209</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>NHPI</td>\n",
                            "      <td>average_precision_score</td>\n",
                            "      <td>0.803702</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>5</th>\n",
                            "      <td>White</td>\n",
                            "      <td>average_precision_score</td>\n",
                            "      <td>0.869738</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "    Race                     type     value\n",
                            "0  Black      false_negative_rate  0.039216\n",
                            "1   NHPI      false_negative_rate  0.032258\n",
                            "2  White      false_negative_rate  0.033333\n",
                            "3  Black  average_precision_score  0.860209\n",
                            "4   NHPI  average_precision_score  0.803702\n",
                            "5  White  average_precision_score  0.869738"
                        ]
                    },
                    "execution_count": 37,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "results['ModelFairness Assessment'][1]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10.6 ('dev_gini')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.6"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "975a088070882147f8de9db5a531bf41cdbf56f68ebdbe6c3f86024c84ce6dc0"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
