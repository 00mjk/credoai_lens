{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Binary Classifier\n",
                "This notebook builds a binary classification model and performs performance and fairness assessments for it.\n",
                "\n",
                "The model predicts whether a loan applicant is unqualified or qualified based on their income, credit, etc.\n",
                "\n",
                "Dataset preparation notebook available [here](https://github.com/credo-ai/customer_demos/blob/prod/prod/d3_loan_approval/data_preparation.ipynb)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Loading Libraries\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "import pandas as pd\n",
                "\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "import pickle"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "sensitive_feature_keys = ['Gender', 'Race']\n",
                "label_key = 'loan_approved'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_pickle('../frozen_data/binary/loan_processed.pkl')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "sf = df[sensitive_feature_keys]\n",
                "target = df[label_key]\n",
                "features = df.drop(sensitive_feature_keys + [label_key], axis=1)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train, X_test, y_train, y_test, sf_train, sf_test = train_test_split(\n",
                "    features, target, sf, random_state=0, test_size=0.3\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=0)</pre></div></div></div></div></div>"
                        ],
                        "text/plain": [
                            "LogisticRegression(random_state=0)"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "clf = LogisticRegression(random_state=0)\n",
                "clf.fit(X_train, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_training = pd.concat([sf_train, X_train, y_train], axis=1)\n",
                "df_validation = pd.concat([sf_test, X_test, y_test], axis=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "from credoai.lens import Lens\n",
                "from credoai.artifacts import TabularData, ClassificationModel\n",
                "from credoai.evaluators import *\n",
                "from credoai.governance import Governance\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "credo_model = ClassificationModel(\n",
                "    'loan_default_classifier',\n",
                "    clf\n",
                ")\n",
                "train_data = TabularData(\n",
                "    name='blarg',\n",
                "    X=X_train,\n",
                "    y=y_train,\n",
                "    sensitive_features=sf_train\n",
                ")\n",
                "test_data = TabularData(\n",
                "    name='loan_val',\n",
                "    X=X_test,\n",
                "    y=y_test,\n",
                "    sensitive_features=sf_test\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "validation_data_dict = {\n",
                "    'name': 'loan_val', \n",
                "    'val_features': X_test, \n",
                "    'val_labels': y_test, \n",
                "    'label_name': 'loan_approved', \n",
                "    'sensitive_features': sf_test\n",
                "}\n",
                "\n",
                "with open('../frozen_data/binary/loan_validation.pkl', 'wb') as f:\n",
                "    pickle.dump(validation_data_dict, f)\n",
                "\n",
                "train_data_dict = {\n",
                "    'name': 'loan_train',\n",
                "    'train_features': X_train,\n",
                "    'train_labels': y_train,\n",
                "    'label_name': 'loan_approved',\n",
                "    'sensitive_features': sf_train\n",
                "}\n",
                "\n",
                "with open('../frozen_data/binary/loan_train.pkl', 'wb') as f:\n",
                "    pickle.dump(train_data_dict, f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "# pipeline scan be specifed using a sklearn-like style\n",
                "metrics = [\"false_negative_rate\", \"average_precision_score\"]\n",
                "pipeline = [\n",
                "    (Performance(metrics), 'Performance'),\n",
                "    (ModelFairness(metrics), 'ModelFairness'),\n",
                "]\n",
                "\n",
                "pipeline_info = {'metrics': metrics, 'assessments': ['Performance', 'ModelFairness']}\n",
                "\n",
                "with open('../frozen_results/binary/pipeline_info.pkl', 'wb') as f:\n",
                "    pickle.dump(pipeline_info, f)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2022-10-24 15:26:51,761 - lens - INFO - Evaluator Performance added to pipeline. \n",
                        "2022-10-24 15:26:51,766 - lens - INFO - Evaluator ModelFairness added to pipeline. Dataset used: assessment_data. Sensitive feature: Gender\n",
                        "2022-10-24 15:26:51,771 - lens - INFO - Evaluator ModelFairness added to pipeline. Dataset used: assessment_data. Sensitive feature: Race\n",
                        "2022-10-24 15:26:51,771 - lens - INFO - Running evaluation for step: Performance\n",
                        "2022-10-24 15:26:51,773 - lens - INFO - Running evaluation for step: ModelFairness\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "<credoai.lens.lens.Lens at 0x14ba3e470>"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "lens = Lens(\n",
                "    model=credo_model,\n",
                "    # training_data=train_data,\n",
                "    assessment_data=test_data,\n",
                "    pipeline=pipeline\n",
                ")\n",
                "\n",
                "# lens.add(ModelFairness(metrics), \"ModelFairness\")\n",
                "\n",
                "lens.run()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'Performance': [                      type     value\n",
                            "  0      false_negative_rate  0.034351\n",
                            "  1  average_precision_score  0.859587],\n",
                            " 'ModelFairness': [                             type     value\n",
                            "  0      false_negative_rate_parity  0.006958\n",
                            "  1  average_precision_score_parity  0.066035,\n",
                            "      Race                     type     value\n",
                            "  0  Black      false_negative_rate  0.039216\n",
                            "  1   NHPI      false_negative_rate  0.032258\n",
                            "  2  White      false_negative_rate  0.033333\n",
                            "  3  Black  average_precision_score  0.860209\n",
                            "  4   NHPI  average_precision_score  0.803702\n",
                            "  5  White  average_precision_score  0.869738]}"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "lens.get_results()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "results = lens.get_results()\n",
                "for assessment, result in results.items():\n",
                "    with open('../frozen_results/binary/binary_clf_' + assessment + '_results.pkl', 'wb') as f:\n",
                "        pickle.dump(result, f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[                      type     value\n",
                        "0      false_negative_rate  0.034351\n",
                        "1  average_precision_score  0.859587]\n",
                        "[                             type     value\n",
                        "0      false_negative_rate_parity  0.006958\n",
                        "1  average_precision_score_parity  0.066035,     Race                     type     value\n",
                        "0  Black      false_negative_rate  0.039216\n",
                        "1   NHPI      false_negative_rate  0.032258\n",
                        "2  White      false_negative_rate  0.033333\n",
                        "3  Black  average_precision_score  0.860209\n",
                        "4   NHPI  average_precision_score  0.803702\n",
                        "5  White  average_precision_score  0.869738]\n"
                    ]
                }
            ],
            "source": [
                "for assessment, result in results.items():\n",
                "    print(result)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[                             type     value\n",
                            " 0      false_negative_rate_parity  0.006958\n",
                            " 1  average_precision_score_parity  0.066035,\n",
                            "     Race                     type     value\n",
                            " 0  Black      false_negative_rate  0.039216\n",
                            " 1   NHPI      false_negative_rate  0.032258\n",
                            " 2  White      false_negative_rate  0.033333\n",
                            " 3  Black  average_precision_score  0.860209\n",
                            " 4   NHPI  average_precision_score  0.803702\n",
                            " 5  White  average_precision_score  0.869738]"
                        ]
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "results['ModelFairness']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10.6 ('dev_gini')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.6"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "975a088070882147f8de9db5a531bf41cdbf56f68ebdbe6c3f86024c84ce6dc0"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
