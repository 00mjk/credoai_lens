{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b97cd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Connecting with the Credo AI Governance Platform\n",
    "\n",
    "## Overview\n",
    "Connecting with the Credo AI Governance Platform is straightforward using Lens. After following this tutorial you'll be up and running!\n",
    "\n",
    "### Find the code\n",
    "This notebook can be found on [github](https://github.com/credo-ai/credoai_lens/blob/develop/docs/notebooks/governance_integration.ipynb).\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "1. [Setup](#Setup)\n",
    "1. [Using Lens](#Using-Lens)\n",
    "    1. [Exporting to Credo AI's Governance Platform from Lens](#Exporting-to-Credo-AI's-Governance-Platform-from-Lens)\n",
    "    2. [CredoGovernance](#Credo-Governance-Artifact)\n",
    "    3. [Registering a model while assessing it](#Registering-a-model-while-assessing-it)\n",
    "1. [Air Gap Environment](#Air-Gap-Environment)\n",
    "    1. [Getting the assessment spec](#Getting-the-assessment-spec) \n",
    "    2. [Exporting Results](#Exporting-Results)\n",
    "1. [Explicitly registering model and data artifacts](#Explicitly-registering-model-and-data-artifacts)\n",
    "    1. [Registering a Project and Model](#Registering-a-Project-and-Model)\n",
    "\n",
    "### Config File\n",
    "First, ensure you have a config file somewhere on your computer named \".credoconfig\". The default location where Lens will look for it in your home directory (`~/.credoconfig`). The structure of the config should look like this\n",
    "\n",
    "```\n",
    "TENANT=<tenant name> # Example: credoai\n",
    "API_KEY=<your api key> # Example: JSMmd26...\n",
    "```\n",
    "\n",
    "This config gives Lens API access to the Governance Platform.\n",
    "\n",
    "### Connecting to a particular model, dataset, or Use Case\n",
    "\n",
    "Lens connects to the Governance Platform via a `CredoGovernance` object. This specifies the IDs of the Use Case, model and data you want to associate your assessments with. The IDs are found in the url of your Use Case or model.\n",
    "\n",
    "For instance, the Use Case ID is found here: `...credo.com/use-cases/{use_case_id}`\n",
    "\n",
    "And the model Id is found here: `...credo.com/models/{model_id}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8f0aca",
   "metadata": {},
   "source": [
    "## Train your Model\n",
    "\n",
    "Some quick setup... This is all of your datascience work before assessment and integration with Credo AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a704197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for example data and model training\n",
    "from credoai.data import fetch_creditdefault\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Base Lens imports\n",
    "import credoai.lens as cl\n",
    "\n",
    "# set default format for image displays. Change to 'png' if 'svg' is failing\n",
    "%config InlineBackend.figure_formats = ['svg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc88b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_creditdefault()\n",
    "X = data['data'].drop(columns=['SEX'])\n",
    "y = data['target']\n",
    "sensitive_feature = data['data']['SEX']\n",
    "\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cf87db",
   "metadata": {},
   "source": [
    "## Using Lens\n",
    "\n",
    "This is going to be familiar if you've gone through our quickstart tutorial! The only addition is the use of a `CredoGovernance` object.\n",
    "\n",
    "### Credo Governance Artifact\n",
    "The CredoGovernance object has a few functionalities.\n",
    "\n",
    "* Retrieving an alignment spec from the Use Case (created during the aligned process). The alignment spec is either downloaded straight from the Governance Platform, or supplied as a json file for air gap deployment. This alignment spec can be supplemented (or overwritten) with an alignment spec you define in code and pass to Lens.\n",
    "* Registering models and projects to the Governance Platform.\n",
    "* Encoding the IDs of governance objects so Lens can easily export metrics and reports.\n",
    "\n",
    "\\** **Attention** \\**\n",
    "\n",
    "The metrics and reports will _either_ be exported to the model and data specified in by `CredoGovernance` (using the `model_id` and `data_id` arguments) OR a new model and data will be registered on your CredoGovernance platform automatically, which will then be used for export. You can also set the ids using the model/dataset _name_ by calling `credo_governance.set_governance_info_by_name`.\n",
    "\n",
    "With that said, let's see how this works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7af9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "credo_model = cl.CredoModel(name='credit_default_classifier',\n",
    "                            model=model)\n",
    "\n",
    "credo_data = cl.CredoData(name='UCI-credit-default',\n",
    "                          X=X, \n",
    "                          y=y.astype(int),\n",
    "                          sensitive_features=sensitive_feature)\n",
    "\n",
    "# New artifact for governance\n",
    "credo_governance = cl.CredoGovernance(use_case_id=\"your-use-case-id\",\n",
    "                                      model_id=\"your-model-id\"\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eccda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the metrics that will be used by the FairnessBase assessment. \n",
    "# This spec will supplement (or overide!) whatever spec is \n",
    "# pulled down from the Governance Platform\n",
    "alignment_spec = {\n",
    "    'FairnessBase': {'metrics': ['precision_score', 'recall_score']}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a60052d",
   "metadata": {},
   "source": [
    "### Interacting with Credo AI's Governance Platform from Lens\n",
    "\n",
    "In addition to the model and data, we can pass the CredoGovernance object. This is how Lens knows which Use Case, model and/or data to interacts with. If you are running assessments on multiple models, you'll create a separate `CredoGovernance` object (and separate Lens instances) for each.\n",
    "\n",
    "That's it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24830c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.assessment import FairnessBaseAssessment\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               governance=credo_governance,\n",
    "               assessments = [FairnessBaseAssessment()],\n",
    "               data=credo_data,\n",
    "               spec=alignment_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8496e7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just add the export option and the metrics will be sent to the Governance Platform.\n",
    "lens.run_assessments(export=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505351ae-5a65-43b7-bbcd-c64a4bbe7e83",
   "metadata": {},
   "source": [
    "#### Reports\n",
    "\n",
    "Reporting is a critical feature of Lens. Your assessments must be packaged in an easily digestible way to support downstream governance. As such, Lens supports robust reporting functionality.\n",
    "\n",
    "Lens creates a jupyter notebook report that collates all of your assessment results into one place. It then exports converts that notebook to HTML and sends it to Credo AI's Governance Platform. To export reports you must supply Lens a CredoGovernance object with a use_case_id. The report will be associated with that use-case, as well as a model (either the model you supplied to CredoGovernance, or the model [registered](#Registering-a-model-while-assessing-it) by Lens).\n",
    "\n",
    "You can also display the reports within your active notebook or locally save the report by passing different arguments to `create_report`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cec3526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reports can be exported too!\n",
    "lens.create_reports(\"My Report\", export=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434b6551",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Registering a model while assessing it\n",
    "\n",
    "**No Governance Info? No Problem**\n",
    "\n",
    "Sometimes you may not want to create a `CredoGovernance` object before using Lens. There may not be an `assessment spec` that you have to pull down for instance. Or you may not have a model registered yet in the Governance Platform to receive your assessments.\n",
    "\n",
    "If you still want to log a model assessment, Lens will take care of registering a new model and project on Credo AI's Governance Platform first. The below will create a project called `an example model project` and log the model `an example model ` under it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6744db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.assessment import FairnessBaseAssessment\n",
    "credo_model = cl.CredoModel(name='an example model',\n",
    "                            model=model)\n",
    "\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               assessments = [FairnessBaseAssessment()],\n",
    "               data=credo_data,\n",
    "               spec=alignment_spec)\n",
    "lens.run_assessments(export=True)\n",
    "\n",
    "# Note! The below doesn't do anything unless you supply a use-case id\n",
    "# A warning will be raised to iifnorm you of this.\n",
    "lens.create_reports('Full Report', export=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13de77a",
   "metadata": {},
   "source": [
    "The governance object is created for you, and can be interrogated or used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8890971",
   "metadata": {},
   "outputs": [],
   "source": [
    "gov = lens.get_governance()\n",
    "gov.get_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936b6b7d",
   "metadata": {},
   "source": [
    "\\** **Attention** \\**\n",
    "\n",
    "\n",
    "Note that if you run the above a second time, the model doesn't have to be registered again - it already was registered! Instead, Lens will find the ID for model name \"an example model\" and use that (this is because model names must be unique, and are only associated with one ID). If you would like to run Lens again for the same model, but store the data somewhere else you will have to either change the name of the model or explicitly supply a different model_id."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633a18ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Air Gap Environment\n",
    "\n",
    "If you cannot interact with Credo AI's Governance Platfrom via the internet, this section is for you!\n",
    "\n",
    "Instead of Lens automating the communication between your assessment environment and governance, you will instead have to download assets and upload them. \n",
    "\n",
    "### Getting the assessment spec\n",
    "The asset that you may have to get _from_ the governance platform is the assessment spec. This can be found under the \"Technical Requirements\" of your Use Case. Once you download the assessments spec, you can read it with a CredoGovernance object:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a5ac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "gov = cl.CredoGovernance()\n",
    "# you must explicitly retrieve the assessment spec \n",
    "# in an air gapped environment!\n",
    "gov.retrieve_assessment_spec('{path to assessment spec}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26781cc",
   "metadata": {},
   "source": [
    "Following that, you can pass the governance object to Lens as normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f1b3bc",
   "metadata": {},
   "source": [
    "### Exporting Results\n",
    "\n",
    "Lens can export assessment results to file as a json object. This is generally useful if you want to store your results locally, but particularly useful if you are in an air gapped environment where you cannot access Credo AI's Governance Platform directly from your development environment.\n",
    "\n",
    "Doing this only requires a one line change in our current flow. We change the `export` argument from `True` to a local path:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aa0097",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens.run_assessments(export='{path where assessments should be exported}')\n",
    "\n",
    "# reports are exported to file in an analagous way.\n",
    "lens.create_reports(export='{path where assessments should be exported}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad34f162",
   "metadata": {},
   "source": [
    "Running the above will create a folder that you specified, if it doesn't exist. \n",
    "\n",
    "The `run_assessments` line will save each assessment's output in that folder as a separate json file. This json file can be uploaded by going to the **Asset Store** of your Model on the Governance Platform, pressing the purple \"+\" button under \"Metrics\" and uploading the json. Reports should be uploaded under \"Chart\".\n",
    "\n",
    "The `create_reports` line will save each report as an html file, which can be uploaded directly to the **Use-Case**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6dbbc5-4caa-409c-9c54-c12278a3e331",
   "metadata": {},
   "source": [
    "## Exporting Reports\n",
    "\n",
    "Reports can also be exported to file. To do so, pass a path to the `export` argument of `create_notebooks`. This will save a \".ipynb\" notebook. To upload to Credo AI's Governance Platform this notebook needs to be converted to an html file. To do so make use of the [nbconvert](https://nbconvert.readthedocs.io/en/latest/usage.html) utility:\n",
    "\n",
    "` jupyter nbconvert --to html notebook.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec03249-b890-48dd-b837-bce9ef22715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_directory = os.path.join(os.path.expanduser('~'), 'credoai_test_location')\n",
    "lens.create_reports('Full Report', export=output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699e2259",
   "metadata": {},
   "source": [
    "## Explicitly registering model and data artifacts\n",
    "\n",
    "While Lens will take care of registering a model and data for you if no `model_id` or `data_id` is provided, you can also manually register models yourself using a `CredoGovernance` object.\n",
    "\n",
    "This is helpful if you want to run Lens multiple times, or want to have more control over the names of your project and/or model. \n",
    "* First, you create a CredoGovernance object\n",
    "* Second register your project and model\n",
    "* Third, pass to Lens as normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e66846",
   "metadata": {},
   "source": [
    "### Registering a Project and Model\n",
    "\n",
    "**Models** are organized under **Projects**. If you register a Model, a Project will be automatically created to house the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01d3ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gov = cl.CredoGovernance()\n",
    "gov.register_model(model_name='my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf3dbba",
   "metadata": {},
   "source": [
    "If a Project already exists that you'd like to use (registering a model _to_ that project), supply the Project ID to the `CredoGovernance` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb1141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gov = cl.CredoGovernance(model_project_id = 'existing project')\n",
    "gov.register_model(model_name='my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e5ad94",
   "metadata": {},
   "source": [
    "If you want to register a new project and control its name, register the Project explicitly before registering the Model.\n",
    "\n",
    "Note - if the model already exists, and is associated with a project, the project will be registered without a model! (The model already is registered to another project)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bb3061",
   "metadata": {},
   "outputs": [],
   "source": [
    "gov = cl.CredoGovernance()\n",
    "gov.register_project('my_project')\n",
    "gov.register_model(model_name='my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a258d4f7-d9a1-40cf-8c69-89cac57db8dd",
   "metadata": {},
   "source": [
    "**Datasets** can be registered analogously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fd9d2e-36ad-4bb5-9884-7718af56355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gov.register_dataset('my_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4ac4f8-5f4b-438d-bc59-2173a0527b42",
   "metadata": {},
   "source": [
    "**Use Cases** can also be used. If CredoGovernance has a Use Case ID, the model will automatically be associated with that Use Case ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac34ddf5-e53e-4082-9370-63b27dd52055",
   "metadata": {},
   "outputs": [],
   "source": [
    "gov = cl.CredoGovernance(use_case_id=\"your-use-case-id\")\n",
    "gov.register_model(model_name='my_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
