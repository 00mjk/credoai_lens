{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Connecting with the Credo AI Governance App\n",
    "\n",
    "Connecting with the Credo AI Governance App is straightforward using Lens. After following this tutorial you'll be up and running!\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Find the code\n",
    "This notebook can be found on [github](https://github.com/credo-ai/credoai_lens/blob/develop/docs/notebooks/governance_integration.ipynb).\n",
    "\n",
    "### Config File\n",
    "First, ensure you have a config file somewhere on your computer named \".credoconfig\". The default location where Lens will look for it in your home directory (`~/.credoconfig`). The structure of the config should look like this\n",
    "\n",
    "```\n",
    "TENANT=<tenant name> # Example: credoai\n",
    "API_KEY=<your api key> # Example: JSMmd26...\n",
    "```\n",
    "\n",
    "This config gives Lens API access to the Governance App."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data + Model Preparation (before Lens)\n",
    "\n",
    "Some quick setup... This is all of your datascience work before assessment and integration with Credo AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for example data and model training\n",
    "from credoai.data import fetch_creditdefault\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Base Lens imports\n",
    "import credoai.lens as cl\n",
    "\n",
    "\n",
    "# set logging level to verbose\n",
    "cl.set_logging_level('info')\n",
    "# set default format for image displays. Change to 'png' if 'svg' is failing\n",
    "%config InlineBackend.figure_formats = ['svg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_creditdefault()\n",
    "df = data['data']\n",
    "df['target'] = data['target'].astype(int)\n",
    "\n",
    "# fit model\n",
    "model = GradientBoostingClassifier()\n",
    "X = df.drop(columns=['SEX', 'target'])\n",
    "y = df['target']\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credo Governance Artifact\n",
    "\n",
    "Lens connects to the Governance App via a `CredoGovernance` object. This specifies the IDs of the Use Case, model and data you want to associate your assessments with. The IDs are found in the url of your Use Case or model.\n",
    "\n",
    "For instance, the Use Case ID is found here: `...credo.com/use-cases/{use_case_id}`\n",
    "\n",
    "And the model Id is found here: `...credo.com/models/{model_id}`\n",
    "\n",
    "The CredoGovernance object has a few functionalities.\n",
    "\n",
    "* Retrieving an alignment spec from the Use Case (created during the aligned process). The alignment spec is either downloaded straight from the Governance App, or supplied as a json file for air gap deployment. This alignment spec can be supplemented (or overwritten) with an alignment spec you define in code and pass to Lens.\n",
    "* Registering artifacts (models, datasets) to the Governance App.\n",
    "* Encoding the IDs of governance objects so Lens can easily export metrics and reports.\n",
    "\n",
    "\\** **Attention** \\**\n",
    "\n",
    "The metrics and reports will _either_ be exported to the model and data specified in by `CredoGovernance` (using the `model_id` and `data_id` arguments) OR a new model and data will be registered on your CredoGovernance app automatically, which will then be used for export. You can also set the ids using the model/dataset _name_ by calling `credo_gov.set_governance_info_by_name`.\n",
    "\n",
    "With that said, let's see how this works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New artifact for governance\n",
    "credo_gov = cl.CredoGovernance(use_case_id=\"your-use-case-id\",\n",
    "                               model_id=\"your-model-id\"\n",
    "                               )\n",
    "\n",
    "credo_model = cl.CredoModel(name='credit_default_classifier',\n",
    "                            model=model)\n",
    "\n",
    "credo_data = cl.CredoData(name='UCI-credit-default',\n",
    "                          data=df,\n",
    "                          sensitive_feature_key='SEX',\n",
    "                          label_key='target'\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the metrics that will be used by the Fairness assessment. \n",
    "# This spec will supplement (or overide!) whatever spec is \n",
    "# pulled down from the Governance App\n",
    "alignment_spec = {\n",
    "    'Fairness': {'metrics': ['precision_score', 'recall_score']}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with Credo AI's Governance App from Lens\n",
    "\n",
    "In addition to the model and data, we can pass the CredoGovernance object. This is how Lens knows which Use Case, model and/or data to interacts with. If you are running assessments on multiple models, you'll create a separate `CredoGovernance` object (and separate Lens instances) for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.assessment import FairnessAssessment\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               governance=credo_gov,\n",
    "               assessments = [FairnessAssessment],\n",
    "               data=credo_data,\n",
    "               spec=alignment_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running, you just have to export and the metrics \n",
    "# will be sent to the Governance App\n",
    "lens.run_assessments().export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reports\n",
    "\n",
    "Reporting is a critical feature of Lens. Your assessments must be packaged in an easily digestible way to support downstream governance. As such, Lens supports robust reporting functionality.\n",
    "\n",
    "Lens creates a jupyter notebook report that collates all of your assessment results into one place. It then converts that notebook to HTML and sends it to Credo AI's Governance App, along with your metrics when `export` is called. The report will be associated with that use-case, as well as a model (either the model you supplied to CredoGovernance, or the model [registered](#Registering-a-model-while-assessing-it) by Lens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens.create_report().export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Registering a model while assessing it\n",
    "\n",
    "**No model on Credo AI? No Problem**\n",
    "\n",
    "Sometimes you may not have a model or dataset registered yet in the Governance App to receive your assessments.  If you still want to log a model assessment, Lens will take care of registering a new model on Credo AI's Governance App first. \n",
    "\n",
    "You still need to provide a use-case ID however. You can do this by creating a `CredoGovernance` object, or just by supplying the use-case ID to the \"governance\" argument, as we do below.\n",
    "\n",
    "The below will create a project called `an example model project` and log the model `an example model ` under it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.assessment import FairnessAssessment\n",
    "credo_model = cl.CredoModel(name='an example model',\n",
    "                            model=model)\n",
    "\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               assessments = [FairnessAssessment],\n",
    "               data=credo_data,\n",
    "               spec=alignment_spec)\n",
    "\n",
    "# Note! The below won't export anything unless you supply a use-case id\n",
    "# A warning will be raised to inform you of this.\n",
    "lens.run_assessments().create_report().export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The governance object is created for you, and can be interrogated or used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov = lens.get_governance()\n",
    "gov.get_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\** **Attention** \\**\n",
    "\n",
    "\n",
    "Note that if you run the above a second time, the model doesn't have to be registered again - it already was registered! Instead, Lens will find the ID for model name \"an example model\" and use that (this is because model names must be unique, and are only associated with one ID). If you would like to run Lens again for the same model, but store the data somewhere else you will have to either change the name of the model or explicitly supply a different model_id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering Models, Data and Use Cases\n",
    "\n",
    "While Lens will take care of registering a model and data for you if no `model_id` or `data_id` is provided, you can also manually register models yourself using a `CredoGovernance` object.\n",
    "\n",
    "This is helpful if you want to register a model or data without assessing it yet, if you want to run Lens multiple times, or want to have more control over the names of your artifacts. \n",
    "* First, you create a `CredoGovernance` object\n",
    "* Second register your project and model\n",
    "* Third, pass to Lens as normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Registering a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov = cl.CredoGovernance()\n",
    "gov.register_model(model_name='my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering a Dataset\n",
    "\n",
    "**Datasets** can be registered analogously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov.register_dataset('my_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering a Model to Use Case\n",
    "\n",
    "**Use Cases** can also be used. If CredoGovernance has a Use Case ID, the model will automatically be associated with that Use Case ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov = cl.CredoGovernance(use_case_id=\"your-use-case-id\")\n",
    "gov.register_model(model_name='my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Air Gap Environment\n",
    "\n",
    "If you cannot interact with Credo AI's Governance App via the internet, this section is for you!\n",
    "\n",
    "Instead of Lens automating the communication between your assessment environment and governance, you will instead have to download assets and upload them. \n",
    "\n",
    "### Getting the assessment spec\n",
    "The asset that you may have to get _from_ the governance app is the assessment spec. This can be found under your use case's \"Risk Assessment\" tab inside \"Align\". Once you download the assessments spec, you can read it with a CredoGovernance object:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov = cl.CredoGovernance()\n",
    "# you must explicitly retrieve the assessment spec \n",
    "# in an air gapped environment!\n",
    "gov.retrieve_assessment_spec('{path to assessment spec}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following that, you can pass the governance object to Lens as normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting Results\n",
    "\n",
    "Lens can export assessment results to file as a json object. This is generally useful if you want to store your results locally, but particularly useful if you are in an air gapped environment where you cannot access Credo AI's Governance App directly from your development environment.\n",
    "\n",
    "Doing this only requires a one line change in our current flow. We change the `export` argument from `credoai` to a local path:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens.run_assessments().create_report().export('{path where assessments should be exported}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above will create a folder that you specified, if it doesn't exist. It will then create a json file with the assessment results and the report, which can be uploaded to Credo AI's Governance App. This json file can be uploaded by going to the assessments of your **Use-Case** and pressing the purple \"upload assessment\" button."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d7eb8a87bb83596f4cd5aeb66d856dad2a9bb65fe804cea051250e36746a46f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pip_dev': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}