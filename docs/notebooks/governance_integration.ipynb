{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf51ef8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Failed to import tensorflow. Please note that tensorflow is not installed by default when you install tensorflow_hub. This is so that users can decide which tensorflow package to use. To use tensorflow_hub, please install a current version of tensorflow by following the instructions at https://tensorflow.org/install and https://tensorflow.org/hub/installation.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imports for example data and model training\n",
    "from credoai.data import fetch_creditdefault\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Base Lens imports\n",
    "import credoai.lens as cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82e5730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_creditdefault(as_frame=True)\n",
    "X = data['data'].drop(columns=['SEX'])\n",
    "y = data['target']\n",
    "sensitive_feature = data['data']['SEX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8ef89aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ba38b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "credo_model = cl.CredoModel(name='credit_default_classifier',\n",
    "                            model=model)\n",
    "\n",
    "credo_data = cl.CredoData(name='UCI-credit-default',\n",
    "                          X=X, \n",
    "                          y=y.astype(int),\n",
    "                          sensitive_features=sensitive_feature)\n",
    "credo_governance = cl.CredoGovernance(ai_solution_id=None,\n",
    "                                      model_id=\"v3saQTs9UnvVBz9Y9k5Lo2\"\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec487165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the metrics that will be used by the FairnessBase assessment\n",
    "alignment_spec = {\n",
    "    'FairnessBase': {'metrics': ['precision_score']}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f93643",
   "metadata": {},
   "source": [
    "#### Run Lens\n",
    "\n",
    "Once we have the model and data artifacts, as well as the spec, we can run Lens. By default it will automatically infer which assessments to run, just as we manually did above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af334d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.assessment import FairnessBaseAssessment\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               governance=credo_governance,\n",
    "               assessments = [FairnessBaseAssessment()],\n",
    "               data=credo_data,\n",
    "               spec=alignment_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad109240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FairnessBase': {'fairness':                     value\n",
       "  precision_score  0.000543,\n",
       "  'disaggregated_results':          precision_score\n",
       "  SEX                     \n",
       "  1.0             0.695954\n",
       "  2.0             0.695411\n",
       "  overall         0.695652}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens.run_assessments(export=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd09979",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = lens.assessments['FairnessBase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de48ddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "self = lens\n",
    "export=True\n",
    "assessment_kwargs =  {}\n",
    "assessment_results = {}\n",
    "for name, assessment in self.assessments.items():\n",
    "    kwargs = assessment_kwargs.get(name, {})\n",
    "    assessment_results[name] = assessment.run(**kwargs)\n",
    "    if export:\n",
    "        prepared_results = self._prepare_results(assessment, **kwargs)\n",
    "        if type(export) == str:\n",
    "            self._export_results_to_file(prepared_results, export)\n",
    "        else:\n",
    "            self._export_results_to_credo(prepared_results, to_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf554eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import credoai.integration as ci\n",
    "results = prepared_results\n",
    "to_model = True\n",
    "metric_records = ci.record_metrics(results)\n",
    "destination_id = self.gov.model_id if to_model else self.gov.data_id\n",
    "ci.export_to_credo(metric_records, destination_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2cf9a644",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_record = metric_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8283e31d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"data\": [{\"attributes\": {\"type\": \"precision_score\", \"value\": 0.0005434945456098683, \"dataset_variant_id\": null, \"metadata\": {\"creation_time\": \"14-01-2022 16:56:52\", \"model_label\": \"credit_default_classifier\", \"dataset_label\": \"UCI-credit-default\", \"user_id\": null, \"assessment\": \"FairnessBase\", \"lens_version\": \"Lens_v0.0.2\"}}, \"type\": \"model_metrics\"}, {\"attributes\": {\"type\": \"precision_score_SEX-1.0\", \"value\": 0.6959544879898862, \"dataset_variant_id\": null, \"metadata\": {\"creation_time\": \"14-01-2022 16:56:52\", \"model_label\": \"credit_default_classifier\", \"dataset_label\": \"UCI-credit-default\", \"user_id\": null, \"assessment\": \"FairnessBase\", \"lens_version\": \"Lens_v0.0.2\"}}, \"type\": \"model_metrics\"}, {\"attributes\": {\"type\": \"precision_score_SEX-2.0\", \"value\": 0.6954109934442764, \"dataset_variant_id\": null, \"metadata\": {\"creation_time\": \"14-01-2022 16:56:52\", \"model_label\": \"credit_default_classifier\", \"dataset_label\": \"UCI-credit-default\", \"user_id\": null, \"assessment\": \"FairnessBase\", \"lens_version\": \"Lens_v0.0.2\"}}, \"type\": \"model_metrics\"}, {\"attributes\": {\"type\": \"precision_score_SEX-overall\", \"value\": 0.6956521739130435, \"dataset_variant_id\": null, \"metadata\": {\"creation_time\": \"14-01-2022 16:56:52\", \"model_label\": \"credit_default_classifier\", \"dataset_label\": \"UCI-credit-default\", \"user_id\": null, \"assessment\": \"FairnessBase\", \"lens_version\": \"Lens_v0.0.2\"}}, \"type\": \"model_metrics\"}]}'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_record.credoify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "119f0cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.utils.credo_api_utils import *\n",
    "def patch_metrics(model_id, model_record):\n",
    "    \"\"\"Send a model record object to Credo's Governance Platform\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_id : string\n",
    "        Identifier for Model on Credo AI Governance Platform\n",
    "    model_record : Record\n",
    "        Model Record object, see credo.integration.MutliRecord\n",
    "    \"\"\"\n",
    "    end_point = get_end_point(f\"models/{model_id}/relationships/metrics\")\n",
    "    return submit_request('patch', end_point, data=model_record, headers={\"content-type\": \"application/vnd.api+json\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a536af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.credo-qa.com/api/v1/credoai/models/v3saQTs9UnvVBz9Y9k5Lo2/relationships/metrics\n"
     ]
    }
   ],
   "source": [
    "a=patch_metrics(credo_governance.model_id, model_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f90b58f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json_api_doc import serialize\n",
    "a=serialize(data=model_record._struct())\n",
    "a['included'] = []\n",
    "out = json.dumps(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34d927cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_metrics(credo_governance.model_id,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "789e5373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [{'attributes': {'dataset_variant_id': None,\n",
      "                          'metadata': {'assessment': 'FairnessBase',\n",
      "                                       'creation_time': '14-01-2022 16:56:52',\n",
      "                                       'dataset_label': 'UCI-credit-default',\n",
      "                                       'lens_version': 'Lens_v0.0.2',\n",
      "                                       'model_label': 'credit_default_classifier',\n",
      "                                       'user_id': None},\n",
      "                          'type': 'precision_score',\n",
      "                          'value': 0.0005434945456098683},\n",
      "           'type': 'model_metrics'},\n",
      "          {'attributes': {'dataset_variant_id': None,\n",
      "                          'metadata': {'assessment': 'FairnessBase',\n",
      "                                       'creation_time': '14-01-2022 16:56:52',\n",
      "                                       'dataset_label': 'UCI-credit-default',\n",
      "                                       'lens_version': 'Lens_v0.0.2',\n",
      "                                       'model_label': 'credit_default_classifier',\n",
      "                                       'user_id': None},\n",
      "                          'type': 'precision_score_SEX-1.0',\n",
      "                          'value': 0.6959544879898862},\n",
      "           'type': 'model_metrics'},\n",
      "          {'attributes': {'dataset_variant_id': None,\n",
      "                          'metadata': {'assessment': 'FairnessBase',\n",
      "                                       'creation_time': '14-01-2022 16:56:52',\n",
      "                                       'dataset_label': 'UCI-credit-default',\n",
      "                                       'lens_version': 'Lens_v0.0.2',\n",
      "                                       'model_label': 'credit_default_classifier',\n",
      "                                       'user_id': None},\n",
      "                          'type': 'precision_score_SEX-2.0',\n",
      "                          'value': 0.6954109934442764},\n",
      "           'type': 'model_metrics'},\n",
      "          {'attributes': {'dataset_variant_id': None,\n",
      "                          'metadata': {'assessment': 'FairnessBase',\n",
      "                                       'creation_time': '14-01-2022 16:56:52',\n",
      "                                       'dataset_label': 'UCI-credit-default',\n",
      "                                       'lens_version': 'Lens_v0.0.2',\n",
      "                                       'model_label': 'credit_default_classifier',\n",
      "                                       'user_id': None},\n",
      "                          'type': 'precision_score_SEX-overall',\n",
      "                          'value': 0.6956521739130435},\n",
      "           'type': 'model_metrics'}]}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import json\n",
    "pprint.pprint(json.loads(model_record.credoify()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc05596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
