{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56b77dd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Connecting with the Credo AI Governance App\n",
    "\n",
    "Connecting with the Credo AI Governance App is straightforward using Lens. This notebook is a comprehensive overview of how you you can interact with the Governance App.\n",
    "\n",
    "## Setup\n",
    "\n",
    "**Find the code**\n",
    "\n",
    "This notebook can be found on [github](https://github.com/credo-ai/credoai_lens/blob/develop/docs/notebooks/governance_integration.ipynb).\n",
    "\n",
    "**Config File**\n",
    "\n",
    "First, ensure you have a config file somewhere on your computer named \".credoconfig\". The default location where Lens will look for it in your home directory (`~/.credoconfig`). The structure of the config should look like this\n",
    "\n",
    "```\n",
    "TENANT=<tenant name> # Example: credoai\n",
    "CREDO_URL=<your credo url> \n",
    "API_KEY=<your api key> # Example: JSMmd26...\n",
    "```\n",
    "\n",
    "This config gives Lens API access to the Governance App. A config file can be downloaded from the web app. MySettings -> Token\n",
    "\n",
    "**Governance App Setup**\n",
    "Connecting Lens to the Governance App requires that you have already defined a Use-Case and registered a model to that use-case. That model serves as the endpoint for Lens.\n",
    "\n",
    "**Data + Model Preparation (before Lens)**\n",
    "\n",
    "Some quick setup. This script reflects all of your datascience work before assessment and integration with Credo AI.\n",
    "\n",
    "Here we have a gradient boosted classifier trained on the UCI Credit Card Default Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddde09b9-b132-4e8f-a411-395876b6389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and df are defined by this script\n",
    "%run training_script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583063fe-657c-4478-b71a-3cd9fd03687c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fec8ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Lens imports\n",
    "import credoai.lens as cl\n",
    "# set default format for image displays. Change to 'png' if 'svg' is failing\n",
    "%config InlineBackend.figure_formats = ['svg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a77abb-30a0-42a6-ab5b-8d42358d864a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Integration Quickstart\n",
    "\n",
    "Building off of the [quickstart notebook](https://credoai-lens.readthedocs.io/en/latest/notebooks/quickstart.html), let's look at how you can export to a Use Case on the Governance App. We only have to add two new lines.\n",
    "\n",
    "**Note** The below will fail unless you change \"assessment-spec-destination\" to a assessment-spec-destination related to an assessment spec actually defined in your Governance app! \n",
    "\n",
    "The assessment-spec will have the following form:\n",
    "`use_cases/{use_case_id}/models/{model_id}/assessment_spec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f695f324-ed49-44e2-9150-cda6218f26f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "credo_model = cl.CredoModel(name='credit_default_classifier',\n",
    "                            model=model)\n",
    "\n",
    "credo_data = cl.CredoData(name='UCI-credit-default',\n",
    "                          data=df,\n",
    "                          sensitive_feature_keys=['SEX'],\n",
    "                          label_key='target'\n",
    "                          )\n",
    "\n",
    "# specify the metrics that will be used by the Fairness and Performance assessment\n",
    "# run lens\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               governance='assessment-spec-destination' # <- NEW LINE!\n",
    "               )\n",
    "\n",
    "lens.run_assessments().create_report().export() # <- Added export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024f63c1-ad1a-4f88-be94-4fb89da4f3e6",
   "metadata": {},
   "source": [
    "Those two lines did a number of things under the hood.\n",
    "\n",
    "* First a [CredoGovernance Artifact](#Credo-Governance-Artifact) was created with the credo-url provided.\n",
    "* Next the datasets were registered:\n",
    "    * The dataset was registered to the Governanc App\n",
    "    * The dataset was registered as validation data for the associated model and use-case.\n",
    "* Next an assessment plan was downloaded and updated:\n",
    "    * An assessment plan was downloaded for the model (if it exists).\n",
    "    * The assessment plan was updated with the assessment_plan specified in code here.\n",
    "* Finally, when `export` was called the assessment was exported to the model under the use-case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce821181",
   "metadata": {
    "tags": []
   },
   "source": [
    "## In-Depth Overview\n",
    "\n",
    "### Credo Governance Artifact\n",
    "\n",
    "Lens connects to the Governance App via a `CredoGovernance` object. All you need to do is provide a `spec-destination`. The `spec-destination` can be one of two things:\n",
    "* A url for the spec in the AI Governance app (which can be found in the Governance App, or in an email requesting a model assessment))\n",
    "* A file path to the spec after it was downloaded from the AI Governance App (an example assessment_plan.json is included in the repo for this package and will be referenced below).\n",
    "\n",
    "**The CredoGovernance object has a few functionalities.**\n",
    "\n",
    "* Retrieving an alignment spec from the Credo AI Governance App. The alignment spec is either downloaded straight from the Governance App based on a credo_url, or supplied as a json file for air gap deployment. \n",
    "    * This alignment spec includes an assessment plan, which can be supplemented (or overwritten) with an assessment plan you define in code and pass to Lens.\n",
    "* Registering artifacts (e.g., datasets) to the Governance App.\n",
    "* Encoding the IDs of governance objects so Lens can easily export metrics and reports.\n",
    "\n",
    "Let's see how this works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdd91c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict, {'Performance': {'metrics': ['precision_score']}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we reference a local assessment plan, rather than providing a url!\n",
    "# Normally you would only do this in an air-gap environment\n",
    "credo_gov = cl.CredoGovernance(spec_destination = 'assessment_plan.json')\n",
    "\n",
    "# one aspect of the assessment spec is an assessment plan...\n",
    "credo_gov.get_assessment_plan()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee2d06-0863-4274-9618-dcb34d9ca692",
   "metadata": {},
   "source": [
    "The above assessment plan was defined by the assessment spec. It's a primary way in which the Governance App communicates with Lens and defines an assessment strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d025c84d-09ec-4511-aa3d-f8c26c559d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the other artifacts as normal\n",
    "credo_model = cl.CredoModel(name='credit_default_classifier',\n",
    "                            model=model)\n",
    "\n",
    "credo_data = cl.CredoData(name='UCI-credit-default',\n",
    "                          data=df,\n",
    "                          sensitive_feature_keys=['SEX'],\n",
    "                          label_key='target'\n",
    "                         )\n",
    "\n",
    "# specify the metrics that will be used by the Fairness assessment. \n",
    "# This plan will supplement the plan that is defined in the assessment spec!\n",
    "alignment_plan = {\n",
    "    'Fairness': {'metrics': ['precision_score', 'recall_score']}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb999b9",
   "metadata": {},
   "source": [
    "### Interacting with Credo AI's Governance App from Lens\n",
    "\n",
    "In addition to the model and data, we can pass the CredoGovernance object. This is how Lens knows which Use Case, model and/or data to interacts with. If you are running assessments on multiple models, you'll create a separate `CredoGovernance` object (and separate Lens instances) for each.\n",
    "\n",
    "You can also pass the `credo-url` and nothing else. In this case a CredoGovernance object will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abba24d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Automatically Selected Assessments for Model without data\n",
      "\n",
      "WARNING:absl:Model or Data does not conform to Performance assessment's requirements.\n",
      "Assessment will not be run\n",
      "WARNING:absl:Model or Data does not conform to Fairness assessment's requirements.\n",
      "Assessment will not be run\n",
      "INFO:absl:Automatically Selected Assessments for model: credit_default_classifier and validation dataset: UCI-credit-default\n",
      "--\n",
      "INFO:absl:Selected assessments...\n",
      "--Performance\n",
      "--Fairness\n",
      "INFO:absl:Initializing assessments for validation dataset: UCI-credit-default\n",
      "INFO:absl:Running assessment-Performance\n",
      "INFO:absl:Running assessment-Fairness\n"
     ]
    }
   ],
   "source": [
    "from credoai.assessment import FairnessAssessment, PerformanceAssessment\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               governance=credo_gov,\n",
    "               assessments = [PerformanceAssessment, FairnessAssessment],\n",
    "               assessment_plan=alignment_plan)\n",
    "\n",
    "# After running, you just have to export and the metrics \n",
    "# will be sent to the Governance App\n",
    "results = lens.run_assessments().get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b15128b",
   "metadata": {},
   "source": [
    "#### Reports\n",
    "\n",
    "Reporting is a critical feature of Lens. Your assessments must be packaged in an easily digestible way to support downstream governance. As such, Lens supports robust reporting functionality.\n",
    "\n",
    "Lens creates a jupyter notebook report that collates all of your assessment results into one place. It then converts that notebook to HTML and sends it to Credo AI's Governance App, along with your metrics when `export` is called. The report will be associated with that use-case, as well as a model (either the model you supplied to CredoGovernance, or the model [registered](#Registering-a-model-while-assessing-it) by Lens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdae661",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens.create_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3235b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Air Gap Environment\n",
    "\n",
    "If you cannot interact with Credo AI's Governance App via the internet, this section is for you!\n",
    "\n",
    "Instead of Lens automating the communication between your assessment environment and governance, you will instead have to download an assessment_spec, provided it to Lens, and then upload the results assessments. The demo has actually already been doing this with an example spec included in the github repo!\n",
    "\n",
    "### Getting the assessment spec\n",
    "The asset that you have to get _from_ the governance app is the assessment spec. This can be found under your use case's \"Risk Assessment\" tab inside \"Align\". Once you download the assessments spec, you can read it with a CredoGovernance object by changing one argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c82f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "gov = cl.CredoGovernance(spec_destination='{path to assessment spec}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80c1749",
   "metadata": {},
   "source": [
    "Following that, you can pass the governance object to Lens as normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d5d98d",
   "metadata": {},
   "source": [
    "### Exporting Results\n",
    "\n",
    "Lens can export assessment results to file as a json object. This is generally useful if you want to store your results locally, but particularly useful if you are in an air gapped environment where you cannot access Credo AI's Governance App directly from your development environment.\n",
    "\n",
    "Doing this only requires a one line change in our current flow. We change the `export` argument from `credoai` to a local path:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5efbc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens.run_assessments().create_report().export('{path where assessments should be exported}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d3f5c2",
   "metadata": {},
   "source": [
    "Running the above will create a folder that you specified, if it doesn't exist. It will then create a json file with the assessment results and the report, which can be uploaded to Credo AI's Governance App. This json file can be uploaded by going to the assessments of your **Use-Case** and pressing the purple \"upload assessment\" button."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d7eb8a87bb83596f4cd5aeb66d856dad2a9bb65fe804cea051250e36746a46f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
