{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af84a3a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lens FAQ\n",
    "This document answers some of the most common functionality questions you may have.\n",
    "\n",
    "**Find the code**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9783e8bc",
   "metadata": {},
   "source": [
    "Click <a class=\"reference internal\" download=\"\" href=\"../notebooks/lens_faq.ipynb\">here</a> to download this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0fa1a3",
   "metadata": {},
   "source": [
    "**Imports and Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b3080e2-2ace-4267-b948-44c25ce9c01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - You are using deepchecks version 0.11.1, however a newer version is available.Deepchecks is frequently updated with major improvements. You should consider upgrading via the \"python -m pip install --upgrade deepchecks\" command.\n"
     ]
    }
   ],
   "source": [
    "# model and data are defined by this script\n",
    "# This is a classification model\n",
    "%run training_script.py\n",
    "import credoai.lens as cl\n",
    "import credoai.evaluators as evl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc258d5-bb22-452e-9cc8-c16408f7358f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How do I get my model working with Lens?\n",
    "\n",
    "The first step in using Lens is creating a `Model` in the Lens framework. You will use different subclasses of `Model` depending on your model type. Currently, `ClassificationModel` and `RegressionModel` are defined.\n",
    "\n",
    "A model needs to be passed a \"model_like\" object. This is any object that defines the functions needed by the `Model`. For instance, `ClassificationModel` needs to be passed an object with `predict` function. This object can be a sklearn pipeline, a pytorch model, or any other object that conforms to sklearn's `predict` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e8eb583-b3b8-4911-86ad-31010ffecc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.artifacts import ClassificationModel\n",
    "\n",
    "credo_model = ClassificationModel(name='my_model', model_like=model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca689590-30aa-4a13-854a-c8bab1946c80",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Using precomputed values**\n",
    "\n",
    "A common use case you may run into is wanting to assess *pre-computed* predictions. You don't need Lens to perform inference, just use the inferences you've already generated for evaluator.\n",
    "\n",
    "In order to do so you need to coerce your predictions into a \"model-like\" object. To do so you make use of \"Dummy\" Models. Below is an example for `ClassificationModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aab08b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.artifacts import DummyClassifier\n",
    "# precomputed predictions\n",
    "predictions = model.predict(X)\n",
    "probs = model.predict_proba(X)\n",
    "# light wrapping to create the dummy model\n",
    "dummy_model = DummyClassifier(name='dummy', predict_output=predictions, predict_proba_output=probs)\n",
    "# ... which can then be passed to the ClassificationModel\n",
    "credo_model_from_predictions = ClassificationModel(name='my_model_name', model_like=dummy_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4532d934-754e-4140-85ee-730d0da40c78",
   "metadata": {},
   "source": [
    "## How do I get my datasets working with Lens?\n",
    "\n",
    "`Data` is the equivalent of `Model` for datasets. They can be passed to Lens as \"assessment_data\" (which is the validation data to assess the model against) or as \"training_data\" (which will not be used to evaluate the model, but will be assessed itself).\n",
    "\n",
    "`Data` is subclassed for particular data types. Currently `TabularData` is supported, which you can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "629a2bdc-d53c-41fa-8b4e-77a4fcf93856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.artifacts import TabularData\n",
    "credo_data = TabularData(name='my_dataset_name',\n",
    "                          X=X_test,\n",
    "                          y=y_test,\n",
    "                          sensitive_features=sensitive_features_test\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb26592e-61b4-438b-9f9f-5bfaa2d32df4",
   "metadata": {},
   "source": [
    "A number of things happen under the hood when you set up TabularData. For instance, numpy arrays are transformed into dataframes or series, and (optional) sensitive feature intersections are calculated."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3fbb6aef",
   "metadata": {},
   "source": [
    "## What evaluators exist in Lens?\n",
    "\n",
    "There are multiple evaluators in Lens. While you can find information about them through our [documentation](https://credoai-lens.readthedocs.io/en/latest/_autosummary/credoai.evaluators.html#module-credoai.evaluators), you can also view them directly using `list_evaluators`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23873392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DataFairness', data_fairness.DataFairness),\n",
       " ('DataProfiler', data_profiler.DataProfiler),\n",
       " ('Deepchecks', deepchecks_credoai.Deepchecks),\n",
       " ('DataEquity', equity.DataEquity),\n",
       " ('ModelEquity', equity.ModelEquity),\n",
       " ('ModelFairness', fairness.ModelFairness),\n",
       " ('FeatureDrift', feature_drift.FeatureDrift),\n",
       " ('IdentityVerification', identity_verification.IdentityVerification),\n",
       " ('ModelProfiler', model_profiler.ModelProfiler),\n",
       " ('Performance', performance.Performance),\n",
       " ('Privacy', privacy.Privacy),\n",
       " ('RankingFairness', ranking_fairness.RankingFairness),\n",
       " ('Security', security.Security),\n",
       " ('ShapExplainer', shap_credoai.ShapExplainer),\n",
       " ('SurvivalFairness', survival_fairness.SurvivalFairness)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all evaluators can be viewed with list_evaluators\n",
    "from credoai.evaluators import list_evaluators\n",
    "# returns a list of tuples of the form (evaluator_name, evaluator_class)\n",
    "list_evaluators()\n",
    "# Example import statement\n",
    "# from credoai.evaluators import Performance, DataProfiler, DataEquity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26654d8c-7490-4886-bc5b-37d877de0b51",
   "metadata": {},
   "source": [
    "## How do I get evaluators results from Lens?\n",
    "\n",
    "Running evaluators isn't very helpful if you can't view them! You can get results by calling `lens.get_results()`\n",
    "\n",
    "All results will be dataframes.\n",
    "\n",
    "**Note**\n",
    "\n",
    "If you want to export the evaluators to Credo AI's Governance App, check out the [Connecting with Governance App](https://credoai-lens.readthedocs.io/en/stable/notebooks/platform_integration.html) tutorial for directions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e53af3cd-22d1-4c45-baba-e435296cbe15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-10 22:47:54,837 - lens - INFO - Evaluator Performance added to pipeline. \n",
      "2023-04-10 22:47:55,104 - lens - INFO - Evaluator ModelFairness added to pipeline. Sensitive feature: SEX\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "lens = cl.Lens(model=credo_model,\n",
    "               assessment_data=credo_data)\n",
    "results = lens \\\n",
    "    .add(evl.Performance(['precision_score', 'recall_score'])) \\\n",
    "    .add(evl.ModelFairness(['precision_score', 'recall_score'])) \\\n",
    "    .run() \\\n",
    "    .get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c62978e-7d52-4bac-81be-0cb81132dee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluator: Performance\n",
      "              type     value\n",
      "0  precision_score  0.628081\n",
      "1     recall_score  0.360172\n",
      "\n",
      "   true_label predicted_label     value\n",
      "0           0               0  0.940916\n",
      "1           1               0  0.639828\n",
      "2           0               1  0.059084\n",
      "3           1               1  0.360172\n",
      "\n",
      "\n",
      "Evaluator: ModelFairness\n",
      "Sensitive_feature: SEX\n",
      "Dataset_type: assessment_data\n",
      "                     type     value\n",
      "0  precision_score_parity  0.016322\n",
      "1     recall_score_parity  0.027686\n",
      "\n",
      "      SEX             type     value\n",
      "0  female  precision_score  0.618687\n",
      "1    male  precision_score  0.635009\n",
      "2  female     recall_score  0.344585\n",
      "3    male     recall_score  0.372271\n",
      "\n",
      "   true_label predicted_label     value sens_feat_group\n",
      "0           0               0  0.935304          female\n",
      "1           1               0  0.655415          female\n",
      "2           0               1  0.064696          female\n",
      "3           1               1  0.344585          female\n",
      "4           0               0  0.944617            male\n",
      "5           1               0  0.627729            male\n",
      "6           0               1  0.055383            male\n",
      "7           1               1  0.372271            male\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lens.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f49cf1-4832-42f4-af7e-a12e7679e045",
   "metadata": {},
   "source": [
    "## What metrics are available?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1071252a-b96e-4373-88d6-b96ec053ec22",
   "metadata": {},
   "source": [
    "Each evaluator has different configuration options, as discused above. Some evaluators take a set of metrics as their configuration (e.g. the `ModelFairness` and `Performance`).\n",
    "\n",
    "Many metrics are supported out-of-the-box. These metrics can be referenced by string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4d6473e-36d9-4acc-a111-8f730ef1990f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BINARY_CLASSIFICATION\n",
      "\taccuracy_score, average_precision,\n",
      "\taverage_precision_score, balanced_accuracy_score,\n",
      "\tdiscriminatory_gini, discriminatory_gini_index,\n",
      "\tf1_score, fallout_rate, false_discovery_rate,\n",
      "\tfalse_match_rate, false_negative_rate,\n",
      "\tfalse_non_match_rate, false_omission_rate,\n",
      "\tfalse_positive_rate, fdr, fnr, fpr,\n",
      "\tgini_coefficient, gini_index, hit_rate, ks_score,\n",
      "\tks_score_binary,\n",
      "\tmatthews_correlation_coefficient, miss_rate,\n",
      "\toverprediction, precision, precision_score,\n",
      "\trecall, recall_score, roc_auc_score,\n",
      "\tselection_rate, sensitivity, specificity, tnr,\n",
      "\ttpr, true_negative_rate, true_positive_rate,\n",
      "\tunderprediction\n",
      "\n",
      "MULTICLASS_CLASSIFICATION\n",
      "\taccuracy_score, balanced_accuracy_score,\n",
      "\tdiscriminatory_gini, discriminatory_gini_index,\n",
      "\tf1_score, fallout_rate, false_discovery_rate,\n",
      "\tfalse_match_rate, false_negative_rate,\n",
      "\tfalse_non_match_rate, false_positive_rate, fdr,\n",
      "\tfnr, fpr, gini_coefficient, gini_index, hit_rate,\n",
      "\tmatthews_correlation_coefficient, miss_rate,\n",
      "\toverprediction, precision, precision_score,\n",
      "\trecall, recall_score, roc_auc_score,\n",
      "\tselection_rate, sensitivity, specificity, tnr,\n",
      "\ttpr, true_negative_rate, true_positive_rate,\n",
      "\tunderprediction\n",
      "\n",
      "BINARY_CLASSIFICATION_THRESHOLD\n",
      "\tdet_curve, detection_error_tradeoff, gain_chart,\n",
      "\tpr_curve, precision_recall_curve, roc_curve\n",
      "\n",
      "FAIRNESS\n",
      "\tdemographic_parity,\n",
      "\tdemographic_parity_difference,\n",
      "\tdemographic_parity_ratio, disparate_impact,\n",
      "\tequal_opportunity, equal_opportunity_difference,\n",
      "\tequalized_odds, equalized_odds_difference,\n",
      "\tstatistical_parity\n",
      "\n",
      "DATASET\n",
      "\tdemographic_parity_difference,\n",
      "\tdemographic_parity_ratio,\n",
      "\tmax_proxy_mutual_information,\n",
      "\tsensitive_feature_prediction_score\n",
      "\n",
      "PRIVACY\n",
      "\tmembership_inference_attack_score,\n",
      "\tmodel_based_attack_score, rule_based_attack_score\n",
      "\n",
      "SECURITY\n",
      "\tevasion_attack_score, extraction_attack_score\n",
      "\n",
      "REGRESSION\n",
      "\tMAE, MSD, MSE, RMSE, d2_tweedie_score,\n",
      "\texplained_variance_score, ks_score,\n",
      "\tks_score_regression, max_error,\n",
      "\tmean_absolute_error,\n",
      "\tmean_absolute_percentage_error,\n",
      "\tmean_gamma_deviance, mean_pinball_loss,\n",
      "\tmean_poisson_deviance, mean_squared_deviation,\n",
      "\tmean_squared_error, mean_squared_log_error,\n",
      "\tmedian_absolute_error, r2, r2_score, r_squared,\n",
      "\troot_mean_squared_error, target_ks_statistic\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# all out-of-the-box supported metrics can be accessed by calling list_metrics\n",
    "from credoai.modules import list_metrics\n",
    "metrics = list_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b5039c-6d85-452d-98ce-c31c68617f56",
   "metadata": {},
   "source": [
    "Under the hood each metric is wrapped in a `Metric` class. `Metrics` are lightweight wrapper classes that defines a few characteristics of the custom function needed by Lens.\n",
    "\n",
    "This class defines a canonical name for Lens, synonyms, a metric category, the function, and whether the metric takes probabilities or categorical predictions. The metric category defines the expected function signature, as described in `Metric`'s documentation\n",
    "\n",
    "For instance, below is the false positive rate metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36ed135b-fda9-4598-b381-e752726cff0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name='false_positive_rate', metric_category='BINARY_CLASSIFICATION', fun=<function false_positive_rate at 0x16e9fed30>, takes_prob=False, equivalent_names={'fpr', 'false_positive_rate', 'fallout_rate', 'false_match_rate'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from credoai.modules import BINARY_CLASSIFICATION_METRICS\n",
    "BINARY_CLASSIFICATION_METRICS['false_positive_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc23b24-96d2-44b7-b048-b3d6ca66c0e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How do I use my own custom metrics?\n",
    "\n",
    "Custom metrics can be created by using the `Metric` class.  \n",
    "\n",
    "**Example: Confidence Intervals**\n",
    "\n",
    "We will create custom metrics that reflect the lower and upper 95th percentile confidence bound on the true positive rate.\n",
    "\n",
    "Confidence intervals are not supported by default. However, they can be derived for some metrics using the `wilson confidence interval`. We will use a convenience function called `confusion_wilson` returns an array: [lower, upper] bound for metrics like true-positive-rate. \n",
    "\n",
    "Wrapping the wilson function in a `Metric` allows us to use it in Lens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68d83ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.modules.metrics_credoai import confusion_wilson\n",
    "from credoai.modules import Metric\n",
    "\n",
    "# define partial functions for the true positive rate lower bound\n",
    "def lower_bound_tpr(y_true, y_pred):\n",
    "    return confusion_wilson(y_true, y_pred, metric='true_positive_rate', confidence=0.95)[0]\n",
    "\n",
    "# and upper bound\n",
    "def upper_bound_tpr(y_true, y_pred):\n",
    "    return confusion_wilson(y_true, y_pred, metric='true_positive_rate', confidence=0.95)[1]\n",
    "\n",
    "# wrap the functions in fairness functions\n",
    "lower_metric = Metric(name = 'lower_bound_tpr', \n",
    "                      metric_category = \"binary_classification\",\n",
    "                      fun = lower_bound_tpr)\n",
    "\n",
    "upper_metric = Metric(name = 'upper_bound_tpr', \n",
    "                      metric_category = \"binary_classification\",\n",
    "                      fun = upper_bound_tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83a44df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-10 22:47:56,144 - lens - INFO - Evaluator Performance added to pipeline. \n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "Evaluator: Performance\n",
      "              type     value\n",
      "0  lower_bound_tpr  0.337201\n",
      "1              tpr  0.360172\n",
      "2  upper_bound_tpr  0.383802\n",
      "\n",
      "   true_label predicted_label     value\n",
      "0           0               0  0.940916\n",
      "1           1               0  0.639828\n",
      "2           0               1  0.059084\n",
      "3           1               1  0.360172\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run Lens as normal with custom metrics\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               assessment_data=credo_data)\n",
    "lens.add(evl.Performance([lower_metric, 'tpr', upper_metric]))\n",
    "lens.run().print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a1af3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_gini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "975a088070882147f8de9db5a531bf41cdbf56f68ebdbe6c3f86024c84ce6dc0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
