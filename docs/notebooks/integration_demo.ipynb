{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a95103",
   "metadata": {},
   "source": [
    "# Generic Integration With Credo AI's Governance App \n",
    "\n",
    "Lens is primarily a framework for comprehensive assessment of AI models. However, in addition, it is the primary way to integrate assessment analysis with Credo AI's Governance App.\n",
    "\n",
    "In this tutorial, we will take a model created and assessed _completely independently of Lens_ and send that data to Credo AI's Governance App\n",
    "\n",
    "### Find the code\n",
    "This notebook can be found on [github](https://github.com/credo-ai/credoai_lens/blob/develop/docs/notebooks/integration_demo.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b42f63",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create an example ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315f1ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df7a134",
   "metadata": {},
   "source": [
    "### Load data and train model\n",
    "\n",
    "For the purpose of this demonstration, we will be classifying digits after a large amount of noise has been added to each image.\n",
    "\n",
    "We'll create some charts and assessment metrics to reflect our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67745a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# add noise\n",
    "digits.data += np.random.rand(*digits.data.shape)*16\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n",
    "\n",
    "# create and fit model\n",
    "clf = SVC(probability=True)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3be6077",
   "metadata": {},
   "source": [
    "### Visualize example images along with predicted label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e135be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_plot = plt.figure()\n",
    "for i in range(8):\n",
    "    image_data = X_test[i,:]\n",
    "    prediction = digits.target_names[clf.predict(image_data[None,:])[0]]\n",
    "    label = f'Pred: \"{prediction}\"'\n",
    "    # plot\n",
    "    ax = plt.subplot(2,4,i+1)\n",
    "    ax.imshow(image_data.reshape(8,8), cmap='gray')\n",
    "    ax.set_title(label)\n",
    "    ax.tick_params(labelbottom=False, labelleft=False, length=0)\n",
    "plt.suptitle('Example Images and Predictions', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f1c4fc",
   "metadata": {},
   "source": [
    "### Calculate performance metrics and visualize\n",
    "\n",
    "As a multiclassification problem, we can calculate metrics per class, or overall. We record overall metrics, but include figures for individual class performance breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c42a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = classification_report(y_test, clf.predict(X_test), output_dict=True)\n",
    "overall_metrics = metrics['macro avg']\n",
    "del overall_metrics['support']\n",
    "pprint(overall_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7910953",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = clf.predict_proba(X_test)\n",
    "pr_curves = plt.figure(figsize=(8,6))\n",
    "# plot PR curve sper digit\n",
    "for digit in digits.target_names:\n",
    "    y_true = y_test == digit\n",
    "    y_prob = probs[:,digit]\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    plt.plot(recalls, precisions, lw=3, label=f'Digit: {digit}')\n",
    "plt.xlabel('Recall', fontsize=16)\n",
    "plt.ylabel('Precision', fontsize=16)\n",
    "\n",
    "# plot iso lines\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    label = label='ISO f1 curves' if f_score==f_scores[0] else ''\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2, label=label)\n",
    "# final touches\n",
    "plt.xlim([0.5, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.title('PR Curves per Digit', fontsize=20)\n",
    "plt.legend(loc='lower left', fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748b9f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "confusion_plot = plt.figure(figsize=(6,6))\n",
    "plot_confusion_matrix(clf, X_test, y_test, \\\n",
    "                      normalize='true', ax=plt.gca(), colorbar=False)\n",
    "plt.tick_params(labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5885387",
   "metadata": {},
   "source": [
    "## Sending assessment information to Credo AI\n",
    "\n",
    "Now that we have completed training and assessing the model, we will demonstrate how information can be sent to the Credo AI Governance App. Metrics related to performance, fairness, or other governance considerations are the most important kind of evidence needed for governance.\n",
    "\n",
    "In addition, figures are often produced that help communicate metrics better, understand the model, or other contextualize the AI system. Credo can ingest those as well.\n",
    "\n",
    "**Which metrics to record?**\n",
    "\n",
    "Ideally you will have decided on the most important metrics before building the model. We refer to this stage as `Metric Alignment`. This is the phase where your team explicitly determine how you will measure whether your model can be safely deployed. It is part of the more general `Alignment Stage`, which often requires input from multiple stakeholders outside of the team specifically involved in the development of the AI model.\n",
    "\n",
    "Of course, you may want to record more metrics than those explicitly determined during `Metric Alignment`.\n",
    "\n",
    "For instance, in this example let's say that during `Metric Alignment`, the _F1 Score_ is the primary metric used to evaluate model performance. However, we have decided that recall and precision would be helpful supporting. So we will send those three metrics.\n",
    "\n",
    "\n",
    "To reiterate: You are always free to send more metrics - Credo AI will ingest them. It is you and your team's decision which metrics are tracked specifically for governance purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b43c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import credoai.integration as ci\n",
    "from credoai.utils import list_metrics\n",
    "model_name = 'SVC'\n",
    "dataset_name = 'sklearn_digits'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7917b814",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Quick reference\n",
    "\n",
    "Below is all the code needed to record a set of metrics and figures. We will unpack each part below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e051e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "metric_records = ci.record_metrics_from_dict(overall_metrics, \n",
    "                                             model_label=model_name,\n",
    "                                             dataset_label=dataset_name)\n",
    "\n",
    "#figures\n",
    "example_figure_record = ci.Figure(examples_plot._suptitle.get_text(), examples_plot)\n",
    "confusion_figure_record = ci.Figure(confusion_plot.axes[0].get_title(), confusion_plot)\n",
    "\n",
    "pr_curve_caption=\"\"\"Precision-recall curves are shown for each digit separately.\n",
    "These are calculated by treating each class as a separate\n",
    "binary classification problem. The grey lines are \n",
    "ISO f1 curves - all points on each curve have identical\n",
    "f1 scores.\n",
    "\"\"\"\n",
    "pr_curve_figure_record = ci.Figure(pr_curves.axes[0].get_title(),\n",
    "                                figure=pr_curves,\n",
    "                                caption=pr_curve_caption)\n",
    "figure_records = ci.MultiRecord([example_figure_record, confusion_figure_record, pr_curve_figure_record])\n",
    "\n",
    "# export to file\n",
    "# ci.export_to_file(model_record, 'model_record.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94edcdcd",
   "metadata": {},
   "source": [
    "## Metric Record\n",
    "\n",
    "To record a metric you can either record each one manually or ingest a dictionary of metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d635aa",
   "metadata": {},
   "source": [
    "### Manually entering individual metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4457ca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_description = \"\"\"Harmonic mean of precision and recall scores.\n",
    "Ranges from 0-1, with 1 being perfect performance.\"\"\"\n",
    "f1_record = ci.Metric(metric_type='f1', \n",
    "                      value=overall_metrics['f1-score'],\n",
    "                      model_label=model_name, \n",
    "                      dataset_label=dataset_name)\n",
    "\n",
    "precision_record = ci.Metric(metric_type='precision',\n",
    "                             value=overall_metrics['precision'],\n",
    "                             model_label=model_name, \n",
    "                             dataset_label=dataset_name)\n",
    "\n",
    "recall_record = ci.Metric(metric_type='recall', \n",
    "                          value=overall_metrics['recall'],\n",
    "                          model_label=model_name, \n",
    "                          dataset_label=dataset_name)\n",
    "metrics = [f1_record, precision_record, recall_record]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40b0ff0",
   "metadata": {},
   "source": [
    "### Convenience to record multiple metrics\n",
    "\n",
    "Multiple metrics can be recorded as long as they are described using a pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a359f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_records = ci.record_metrics_from_dict(overall_metrics, \n",
    "                                             model_name=model_name, \n",
    "                                             dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264c4b21",
   "metadata": {},
   "source": [
    "## Record figures\n",
    "\n",
    "Credo can accept a path to an image file or a matplotlib figure. Matplotlib figures are converted to PNG images and saved.\n",
    "\n",
    "\n",
    "A caption can be included for futher description. Included a caption is recommended when the image is not self-explanatory, which is most of the time! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2cc922",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_figure_record = ci.Figure(examples_plot._suptitle.get_text(), examples_plot)\n",
    "confusion_figure_record = ci.Figure(confusion_plot.axes[0].get_title(), confusion_plot)\n",
    "\n",
    "pr_curve_caption=\"\"\"Precision-recall curves are shown for each digit separately.\n",
    "These are calculated by treating each class as a separate\n",
    "binary classification problem. The grey lines are \n",
    "ISO f1 curves - all points on each curve have identical\n",
    "f1 scores.\n",
    "\"\"\"\n",
    "pr_curve_figure_record = ci.Figure(pr_curves.axes[0].get_title(),\n",
    "                                   figure=pr_curves,\n",
    "                                   description=pr_curve_caption)\n",
    "figure_records = [example_figure_record, confusion_figure_record, pr_curve_figure_record]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e95e8c3",
   "metadata": {},
   "source": [
    "## MultiRecords\n",
    "\n",
    "To send all the information, we wrap the records in a MuliRecord, which wraps records of the same type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99136f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_records = ci.MultiRecord(metric_records)\n",
    "figure_records = ci.MultiRecord(figure_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88231de",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Export to Credo AI\n",
    "\n",
    "The json object of the model record can be created by calling `MultiRecord.jsonify()`. The convenience function `export_to_file` can be called to export the json record to a file. This file can then be uploaded to Credo AI's Governance App."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3abba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename is the location to save the json object of the model record\n",
    "# filename=\"XXX.json\"\n",
    "# ci.export_to_file(metric_records, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429d1d44",
   "metadata": {},
   "source": [
    "MultiRecords can be directly uploaded to Credo AI's Governance App as well. A model (or data) ID must be known to do so. You use `export_to_credo` to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2679a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"XXX\"\n",
    "# ci.export_to_credo(metric_records, model_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
