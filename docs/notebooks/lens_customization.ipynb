{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7be4a6b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Customizing Lens\n",
    "\n",
    "Lens strives to give you sensible defaults and automatically do the proper assessments whenever possible. However, there are many times where you'll want to change, select, or extend the functionality. Lens is intended to be an _extensible framework_ that can accomodate your own analysis plan.\n",
    "\n",
    "In this document we will describe a number of ways you can customize Lens:\n",
    "* Setting up CredoModels\n",
    "* Selecting which assessments to run\n",
    "* Parameterizing assessments\n",
    "* Creating new modules and assessments\n",
    "* Incorporating new metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0557f536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import credoai.lens as cl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67e648f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup\n",
    "\n",
    "We'll set up data and a model to run through some customization options. See `quickstart` for more information on this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3e63ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for example data and model training\n",
    "from credoai.data import fetch_creditdefault\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# data\n",
    "data = fetch_creditdefault(as_frame=True)\n",
    "X = data['data'].drop(columns=['SEX'])\n",
    "y = data['target']\n",
    "sensitive_feature = data['data']['SEX']\n",
    "# model\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfbaabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "credo_model = cl.CredoModel(name='credit_default_classifier',\n",
    "                            model=model)\n",
    "\n",
    "credo_data = cl.CredoData(name='UCI-credit-default',\n",
    "                          X=X, \n",
    "                          y=y.astype(int),\n",
    "                          sensitive_features=sensitive_feature)\n",
    "alignment_spec = {'metrics': ['precision_score']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617e3dc0-6cd0-42e0-aae1-fd36f93fd136",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating a CredoModel\n",
    "\n",
    "Above the CredoModel functionality was inferred from the fact that the model (GraidentBoostingClassifier) is a scikit-learn model. But CredoModels are generic wrappers around functions and can be created from scratch.\n",
    "\n",
    "The below \"custom_model\" instantiates two custom functions: `foo` and `bar`. Specific functionality is required by different assessments (see below). For instance, some assessments require that the CredoModel instantiates a `prob_fun`.\n",
    "\n",
    "Bottom line - CredoModels instantiate certain functionality, and this functionality determines which assessments can be run successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd91a65c-6697-45b1-ba6f-be32b4fdff4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'foo response'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = {'foo': lambda: 'foo response', 'bar': lambda: 'bar response'}\n",
    "custom_model = cl.CredoModel(name = 'example',\n",
    "                             model_config = model_config)\n",
    "\n",
    "custom_model.foo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f107e0b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Selecting Assessments\n",
    "\n",
    "**Automatic selection of assessments**\n",
    "\n",
    "Lens has a number of assessments available, each of which works with different kinds of models or datasets. By default, Lens will automatically run every assessment that has its prerequesites met. The prerequesities can be queried by calling the `get_requirements` function on an assessment. These indicate the set of features or functions your model and data must instantiate in order for the assessment to be run. \n",
    "\n",
    "These requirements are specific for the model and the data. Each requirement is either a single requirements or a tuple. If a tuple, only one of the requirements within the tuple must be met. For instance, the FairnessBaseAssessment needs *either* `prob_fun` OR `pred_fun`. See `credoai.assessments.credo_assessment.AssessmentRequirements` for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "363322ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_requirements': [('prob_fun', 'pred_fun')],\n",
       " 'data_requirements': ['X', 'y', 'sensitive_features']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from credoai.assessment import FairnessBaseAssessment\n",
    "assessment = FairnessBaseAssessment()\n",
    "assessment.get_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e6b1ed-7f3c-4555-9e95-c94cc9eddadd",
   "metadata": {},
   "source": [
    "You can also get the requirements for all assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cbb0615-bec7-4b62-8436-e9108cf8e053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DatasetAssessment': {'model_requirements': [],\n",
       "  'data_requirements': ['X', 'y', 'sensitive_features']},\n",
       " 'FairnessBaseAssessment': {'model_requirements': [('prob_fun', 'pred_fun')],\n",
       "  'data_requirements': ['X', 'y', 'sensitive_features']},\n",
       " 'NLPEmbeddingBiasAssessment': {'model_requirements': ['embedding_fun'],\n",
       "  'data_requirements': []},\n",
       " 'NLPGeneratorAssessment': {'model_requirements': ['generation_fun',\n",
       "   'assessment_config'],\n",
       "  'data_requirements': []}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from credoai.assessment import get_assessment_requirements\n",
    "get_assessment_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ea4789",
   "metadata": {},
   "source": [
    "**Selecting a subset of assessments**\n",
    "\n",
    "But what if you don't want all of those assessments? No problem! Just pass the assessments you do want to Lens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dbbb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Remember to instantiate the assessment!\n",
    "selected_assessments = [FairnessBaseAssessment()]\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               assessments=selected_assessments,\n",
    "               spec=alignment_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf48e4f",
   "metadata": {},
   "source": [
    "## Parameterizing Assessments\n",
    "\n",
    "Now that we can select assessments, how about customizing them? There are two places where assessments can be customized: (1) when their underlying module is initialized and (2) when they are ran (which runs the underlying module).\n",
    "\n",
    "### Customizing initialization\n",
    "Below we can see how to customize the initialization of the assessment. In this case we want to run an additional metric that isn't part of the `alignment spec`. The parameters that can be passed at this stage are the same parameters passed to the assessment's `init_module` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a888dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Remember to instantiate the assessment!\n",
    "selected_assessments = [FairnessBaseAssessment()]\n",
    "init_kwargs = {'FairnessBase': {'additional_metrics': ['recall_score']}}\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               assessments=selected_assessments,\n",
    "               spec=alignment_spec,\n",
    "               init_assessment_kwargs=init_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8e78f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens.run_assessments()['FairnessBase']['fairness']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d1e0a0",
   "metadata": {},
   "source": [
    "### Customizing running assessments \n",
    "\n",
    "The other way of parameterizing the assessments is by passing arguments to the assessment's `run` function. These kwargs are passed to `lens.run_assessments`, which are, in turn passed to the assessment's initialized module.\n",
    "\n",
    "For instance, the `FairnessBase` assessment initializes `mod.FairnessBase`, whose `run` argument can take a `method` parameter which controls how fairness scores are calculated. The default is \"between_groups\", but we can change it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc8e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_kwargs = {'FairnessBase': {'method': 'to_overall'}}\n",
    "lens.run_assessments(assessment_kwargs = run_kwargs)['FairnessBase']['fairness']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a80dc8d",
   "metadata": {},
   "source": [
    "## Creating New Modules & Assessments\n",
    "\n",
    "WIP section!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec53fc9",
   "metadata": {},
   "source": [
    "## Module Specific Customization - Custom Metrics for Fairness Base\n",
    "\n",
    "Each module has different parameterization options, as discused above. The FairnessBase module takes a set of metrics to calculate on the model and data. Many metrics are supported out-of-the-box. These metrics can be referenced by string. However, custom metrics can be created as well. Doing so will allow you to calculate any metric that takes in a `y_true` and some kind of prediction\n",
    "\n",
    "Custom metrics can be incorporated by creating a `FairnessFunction`. The `FairnessFunction` is a lightweight wrapper class that defines a few characteristics of the custom function needed by the module. \n",
    "\n",
    "**Example: Confidence Intervals**\n",
    "\n",
    "Confidence intervals are not generically supported. However, they can be derived for metrics derived from a confusion matrix using the `wilson confidence interval`. A convenience function called `confusion_wilson` is supplied which returns an array: [lower, upper] bound for the metric. \n",
    "\n",
    "However, the module is not designed to accomodate metrics that return arrays. If you wish to incorporate CIs, we'd suggest creating a derived function that returns the lower bound, for example, and then wrapping it in a `FairnessFunction` as we mentioned for custom functions above. We show a case of that below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac8aa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.utils.metrics import confusion_wilson\n",
    "from credoai.assessment.model_assessments.fairness_base import FairnessFunction\n",
    "\n",
    "# define partial functions for the true positive rate lower bound\n",
    "def lower_bound_tpr(y_true, y_pred):\n",
    "    return confusion_wilson(y_true, y_pred, metric='tpr', confidence=0.95)[0]\n",
    "\n",
    "# and upper bound\n",
    "def upper_bound_tpr(y_true, y_pred):\n",
    "    return confusion_wilson(y_true, y_pred, metric='tpr', confidence=0.95)[1]\n",
    "\n",
    "# wrap the functions in fairness functions\n",
    "lower_metric = FairnessFunction(name = 'lower_bound_tpr', \n",
    "                          func = lower_bound_tpr, \n",
    "                          takes_sensitive_features = False, \n",
    "                          takes_prob = False)\n",
    "\n",
    "upper_metric = FairnessFunction(name = 'upper_bound_tpr', \n",
    "                          func = upper_bound_tpr, \n",
    "                          takes_sensitive_features = False, \n",
    "                          takes_prob = False)\n",
    "           \n",
    "# then proceed as normally\n",
    "custom_assessment = FairnessModule([lower_metric, \n",
    "                           'true_positive_rate', \n",
    "                           upper_metric],\n",
    "                A_str_test,\n",
    "                Y_test,\n",
    "                test_preds,\n",
    "                test_scores,\n",
    "                fairness_bounds\n",
    "               )\n",
    "\n",
    "custom_assessment.get_disaggregated_results(calculate_risk=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
