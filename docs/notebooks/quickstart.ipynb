{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d608c2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Quickstart\n",
    "\n",
    "Get started here. We will assess a payment default prediction model for gender fairness using Lens, in 5 minutes. More in-depth information can be found in the [lens FAQ](https://credoai-lens.readthedocs.io/en/latest/notebooks/lens_faq.html#How-can-I-choose-which-assessments-to-run?)\n",
    "\n",
    "**Setup**\n",
    "\n",
    "Lens installation instruction can be found on [readthedocs](https://credoai-lens.readthedocs.io/en/stable/setup.html)\n",
    "\n",
    "**Find the code**\n",
    "\n",
    "Download this notebook [from github.](https://raw.githubusercontent.com/credo-ai/credoai_lens/develop/docs/notebooks/quickstart.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d800af",
   "metadata": {},
   "source": [
    "### Get your ML environment ready\n",
    " In this tutorial we will emulate the modeling phase by running a quick script. This script loads a dataset, splits it into training and testing, and fits a model. You can see the full script [here](https://github.com/credo-ai/credoai_lens/blob/release/1.0.0/docs/notebooks/training_script.py).\n",
    "\n",
    "Here we have a gradient boosted classifier trained on the UCI Credit Card Default Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f914dde-164c-40e0-8dc3-f0f5e00121ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/dev/lib/python3.9/site-packages/huggingface_hub/snapshot_download.py:6: FutureWarning: snapshot_download.py has been made private and will no longer be available from version 0.11. Please use `from huggingface_hub import snapshot_download` to import the only public function in this module. Other members of the file may be changed without a deprecation notice.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# model and X_test, y_test, etc. are defined by this script\n",
    "%run training_script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7557a63e-a7d8-4ade-9f71-71416d1abe2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1261860b-4ec3-42e1-872a-d357e22bd8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Lens and necessary artifacts\n",
    "from credoai.lens import Lens\n",
    "from credoai.artifacts import ClassificationModel, TabularData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e5f9e9",
   "metadata": {},
   "source": [
    "In lens, the classes that evaluate models and/or datasets are called `evaluators`. In this example we are interested in evaluating the model's fairness. For this we can use the `ModelFairness` evaluator. We'll\n",
    "also evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d780fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.evaluators import ModelFairness, Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0797a51",
   "metadata": {},
   "source": [
    "## Lens in 5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce779fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "Below is a basic example where our goal is to evaluate the above model. We'll break down this code [below](#Breaking-Down-The-Steps).\n",
    "\n",
    "Briefly, the code is doing four things:\n",
    "\n",
    "* Wrapping ML artifacts (like models and data) in Lens objects\n",
    "* Initializing an instance of Lens. Lens is the main object that performs evaluations. Under the hood, it creates a `pipeline` of evaluations that are run.\n",
    "* Add evaluators to Lens.\n",
    "* Run Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b4e6a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-28 18:31:13,560 - lens - INFO - Evaluator ModelFairness added to pipeline. Dataset used: assessment_data. Sensitive feature: SEX\n",
      "2022-09-28 18:31:13,759 - lens - INFO - fairness metric, equal_opportunity, unused by PerformanceModule\n",
      "2022-09-28 18:31:13,782 - lens - INFO - Evaluator Performance added to pipeline. \n",
      "2022-09-28 18:31:13,782 - lens - INFO - Running evaluation for step: MyModelFairness\n",
      "2022-09-28 18:31:13,806 - lens - INFO - Running evaluation for step: MyModelPerformance\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>value</th>\n",
       "      <th>subtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>equal_opportunity</td>\n",
       "      <td>0.027686</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision_score</td>\n",
       "      <td>0.016322</td>\n",
       "      <td>parity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall_score</td>\n",
       "      <td>0.027686</td>\n",
       "      <td>parity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                type     value   subtype\n",
       "0  equal_opportunity  0.027686  fairness\n",
       "1    precision_score  0.016322    parity\n",
       "2       recall_score  0.027686    parity"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up model and data artifacts\n",
    "credo_model = ClassificationModel(name=\"credit_default_classifier\", model_like=model)\n",
    "credo_data = TabularData(\n",
    "    name=\"UCI-credit-default\",\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    sensitive_features=sensitive_features_test,\n",
    ")\n",
    "\n",
    "# Initialization of the Lens object\n",
    "lens = Lens(model=credo_model, assessment_data=credo_data)\n",
    "\n",
    "# initialize the evaluator and add it to Lens\n",
    "metrics = ['precision_score', 'recall_score', 'equal_opportunity']\n",
    "lens.add(ModelFairness(metrics=metrics), id='MyModelFairness')\n",
    "lens.add(Performance(metrics=metrics), id='MyModelPerformance')\n",
    "\n",
    "# run Lens\n",
    "lens.run()\n",
    "lens.get_results()['MyModelFairness'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa1eab0",
   "metadata": {},
   "source": [
    "`lens.get_results()` provides a dictionary where the results of the evaluators are stored as values, and the keys correspond to the ids of the evaluators.  \n",
    "\n",
    "In the previous case we specified the id of the evaluator when we added `ModelFairness` to the pipeline, however `id` is an optional argument for the `add` method. If omitted, a random one will be generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ca7568",
   "metadata": {},
   "source": [
    "## Using Len's pipeline argument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498a11b3",
   "metadata": {},
   "source": [
    "If we want to add multiple evaluators to our pipeline, one way of doing it could be repeating the `add` step, as shown above. Another way is to define the pipeline steps, and pass it to `Lens` at initialization time. Let's explore the latter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f506b7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-28 18:31:14,085 - lens - INFO - Evaluator ModelFairness added to pipeline. Dataset used: assessment_data. Sensitive feature: SEX\n",
      "2022-09-28 18:31:14,283 - lens - INFO - fairness metric, equal_opportunity, unused by PerformanceModule\n",
      "2022-09-28 18:31:14,305 - lens - INFO - Evaluator Performance added to pipeline. \n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "    (ModelFairness(metrics), \"MyModelFairness\"),\n",
    "    (Performance(metrics), \"MyModelPerformance\"),\n",
    "]\n",
    "lens = Lens(model=credo_model, assessment_data=credo_data, pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa465d26",
   "metadata": {},
   "source": [
    "Above, each of the `tuples` in the `list` is in the form `(instantiated_evaluator, id)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c46bdd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-28 18:31:14,322 - lens - INFO - Running evaluation for step: MyModelFairness\n",
      "2022-09-28 18:31:14,346 - lens - INFO - Running evaluation for step: MyModelPerformance\n"
     ]
    }
   ],
   "source": [
    "# notice that Lens functions can be chained together\n",
    "results = lens.run().get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601f5203",
   "metadata": {},
   "source": [
    "Let's check that we have results for both of our evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab0ed632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>value</th>\n",
       "      <th>subtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>equal_opportunity</td>\n",
       "      <td>0.027686</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision_score</td>\n",
       "      <td>0.016322</td>\n",
       "      <td>parity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall_score</td>\n",
       "      <td>0.027686</td>\n",
       "      <td>parity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                type     value   subtype\n",
       "0  equal_opportunity  0.027686  fairness\n",
       "1    precision_score  0.016322    parity\n",
       "2       recall_score  0.027686    parity"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['MyModelFairness'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bb680a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>precision_score</td>\n",
       "      <td>0.628081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall_score</td>\n",
       "      <td>0.360172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              type     value\n",
       "0  precision_score  0.628081\n",
       "1     recall_score  0.360172"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['MyModelPerformance'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb18d57e",
   "metadata": {},
   "source": [
    "## That's it!\n",
    "\n",
    "That should get you up and running. Next steps include:\n",
    "\n",
    "* Trying out other evaluators (they are all accessible via `credoai.evaluators`)\n",
    "* Checking out our developer guide to better understand the Lens ecosystem and see how you can extend it.\n",
    "* Exploring the Credo AI Governance Platform, which will connect AI assessments with customizable governance to support reporting, compliance, multi-stakeholder translation and more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b8dd11-bc17-42b4-b73f-a42324f31780",
   "metadata": {},
   "source": [
    "## In-Depth Overview\n",
    "\n",
    "CredoAI Lens is the assessment framework component of the broader CredoAI suite.\n",
    "It is usable as a standalone gateway to assessments or in combination\n",
    "with CredoAI's Governance Platform. \n",
    "\n",
    "Understanding how your AI systems are operating is the most important step in intervening upon your system. From the technically complicated questions of improving a system to the business-relevant questions of whether to deploy a system - everything is fundamentally founded upon good observability. Lens strives to make assessment comprehensive, easy, and adaptable. The primary outputs from Lens are **assessment results** in the form of various metrics.\n",
    "\n",
    "When used in combination with CredoAI's Governance Platform, Lens can also be used to provide `evidences` to the Governance object. See !insert link to notebook! for more details on any governance related operations.\n",
    "\n",
    "### Evaluators\n",
    "\n",
    "Evaluators are the core units that perform any type of assessment on models and/or datasets. They can be developed internally by CredoAI, as well leverage existing frameworks that are alreayd part of the broader open-source ecosystem. Cusom analytics can be also easily wrapped into an evaluator (see `developer guide ! insert link`).\n",
    "\n",
    "AI system assessment starts with verifying standard performance metrics to an evolving set of assessments falling under the banner of *Responsible AI*. A non-exhaustive list includes\n",
    "\n",
    "* Fairness\n",
    "* Explainability\n",
    "* Performance\n",
    "* Robustness\n",
    "* Equity\n",
    "* Privacy\n",
    "* Security\n",
    "\n",
    "These different categories of assessment differ substantially based on whether one is \n",
    "evaluating datasets or models, what kind of model (e.g., tabular, NLP, computer vision), and the use-case. As the ecosystem develops, Lens will support assessing a broader range of AI systems. Currently, we are focused on Fairness.\n",
    "\n",
    "### Governance\n",
    "\n",
    "While Lens is a stand-alone assessment framework, its value is increased when combined with the CredoAI Governance Platform. The platform supports multi-stakeholder `Alignment` on how to assess your AI systems (e.g., what does good look like for this system?). \n",
    "\n",
    "It also supports translating a set of requirements, defined either within an organization or by existing legistlation, into a cohesive report that is accessible to a range of diverse stakeholders.\n",
    "\n",
    "Check out the [Connecting with Governance App](https://credoai-lens.readthedocs.io/en/latest/notebooks/governance_integration.html) for information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c05dfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Breaking Down The Steps\n",
    "\n",
    "### Preparing artifacts\n",
    "\n",
    "Lens interacts with Artifacts which wrap model and data objects and standardize them for use by different evaluators.\n",
    "\n",
    "Below we create a `ClassifiactionModel` artifact. This is a light wrapper for any kind of fitted classification model-like object. \n",
    "\n",
    "We also create a `TabularData` artifact which stores X, y and sensitive features.\n",
    "\n",
    "Both of these objects are customizable. See `lens_customization.ipynb` for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4f1fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model and data artifacts\n",
    "credo_model = ClassificationModel(name=\"credit_default_classifier\", model_like=model)\n",
    "credo_data = TabularData(\n",
    "    name=\"UCI-credit-default\",\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    sensitive_features=sensitive_features_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114bd0bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Model\n",
    "\n",
    "Model type objects, like `ClassificationModel` used above, serve as adapters between arbitrary models and the evaluators in Lens. Some evaluators depend on Model instantiating certain methods. For example, `ClassificationModel` can accept any generic object having `predict` and `predict_proba` methods, including fitted sklear pipelines. For more details see the source code: !!Insertlink!!\n",
    "\n",
    "\n",
    "The user can make use of existing model artifacts or create their own. In order to do that, see deatails in !!insertlink!!\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7fb09b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### CredoData\n",
    "\n",
    "_Data_ type artifact, like `TabularData` serve as adapters between datasets and the evaluators in Lens.\n",
    "\n",
    "When you pass data to a _Data_ artifact, the artifact performs various steps of validation, and formats them so that they can be used by evaluators. The aim of this procedure is to preempt errors down the line.\n",
    "\n",
    "You can pass Data to Lens as a **training dataset** or an **assessment dataset** (see lens class documentation). If the former, it will not be used to assess the model. Instead, dataset assessments will be performed on the dataset (e.g., fairness assessment). The validation dataset will be assessed in the same way, but _also_ used to assess the model, if provided.\n",
    "\n",
    "Similarly to _Model_ type objects, _Data_ objects can be customized, see !!insertlink!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4583249",
   "metadata": {},
   "source": [
    "### Evaluators \n",
    "\n",
    "Lens uses the above artifacts to ensure a successfull run of the evaluators. As we have seen in the sections [Lens in 5 minutes](##Lens-in-5-minutes) and [Adding multiple evaluators](##Adding-multiple-evaluators), multiple evaluators can be added to _Lens_ pipeline. Each evaluators contains information on what it needs in order to run successfully, and it executes a validation step at _add_ time.\n",
    "\n",
    "The result of the validation depends on what artifacts are available, their content and the type of evaluator being added to the pipeline. In case the validation process fails, the user is notified the reason why the evaluator cannot be added to the pipeline.\n",
    "\n",
    "See for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "836cccd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-28 18:31:14,477 - lens - INFO - Evaluator Privacy NOT added to the pipeline: Missing object training_data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<credoai.lens.lens.Lens at 0x293ab1be0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from credoai.evaluators import Privacy\n",
    "lens.add(Privacy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76369e8",
   "metadata": {},
   "source": [
    "Currently no automatic run of evaluators is supported. However, when Lens is used in combination with Credo AI Platform, it is possible to download an assessment plan which then gets converted into a set of evaluations that Lens can run programmatically. For more information: !!insert link!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb3c5b8",
   "metadata": {},
   "source": [
    "### Run Lens\n",
    "\n",
    "After we have initialized _Lens_ the _Model_ and _Data_ (`ClassificationModel` and `TabularData` in our example) type artifacts, we can add whichever evaluators we want to the pipeline, and finally run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc76430f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-28 18:31:14,745 - lens - INFO - Evaluator ModelFairness added to pipeline. Dataset used: assessment_data. Sensitive feature: SEX\n",
      "2022-09-28 18:31:14,746 - lens - INFO - Running evaluation for step: MyModelFairness\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<credoai.lens.lens.Lens at 0x293af0250>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = Lens(model=credo_model, assessment_data=credo_data)\n",
    "metrics = ['precision_score', 'recall_score', 'equal_opportunity']\n",
    "lens.add(ModelFairness(metrics=metrics), id='MyModelFairness')\n",
    "lens.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673dbbc9",
   "metadata": {},
   "source": [
    "As you can notice, when adding _evaluators_ to lens, they need to be instantiated. If any extra arguments need to be passed to the evaluator (like metrics in this case), this is the time to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5dcbe3",
   "metadata": {},
   "source": [
    "**Getting Assessment Results**\n",
    "\n",
    "Afte the pipeline is run, the results become accessible via the method `get_results()`\n",
    "\n",
    "`lens.get_results()` provides a dictionary where the results of the evaluators are stored as values, and the keys correspond to the ids of the evaluators.  \n",
    "\n",
    "In the previous case we specified the id of the evaluator when we added `ModelFairness` to the pipeline, however `id` is an optional argument for the `add` method. If omitted, a random one will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "031d237a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MyModelFairness': [                type     value   subtype\n",
       "  0  equal_opportunity  0.027686  fairness\n",
       "  1    precision_score  0.016322    parity\n",
       "  2       recall_score  0.027686    parity,\n",
       "        SEX             type     value\n",
       "  0  female  precision_score  0.618687\n",
       "  1    male  precision_score  0.635009\n",
       "  2  female     recall_score  0.344585\n",
       "  3    male     recall_score  0.372271]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens.get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3bfe1a",
   "metadata": {},
   "source": [
    "**Credo AI Governance Platform**\n",
    "\n",
    "For information on how to interact with the plaform, please look into: [Connecting with Governance App](https://credoai-lens.readthedocs.io/en/latest/notebooks/governance_integration.html) tutorial for directions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f46a3fd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.env2': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "507a61bde0a0183a71b2d35939f461921273a091e2cc4517af66dd70c4baafc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
