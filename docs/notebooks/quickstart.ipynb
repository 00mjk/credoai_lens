{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d608c2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Quickstart\n",
    "\n",
    "Get started here. We will assess a payment default prediction model for gender fairness using Lens, in 5 minutes. More in-depth information can be found in the [lens FAQ](https://credoai-lens.readthedocs.io/en/latest/notebooks/lens_faq.html#How-can-I-choose-which-assessments-to-run?)\n",
    "\n",
    "## Setup\n",
    "\n",
    "Setup instruction can be found on [readthedocs](https://credoai-lens.readthedocs.io/en/stable/setup.html)\n",
    "\n",
    "**Find the code**\n",
    "\n",
    "This notebook can be found on [github](https://github.com/credo-ai/credoai_lens/blob/develop/docs/notebooks/quickstart.ipynb).\n",
    "\n",
    "**Data + Model Preparation (before Lens)**\n",
    "\n",
    "Some quick setup. This script reflects all of your datascience work before assessment and integration with Credo AI.\n",
    "\n",
    "Here we have a gradient boosted classifier trained on the UCI Credit Card Default Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f914dde-164c-40e0-8dc3-f0f5e00121ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and df are defined by this script\n",
    "%run training_script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7557a63e-a7d8-4ade-9f71-71416d1abe2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1261860b-4ec3-42e1-872a-d357e22bd8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "# Base credo imports\n",
    "import credoai \n",
    "print(credoai.__version__)\n",
    "\n",
    "# Import Lens and necessary artifacts\n",
    "from credoai.lens import Lens\n",
    "from credoai.artifacts import ClassificationModel, TabularData\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0797a51",
   "metadata": {},
   "source": [
    "## Lens in 5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce779fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "Below is a basic example where our goal is to evaluate the above model.\n",
    "We will be evaluating model fairness according to three different metrics.\n",
    "\n",
    "We'll break down what the artifacts are [below](#Breaking-Down-The-Steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b4e6a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up model and data artifacts\n",
    "credo_model = ClassificationModel(name=\"credit_default_classifier\", model_like=model)\n",
    "credo_data = TabularData(\n",
    "    name=\"UCI-credit-default\",\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    sensitive_features=sensitive_features_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e5f9e9",
   "metadata": {},
   "source": [
    "In lens, the modules that evaluate models and/or datasets are called `evaluators`. Since in this case we are interested in evaluate model fairness, we will be importing the `ModelFairness` evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d780fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.evaluators import ModelFairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc7c02f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the Lens object\n",
    "lens = Lens(model=credo_model, assessment_data=credo_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de9cc0d",
   "metadata": {},
   "source": [
    "The next step is to add the evaluator to the initialized lens object. Notice that when we add an evaluator to the lens pipeline, it is initialized with all the necessary parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69a4b841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-26 11:58:48,071 - lens - INFO - Evaluator Fairness added to pipeline. Dataset used: assessment_data. Sensitive feature: SEX\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<credoai.lens.lens.Lens at 0x28a584c40>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = ['precision_score', 'recall_score', 'equal_opportunity']\n",
    "lens.add(ModelFairness(metrics=metrics), id='MyModelFairness')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9920b8eb",
   "metadata": {},
   "source": [
    "Let's run the pipeline and visualize the results!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef47c825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-26 11:59:14,096 - lens - INFO - Running evaluation for step: MyModelFairness\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>value</th>\n",
       "      <th>subtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>equal_opportunity</td>\n",
       "      <td>0.015972</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision_score</td>\n",
       "      <td>0.012715</td>\n",
       "      <td>parity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall_score</td>\n",
       "      <td>0.015972</td>\n",
       "      <td>parity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                type     value   subtype\n",
       "0  equal_opportunity  0.015972  fairness\n",
       "1    precision_score  0.012715    parity\n",
       "2       recall_score  0.015972    parity"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens.run()\n",
    "lens.get_results()['MyModelFairness'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa1eab0",
   "metadata": {},
   "source": [
    "`lens.get_results()` provides a dictionary where the results of the evaluators are stored as values, and the keys correspond to the ids of the evaluators.  \n",
    "\n",
    "In the previous case we specified the id of the evaluator when we added `ModelFairness` to the pipeline, however `id` is an optional argument for the `add` method. If omitted, a random one will be generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ca7568",
   "metadata": {},
   "source": [
    "## Adding multiple evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498a11b3",
   "metadata": {},
   "source": [
    "If we want to add multiple evaluators to our pipeline, one way of doing it could be repeating the `add` step shown above. Another way is to define the pipeline steps, and pass it to `Lens` at initialization time. Let's explore the latter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5a6604c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another evaluator that assess fairness in your dataset\n",
    "from credoai.evaluators import DataFairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f506b7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-26 12:15:50,006 - lens - INFO - Evaluator Fairness added to pipeline. Dataset used: assessment_data. Sensitive feature: SEX\n",
      "2022-09-26 12:15:50,010 - lens - INFO - Evaluator DataFairness added to pipeline. Dataset used: assessment_data. Sensitive feature: SEX\n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "    (ModelFairness(metrics), \"MyModelFairness\"),\n",
    "    (DataFairness(), \"MyDataFairness\"),\n",
    "]\n",
    "lens = Lens(model=credo_model, assessment_data=credo_data, pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa465d26",
   "metadata": {},
   "source": [
    "Above, each of the `tuples` in the `list` is in the form `(instantiated_evaluator, id)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c46bdd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-26 12:16:23,229 - lens - INFO - Running evaluation for step: MyModelFairness\n",
      "2022-09-26 12:16:23,239 - lens - INFO - Running evaluation for step: MyDataFairness\n"
     ]
    }
   ],
   "source": [
    "lens.run()\n",
    "results = lens.get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601f5203",
   "metadata": {},
   "source": [
    "Let's check that we have results for both of our evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab0ed632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>value</th>\n",
       "      <th>subtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>equal_opportunity</td>\n",
       "      <td>0.015972</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision_score</td>\n",
       "      <td>0.012715</td>\n",
       "      <td>parity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall_score</td>\n",
       "      <td>0.015972</td>\n",
       "      <td>parity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                type     value   subtype\n",
       "0  equal_opportunity  0.015972  fairness\n",
       "1    precision_score  0.012715    parity\n",
       "2       recall_score  0.015972    parity"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['MyModelFairness'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5bb680a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>target</th>\n",
       "      <th>type</th>\n",
       "      <th>subtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.183853</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sensitive_feature</td>\n",
       "      <td>prediction_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.027886</td>\n",
       "      <td>1.0</td>\n",
       "      <td>demographic_parity</td>\n",
       "      <td>difference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.880573</td>\n",
       "      <td>1.0</td>\n",
       "      <td>demographic_parity</td>\n",
       "      <td>ratio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.025036</td>\n",
       "      <td>NaN</td>\n",
       "      <td>proxy_mutual_information</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      value  target                      type           subtype\n",
       "0  0.183853     NaN         sensitive_feature  prediction_score\n",
       "1  0.027886     1.0        demographic_parity        difference\n",
       "2  0.880573     1.0        demographic_parity             ratio\n",
       "3  0.025036     NaN  proxy_mutual_information               max"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['MyDataFairness'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b8dd11-bc17-42b4-b73f-a42324f31780",
   "metadata": {},
   "source": [
    "## In-Depth Overview\n",
    "\n",
    "CredoAI Lens is the assessment framework component of the broader CredoAI suite.\n",
    "It is usable as a standalone gateway to assessments or in combination\n",
    "with CredoAI's Governance Platform. \n",
    "\n",
    "Understanding how your AI systems are operating is the most important step in intervening upon your system. From the technically complicated questions of improving a system to the business-relevant questions of whether to deploy a system - everything is fundamentally founded upon good observability. Lens strives to make assessment comprehensive, easy, and adaptable. The primary outputs from Lens are **assessment results** in the form of various metrics.\n",
    "\n",
    "When used in combination with CredoAI's Governance Platform, Lens can also be used to provide `evidences` to the Governance object. See !insert link to notebook! for more details on any governance related operations.\n",
    "\n",
    "### Evaluators\n",
    "\n",
    "Evaluators are the core units that perform any type of assessment on models and/or datasets. They can be developed internally by CredoAI, as well leverage existing frameworks that are alreayd part of the broader open-source ecosystem. Cusom analytics can be also easily wrapped into an evaluator (see `developer guide ! insert link`).\n",
    "\n",
    "AI system assessment starts with verifying standard performance metrics to an evolving set of assessments falling under the banner of *Responsible AI*. A non-exhaustive list includes\n",
    "\n",
    "* Fairness\n",
    "* Explainability\n",
    "* Performance\n",
    "* Robustness\n",
    "* Equity\n",
    "* Privacy\n",
    "* Security\n",
    "\n",
    "These different categories of assessment differ substantially based on whether one is \n",
    "evaluating datasets or models, what kind of model (e.g., tabular, NLP, computer vision), and the use-case. As the ecosystem develops, Lens will support assessing a broader range of AI systems. Currently, we are focused on Fairness.\n",
    "\n",
    "### Governance\n",
    "\n",
    "While Lens is a stand-alone assessment framework, its value is increased when combined with the CredoAI Governance Platform. The platform supports multi-stakeholder `Alignment` on how to assess your AI systems (e.g., what does good look like for this system?). \n",
    "\n",
    "It also supports translating a set of requirements, defined either within an organization or by existing legistlation, into a cohesive report that is accessible to a range of diverse stakeholders.\n",
    "\n",
    "Check out the [Connecting with Governance App](https://credoai-lens.readthedocs.io/en/latest/notebooks/governance_integration.html) for information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c05dfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Breaking Down The Steps\n",
    "\n",
    "### Preparing artifacts\n",
    "\n",
    "Lens interacts with Artifacts which wrap model and data objects and standardize them for use by different evaluators.\n",
    "\n",
    "Below we create a `ClassifiactionModel` artifact. This is a light wrapper for any kind of fitted classification model-like object. \n",
    "\n",
    "We also create a `TabularData` artifact which stores X, y and sensitive features.\n",
    "\n",
    "Both of these objects are customizable. See `lens_customization.ipynb` for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a4f1fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model and data artifacts\n",
    "credo_model = ClassificationModel(name=\"credit_default_classifier\", model_like=model)\n",
    "credo_data = TabularData(\n",
    "    name=\"UCI-credit-default\",\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    sensitive_features=sensitive_features_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114bd0bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Model\n",
    "\n",
    "Model type objects, like `ClassificationModel` used above, serve as adapters between arbitrary models and the evaluators in Lens. Some evaluators depend on Model instantiating certain methods. For example, `ClassificationModel` can accept any generic object having `predict` and `predict_proba` methods, including fitted sklear pipelines. For more details see the source code: !!Insertlink!!\n",
    "\n",
    "\n",
    "The user can make use of existing model artifacts or create their own. In order to do that, see deatails in !!insertlink!!\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7fb09b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### CredoData\n",
    "\n",
    "_Data_ type artifact, like `TabularData` serve as adapters between datasets and the evaluators in Lens.\n",
    "\n",
    "When you pass data to a _Data_ artifact, the artifact performs various steps of validation, and formats them so that they can be used by evaluators. The aim of this procedure is to preempt errors down the line.\n",
    "\n",
    "You can pass Data to Lens as a **training dataset** or an **assessment dataset** (see lens class documentation). If the former, it will not be used to assess the model. Instead, dataset assessments will be performed on the dataset (e.g., fairness assessment). The validation dataset will be assessed in the same way, but _also_ used to assess the model, if provided.\n",
    "\n",
    "Similarly to _Model_ type objects, _Data_ objects can be customized, see !!insertlink!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4583249",
   "metadata": {},
   "source": [
    "### Evaluators \n",
    "\n",
    "Lens uses the above artifacts to ensure a successfull run of the evaluators. As we have seen in the sections [Lens in 5 minutes](##Lens-in-5-minutes) and [Adding multiple evaluators](##Adding-multiple-evaluators), multiple evaluators can be added to _Lens_ pipeline. Each evaluators contains information on what it needs in order to run successfully, and it executes a validation step at _add_ time.\n",
    "\n",
    "The result of the validation depends on what artifacts are available, their content and the type of evaluator being added to the pipeline. In case the validation process fails, the user is notified the reason why the evaluator cannot be added to the pipeline.\n",
    "\n",
    "See for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "836cccd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-26 15:14:27,368 - lens - INFO - Evaluator Privacy NOT added to the pipeline: Missing object training_data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<credoai.lens.lens.Lens at 0x28a638fd0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from credoai.evaluators import Privacy\n",
    "lens.add(Privacy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76369e8",
   "metadata": {},
   "source": [
    "Currently no automatic run of evaluators is supported. However, when Lens is used in combination with Credo AI Platform, it is possible to download an assessment plan which then gets converted into a set of evaluations that Lens can run programmatically. For more information: !!insert link!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb3c5b8",
   "metadata": {},
   "source": [
    "### Run Lens\n",
    "\n",
    "After we have initialized _Lens_ the _Model_ and _Data_ (`ClassificationModel` and `TabularData` in our example) type artifacts, we can add whichever evaluators we want to the pipeline, and finally run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fc76430f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-26 15:48:33,916 - lens - INFO - Evaluator Fairness added to pipeline. Dataset used: assessment_data. Sensitive feature: SEX\n",
      "2022-09-26 15:48:33,918 - lens - INFO - Running evaluation for step: MyModelFairness\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<credoai.lens.lens.Lens at 0x28d995460>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = Lens(model=credo_model, assessment_data=credo_data)\n",
    "metrics = ['precision_score', 'recall_score', 'equal_opportunity']\n",
    "lens.add(ModelFairness(metrics=metrics), id='MyModelFairness')\n",
    "lens.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673dbbc9",
   "metadata": {},
   "source": [
    "As you can notice, when adding _evaluators_ to lens, they need to be instantiated. If any extra arguments need to be passed to the evaluator (like metrics in this case), this is the time to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5dcbe3",
   "metadata": {},
   "source": [
    "**Getting Assessment Results**\n",
    "\n",
    "Afte the pipeline is run, the results become accessible via the method `get_results()`\n",
    "\n",
    "`lens.get_results()` provides a dictionary where the results of the evaluators are stored as values, and the keys correspond to the ids of the evaluators.  \n",
    "\n",
    "In the previous case we specified the id of the evaluator when we added `ModelFairness` to the pipeline, however `id` is an optional argument for the `add` method. If omitted, a random one will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "031d237a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MyModelFairness': [                type     value   subtype\n",
       "  0  equal_opportunity  0.013653  fairness\n",
       "  1    precision_score  0.019533    parity\n",
       "  2       recall_score  0.013653    parity]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens.get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3bfe1a",
   "metadata": {},
   "source": [
    "**Credo AI Governance Platform**\n",
    "\n",
    "For information on how to interact with the plaform, please look into: [Connecting with Governance App](https://credoai-lens.readthedocs.io/en/latest/notebooks/governance_integration.html) tutorial for directions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f46a3fd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.env2': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "507a61bde0a0183a71b2d35939f461921273a091e2cc4517af66dd70c4baafc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
