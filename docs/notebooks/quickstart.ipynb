{
   "cells": [
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "5d608c2a",
         "metadata": {
            "tags": []
         },
         "source": [
            "\n",
            "\n",
            "# Quickstart\n",
            "\n",
            "Get started here. We will assess a payment default prediction model for performance and gender [fairness](https://www.credo.ai/glossary/fairness) using Lens, in 5 minutes. More in-depth information can be found in the [lens FAQ](https://credoai-lens.readthedocs.io/en/stable/notebooks/lens_faq.html#How-can-I-choose-which-assessments-to-run?)\n",
            "\n",
            "**Setup**\n",
            "\n",
            "Lens installation instruction can be found on [readthedocs](https://credoai-lens.readthedocs.io/en/stable/pages/setup.html)\n",
            "\n",
            "**Find the code**"
         ]
      },
      {
         "cell_type": "raw",
         "id": "cb8273f5",
         "metadata": {},
         "source": [
            "<!--Documentation related cell please ignore.-->\n",
            "Click <a class=\"reference internal\" download=\"\" href=\"../notebooks/quickstart.ipynb\">here</a> to download this notebook."
         ]
      },
      {
         "cell_type": "markdown",
         "id": "f4d800af",
         "metadata": {},
         "source": [
            "### Get your ML environment ready\n",
            " In this tutorial we will emulate the modeling phase by running a quick script. This script loads a dataset, splits it into training and testing, and fits a model. You can see the full script [here](https://raw.githubusercontent.com/credo-ai/credoai_lens/develop/docs/notebooks/training_script.py).\n",
            "\n",
            "Here we have a gradient boosted classifier trained on the UCI Credit Card Default Dataset."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "id": "9f914dde-164c-40e0-8dc3-f0f5e00121ae",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2023-05-01 17:50:07,634 - lens - WARNING - \n",
                  "            You are using credoai-lens version 1.1.8, however a newer version is available.\n",
                  "            Lens is updated regularly with major improvements and bug fixes.\n",
                  "            Please upgrade via the command: \"python -m pip install --upgrade credoai-lens\"\n",
                  "            \n"
               ]
            }
         ],
         "source": [
            "# model and X_test, y_test, etc. are defined by this script\n",
            "from credoai.datasets import fetch_credit_model\n",
            "\n",
            "# model and data are defined by this script\n",
            "X_test, y_test, sensitive_features_test, model = fetch_credit_model()"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "7557a63e-a7d8-4ade-9f71-71416d1abe2d",
         "metadata": {
            "tags": []
         },
         "source": [
            "### Imports"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "id": "1261860b-4ec3-42e1-872a-d357e22bd8b4",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2023-05-01 17:50:12,812 - connect - WARNING - \n",
                  "            You are using credoai-connect version 0.1.2, however a newer version is available.\n",
                  "            Lens is updated regularly with major improvements and bug fixes.\n",
                  "            Please upgrade via the command: \"python -m pip install --upgrade credoai-connect\"\n",
                  "            \n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "/opt/homebrew/Caskroom/miniforge/base/envs/py37/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                  "  from .autonotebook import tqdm as notebook_tqdm\n"
               ]
            }
         ],
         "source": [
            "# Import Lens and necessary artifacts\n",
            "from credoai.lens import Lens\n",
            "from credoai.artifacts import ClassificationModel, TabularData"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "62e5f9e9",
         "metadata": {},
         "source": [
            "In lens, the classes that evaluate models and/or datasets are called `evaluators`. In this example we are interested in evaluating the model's fairness. For this we can use the `ModelFairness` evaluator. We'll\n",
            "also evaluate the model's performance."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "id": "8d780fa4",
         "metadata": {},
         "outputs": [],
         "source": [
            "from credoai.evaluators import ModelFairness, Performance, ModelEquity, DataProfiler"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "c0797a51",
         "metadata": {},
         "source": [
            "## Lens in 5 minutes"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "cce779fd",
         "metadata": {
            "tags": []
         },
         "source": [
            "Below is a basic example where our goal is to evaluate the above model. We'll break down this code [below](#Breaking-Down-The-Steps).\n",
            "\n",
            "Briefly, the code is doing four things:\n",
            "\n",
            "* Wrapping ML artifacts (like models and data) in Lens objects\n",
            "* Initializing an instance of Lens. Lens is the main object that performs evaluations. Under the hood, it creates a `pipeline` of evaluations that are run.\n",
            "* Add evaluators to Lens.\n",
            "* Run Lens"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "id": "5b4e6a6a",
         "metadata": {
            "tags": []
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2023-05-01 17:50:14,667 - lens - INFO - Evaluator ModelFairness added to pipeline. Sensitive feature: SEX\n",
                  "2023-05-01 17:50:14,950 - lens - INFO - Evaluator Performance added to pipeline. \n",
                  "2023-05-01 17:50:15,069 - lens - INFO - Evaluator ModelEquity added to pipeline. Sensitive feature: SEX\n",
                  "2023-05-01 17:50:15,071 - lens - INFO - Evaluator DataProfiler added to pipeline. \n",
                  "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
                  "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
                  "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
                  "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s remaining:    0.0s\n",
                  "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
                  "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.1s finished\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "<credoai.lens.lens.Lens at 0x7fa2c1adde10>"
                  ]
               },
               "execution_count": 4,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# set up model and data artifacts\n",
            "credo_model = ClassificationModel(name=\"credit_default_classifier\", model_like=model)\n",
            "credo_data = TabularData(\n",
            "    name=\"UCI-credit-default\",\n",
            "    X=X_test,\n",
            "    y=y_test,\n",
            "    sensitive_features=sensitive_features_test,\n",
            ")\n",
            "\n",
            "# Initialization of the Lens object\n",
            "lens = Lens(model=credo_model, assessment_data=credo_data)\n",
            "\n",
            "# initialize the evaluator and add it to Lens\n",
            "metrics = ['precision_score', 'recall_score', 'equal_opportunity']\n",
            "lens.add(ModelFairness(metrics=metrics))\n",
            "lens.add(Performance(metrics=metrics))\n",
            "lens.add(ModelEquity())\n",
            "lens.add(DataProfiler())\n",
            "\n",
            "\n",
            "# run Lens\n",
            "lens.run()"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "c188b100-2ebb-42c8-9451-2812fe7383d6",
         "metadata": {},
         "source": [
            "### Getting results within your python environment\n",
            "\n",
            "`lens.get_results()` provides a list where the results of the evaluators (a list of dataframes) are stored along with the evaluator metadata. In this case, there are 2 results - one for each evaluator."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "id": "7eeac730-29bd-4300-8e02-d8677e10123e",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Results for 4 evaluators\n"
               ]
            }
         ],
         "source": [
            "results = lens.get_results()\n",
            "print(f\"Results for {len(results)} evaluators\")"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "8fa1eab0",
         "metadata": {},
         "source": [
            "`lens.get_results()` has some arguments, which makes it easier for you to get a subset of results. These are the same arguments that can be passed to `lens.get_pipeline` and `lens.get_evidence`."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "id": "f7735dd2-5797-45aa-90f2-212faa063edd",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/html": [
                     "<div>\n",
                     "<style scoped>\n",
                     "    .dataframe tbody tr th:only-of-type {\n",
                     "        vertical-align: middle;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe tbody tr th {\n",
                     "        vertical-align: top;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe thead th {\n",
                     "        text-align: right;\n",
                     "    }\n",
                     "</style>\n",
                     "<table border=\"1\" class=\"dataframe\">\n",
                     "  <thead>\n",
                     "    <tr style=\"text-align: right;\">\n",
                     "      <th></th>\n",
                     "      <th>type</th>\n",
                     "      <th>value</th>\n",
                     "    </tr>\n",
                     "  </thead>\n",
                     "  <tbody>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>precision_score</td>\n",
                     "      <td>0.628081</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>1</th>\n",
                     "      <td>recall_score</td>\n",
                     "      <td>0.360172</td>\n",
                     "    </tr>\n",
                     "  </tbody>\n",
                     "</table>\n",
                     "</div>"
                  ],
                  "text/plain": [
                     "              type     value\n",
                     "0  precision_score  0.628081\n",
                     "1     recall_score  0.360172"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "data": {
                  "text/html": [
                     "<div>\n",
                     "<style scoped>\n",
                     "    .dataframe tbody tr th:only-of-type {\n",
                     "        vertical-align: middle;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe tbody tr th {\n",
                     "        vertical-align: top;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe thead th {\n",
                     "        text-align: right;\n",
                     "    }\n",
                     "</style>\n",
                     "<table border=\"1\" class=\"dataframe\">\n",
                     "  <thead>\n",
                     "    <tr style=\"text-align: right;\">\n",
                     "      <th></th>\n",
                     "      <th>true_label</th>\n",
                     "      <th>predicted_label</th>\n",
                     "      <th>value</th>\n",
                     "    </tr>\n",
                     "  </thead>\n",
                     "  <tbody>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>0</td>\n",
                     "      <td>0</td>\n",
                     "      <td>0.940916</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>1</th>\n",
                     "      <td>1</td>\n",
                     "      <td>0</td>\n",
                     "      <td>0.639828</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>2</th>\n",
                     "      <td>0</td>\n",
                     "      <td>1</td>\n",
                     "      <td>0.059084</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>3</th>\n",
                     "      <td>1</td>\n",
                     "      <td>1</td>\n",
                     "      <td>0.360172</td>\n",
                     "    </tr>\n",
                     "  </tbody>\n",
                     "</table>\n",
                     "</div>"
                  ],
                  "text/plain": [
                     "   true_label predicted_label     value\n",
                     "0           0               0  0.940916\n",
                     "1           1               0  0.639828\n",
                     "2           0               1  0.059084\n",
                     "3           1               1  0.360172"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            }
         ],
         "source": [
            "performance_results = lens.get_results(evaluator_name='Performance')[0]\n",
            "# first dataframe is overall metrics\n",
            "display(performance_results['results'][0])\n",
            "# second dataframe is the long form of the confusion matrix\n",
            "display(performance_results['results'][1])"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "94ca7568",
         "metadata": {},
         "source": [
            "## Using Len's pipeline argument"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "498a11b3",
         "metadata": {},
         "source": [
            "If we want to add multiple evaluators to our pipeline, one way of doing it could be repeating the `add` step, as shown above. Another way is to define the pipeline steps, and pass it to `Lens` at initialization time. Let's explore the latter!"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "id": "f506b7d8",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2023-05-01 17:50:15,649 - lens - INFO - Evaluator ModelFairness added to pipeline. Sensitive feature: SEX\n",
                  "2023-05-01 17:50:15,919 - lens - INFO - Evaluator Performance added to pipeline. \n"
               ]
            }
         ],
         "source": [
            "pipeline = [\n",
            "    (ModelFairness(metrics)),\n",
            "    (Performance(metrics)),\n",
            "]\n",
            "lens = Lens(model=credo_model, assessment_data=credo_data, pipeline=pipeline)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "aa465d26",
         "metadata": {},
         "source": [
            "Above, each of the `tuples` in the `list` is in the form `(instantiated_evaluator, id)`."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "id": "c46bdd9e",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
                  "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
                  "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
                  "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s finished\n",
                  "\n",
                  "Found results for 2 evaluators\n"
               ]
            }
         ],
         "source": [
            "# notice that Lens functions can be chained together\n",
            "results = lens.run().get_results()\n",
            "print(f'\\nFound results for {len(results)} evaluators')"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "601f5203",
         "metadata": {},
         "source": [
            "Let's check that we have results for both of our evaluators. We'll print the metadata for each evaluation run followed by the dataframes for each result."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "id": "ab0ed632",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "evaluator: ModelFairness\n",
                  "sensitive_feature: SEX\n",
                  "dataset_type: assessment_data\n"
               ]
            },
            {
               "data": {
                  "text/html": [
                     "<div>\n",
                     "<style scoped>\n",
                     "    .dataframe tbody tr th:only-of-type {\n",
                     "        vertical-align: middle;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe tbody tr th {\n",
                     "        vertical-align: top;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe thead th {\n",
                     "        text-align: right;\n",
                     "    }\n",
                     "</style>\n",
                     "<table border=\"1\" class=\"dataframe\">\n",
                     "  <thead>\n",
                     "    <tr style=\"text-align: right;\">\n",
                     "      <th></th>\n",
                     "      <th>type</th>\n",
                     "      <th>value</th>\n",
                     "    </tr>\n",
                     "  </thead>\n",
                     "  <tbody>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>equal_opportunity</td>\n",
                     "      <td>0.027686</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>precision_score_parity</td>\n",
                     "      <td>0.016322</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>1</th>\n",
                     "      <td>recall_score_parity</td>\n",
                     "      <td>0.027686</td>\n",
                     "    </tr>\n",
                     "  </tbody>\n",
                     "</table>\n",
                     "</div>"
                  ],
                  "text/plain": [
                     "                     type     value\n",
                     "0       equal_opportunity  0.027686\n",
                     "0  precision_score_parity  0.016322\n",
                     "1     recall_score_parity  0.027686"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "data": {
                  "text/html": [
                     "<div>\n",
                     "<style scoped>\n",
                     "    .dataframe tbody tr th:only-of-type {\n",
                     "        vertical-align: middle;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe tbody tr th {\n",
                     "        vertical-align: top;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe thead th {\n",
                     "        text-align: right;\n",
                     "    }\n",
                     "</style>\n",
                     "<table border=\"1\" class=\"dataframe\">\n",
                     "  <thead>\n",
                     "    <tr style=\"text-align: right;\">\n",
                     "      <th></th>\n",
                     "      <th>SEX</th>\n",
                     "      <th>type</th>\n",
                     "      <th>value</th>\n",
                     "    </tr>\n",
                     "  </thead>\n",
                     "  <tbody>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>female</td>\n",
                     "      <td>precision_score</td>\n",
                     "      <td>0.618687</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>1</th>\n",
                     "      <td>male</td>\n",
                     "      <td>precision_score</td>\n",
                     "      <td>0.635009</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>2</th>\n",
                     "      <td>female</td>\n",
                     "      <td>recall_score</td>\n",
                     "      <td>0.344585</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>3</th>\n",
                     "      <td>male</td>\n",
                     "      <td>recall_score</td>\n",
                     "      <td>0.372271</td>\n",
                     "    </tr>\n",
                     "  </tbody>\n",
                     "</table>\n",
                     "</div>"
                  ],
                  "text/plain": [
                     "      SEX             type     value\n",
                     "0  female  precision_score  0.618687\n",
                     "1    male  precision_score  0.635009\n",
                     "2  female     recall_score  0.344585\n",
                     "3    male     recall_score  0.372271"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "data": {
                  "text/html": [
                     "<div>\n",
                     "<style scoped>\n",
                     "    .dataframe tbody tr th:only-of-type {\n",
                     "        vertical-align: middle;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe tbody tr th {\n",
                     "        vertical-align: top;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe thead th {\n",
                     "        text-align: right;\n",
                     "    }\n",
                     "</style>\n",
                     "<table border=\"1\" class=\"dataframe\">\n",
                     "  <thead>\n",
                     "    <tr style=\"text-align: right;\">\n",
                     "      <th></th>\n",
                     "      <th>true_label</th>\n",
                     "      <th>predicted_label</th>\n",
                     "      <th>value</th>\n",
                     "      <th>sens_feat_group</th>\n",
                     "    </tr>\n",
                     "  </thead>\n",
                     "  <tbody>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>0</td>\n",
                     "      <td>0</td>\n",
                     "      <td>0.935304</td>\n",
                     "      <td>female</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>1</th>\n",
                     "      <td>1</td>\n",
                     "      <td>0</td>\n",
                     "      <td>0.655415</td>\n",
                     "      <td>female</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>2</th>\n",
                     "      <td>0</td>\n",
                     "      <td>1</td>\n",
                     "      <td>0.064696</td>\n",
                     "      <td>female</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>3</th>\n",
                     "      <td>1</td>\n",
                     "      <td>1</td>\n",
                     "      <td>0.344585</td>\n",
                     "      <td>female</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>4</th>\n",
                     "      <td>0</td>\n",
                     "      <td>0</td>\n",
                     "      <td>0.944617</td>\n",
                     "      <td>male</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>5</th>\n",
                     "      <td>1</td>\n",
                     "      <td>0</td>\n",
                     "      <td>0.627729</td>\n",
                     "      <td>male</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>6</th>\n",
                     "      <td>0</td>\n",
                     "      <td>1</td>\n",
                     "      <td>0.055383</td>\n",
                     "      <td>male</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>7</th>\n",
                     "      <td>1</td>\n",
                     "      <td>1</td>\n",
                     "      <td>0.372271</td>\n",
                     "      <td>male</td>\n",
                     "    </tr>\n",
                     "  </tbody>\n",
                     "</table>\n",
                     "</div>"
                  ],
                  "text/plain": [
                     "   true_label predicted_label     value sens_feat_group\n",
                     "0           0               0  0.935304          female\n",
                     "1           1               0  0.655415          female\n",
                     "2           0               1  0.064696          female\n",
                     "3           1               1  0.344585          female\n",
                     "4           0               0  0.944617            male\n",
                     "5           1               0  0.627729            male\n",
                     "6           0               1  0.055383            male\n",
                     "7           1               1  0.372271            male"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "\n",
                  "\n",
                  "evaluator: Performance\n"
               ]
            },
            {
               "data": {
                  "text/html": [
                     "<div>\n",
                     "<style scoped>\n",
                     "    .dataframe tbody tr th:only-of-type {\n",
                     "        vertical-align: middle;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe tbody tr th {\n",
                     "        vertical-align: top;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe thead th {\n",
                     "        text-align: right;\n",
                     "    }\n",
                     "</style>\n",
                     "<table border=\"1\" class=\"dataframe\">\n",
                     "  <thead>\n",
                     "    <tr style=\"text-align: right;\">\n",
                     "      <th></th>\n",
                     "      <th>type</th>\n",
                     "      <th>value</th>\n",
                     "    </tr>\n",
                     "  </thead>\n",
                     "  <tbody>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>precision_score</td>\n",
                     "      <td>0.628081</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>1</th>\n",
                     "      <td>recall_score</td>\n",
                     "      <td>0.360172</td>\n",
                     "    </tr>\n",
                     "  </tbody>\n",
                     "</table>\n",
                     "</div>"
                  ],
                  "text/plain": [
                     "              type     value\n",
                     "0  precision_score  0.628081\n",
                     "1     recall_score  0.360172"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "data": {
                  "text/html": [
                     "<div>\n",
                     "<style scoped>\n",
                     "    .dataframe tbody tr th:only-of-type {\n",
                     "        vertical-align: middle;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe tbody tr th {\n",
                     "        vertical-align: top;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe thead th {\n",
                     "        text-align: right;\n",
                     "    }\n",
                     "</style>\n",
                     "<table border=\"1\" class=\"dataframe\">\n",
                     "  <thead>\n",
                     "    <tr style=\"text-align: right;\">\n",
                     "      <th></th>\n",
                     "      <th>true_label</th>\n",
                     "      <th>predicted_label</th>\n",
                     "      <th>value</th>\n",
                     "    </tr>\n",
                     "  </thead>\n",
                     "  <tbody>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>0</td>\n",
                     "      <td>0</td>\n",
                     "      <td>0.940916</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>1</th>\n",
                     "      <td>1</td>\n",
                     "      <td>0</td>\n",
                     "      <td>0.639828</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>2</th>\n",
                     "      <td>0</td>\n",
                     "      <td>1</td>\n",
                     "      <td>0.059084</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>3</th>\n",
                     "      <td>1</td>\n",
                     "      <td>1</td>\n",
                     "      <td>0.360172</td>\n",
                     "    </tr>\n",
                     "  </tbody>\n",
                     "</table>\n",
                     "</div>"
                  ],
                  "text/plain": [
                     "   true_label predicted_label     value\n",
                     "0           0               0  0.940916\n",
                     "1           1               0  0.639828\n",
                     "2           0               1  0.059084\n",
                     "3           1               1  0.360172"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "\n",
                  "\n"
               ]
            }
         ],
         "source": [
            "for r in results:\n",
            "    print('\\n'.join([f\"{k}: {v}\" for k, v in r['metadata'].items()]))\n",
            "    for df in r['results']:\n",
            "        display(df)\n",
            "    print('\\n')\n",
            "\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "bb18d57e",
         "metadata": {},
         "source": [
            "## That's it!\n",
            "\n",
            "That should get you up and running. Next steps include:\n",
            "\n",
            "* Trying out other evaluators (they are all accessible via `credoai.evaluators`)\n",
            "* Checking out our developer guide to better understand the Lens ecosystem and see how you can extend it.\n",
            "* Exploring the Credo AI Governance Platform, which will connect AI assessments with customizable governance to support reporting, compliance, multi-stakeholder translation and more!"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "c6c05dfe",
         "metadata": {
            "tags": []
         },
         "source": [
            "## Breaking Down The Steps\n",
            "\n",
            "### Preparing artifacts\n",
            "\n",
            "Lens interacts with \"AI Artifacts\" which wrap model and data objects and standardize them for use by different evaluators.\n",
            "\n",
            "Below we create a `ClassificationModel` artifact. This is a light wrapper for any kind of fitted classification model-like object. \n",
            "\n",
            "We also create a `TabularData` artifact which stores X, y and sensitive features."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "id": "a4f1fae7",
         "metadata": {},
         "outputs": [],
         "source": [
            "# set up model and data artifacts\n",
            "credo_model = ClassificationModel(name=\"credit_default_classifier\", model_like=model)\n",
            "\n",
            "credo_data = TabularData(\n",
            "    name=\"UCI-credit-default\",\n",
            "    X=X_test,\n",
            "    y=y_test,\n",
            "    sensitive_features=sensitive_features_test,\n",
            ")"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "114bd0bb",
         "metadata": {
            "tags": []
         },
         "source": [
            "#### Model\n",
            "\n",
            "Model type objects, like [ClassificationModel](https://credoai-lens.readthedocs.io/en/latest/_autosummary/credoai.artifacts.model.classification_model.html#module-credoai.artifacts.model.classification_model) used above, serve as adapters between arbitrary models and the evaluators in Lens. Some evaluators depend on Model instantiating certain methods. For example, `ClassificationModel` can accept any generic object having `predict` and `predict_proba` methods, including fitted sklearn pipelines.\n",
            "\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "db7fb09b",
         "metadata": {
            "tags": []
         },
         "source": [
            "#### Data\n",
            "\n",
            "_Data_ type artifact, like `TabularData` serve as adapters between datasets and the evaluators in Lens.\n",
            "\n",
            "When you pass data to a _Data_ artifact, the artifact performs various steps of validation, and formats them so that they can be used by evaluators. The aim of this procedure is to preempt errors down the line.\n",
            "\n",
            "You can pass Data to Lens as a **training dataset** or an **assessment dataset** (see lens [class documentation](https://credoai-lens.readthedocs.io/en/latest/_autosummary/credoai.lens.lens.Lens.html#credoai.lens.lens.Lens)). If the former, it will not be used to assess the model. Instead, dataset assessments will be performed on the dataset (e.g., fairness assessment). The validation dataset will be assessed in the same way, but _also_ used to assess the model, if provided.\n",
            "\n",
            "Similarly to _Model_ type objects, _Data_ objects can be customized, see [documentation](https://credoai-lens.readthedocs.io/en/latest/_autosummary/credoai.artifacts.data.html#module-credoai.artifacts.data)."
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "e4583249",
         "metadata": {},
         "source": [
            "### Evaluators \n",
            "\n",
            "Lens uses the above artifacts to ensure a successfull run of the evaluators. As we have seen in the sections [Lens in 5 minutes](#Lens-in-5-minutes) and [Adding multiple evaluators](#Using-Len's-pipeline-argument), multiple evaluators can be added to _Lens_ pipeline. Each evaluators contains information on what it needs in order to run successfully, and it executes a validation step at _add_ time.\n",
            "\n",
            "The result of the validation depends on what artifacts are available, their content and the type of evaluator being added to the pipeline. In case the validation process fails, the user is notified the reason why the evaluator cannot be added to the pipeline.\n",
            "\n",
            "See for example:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "id": "836cccd0",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2023-05-01 17:50:16,067 - lens - INFO - Evaluator DataFairness added to pipeline. Sensitive feature: SEX\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "<credoai.lens.lens.Lens at 0x7fa2d1a64f90>"
                  ]
               },
               "execution_count": 11,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "from credoai.evaluators import DataFairness\n",
            "lens.add(DataFairness())"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "e76369e8",
         "metadata": {},
         "source": [
            "Currently no automatic run of evaluators is supported. However, when Lens is used in combination with Credo AI Platform, it is possible to download an assessment plan which then gets converted into a set of evaluations that Lens can run programmatically. For more information see the [governance tutorial](https://credoai-lens.readthedocs.io/en/stable/notebooks/platform_integration.html)."
         ]
      },
      {
         "cell_type": "markdown",
         "id": "cdb3c5b8",
         "metadata": {},
         "source": [
            "### Run Lens\n",
            "\n",
            "After we have initialized _Lens_ the _Model_ and _Data_ (`ClassificationModel` and `TabularData` in our example) type artifacts, we can add whichever evaluators we want to the pipeline, and finally run it!"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "id": "fc76430f",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2023-05-01 17:50:16,311 - lens - INFO - Evaluator ModelFairness added to pipeline. Sensitive feature: SEX\n",
                  "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
                  "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
                  "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "<credoai.lens.lens.Lens at 0x7fa2e34738d0>"
                  ]
               },
               "execution_count": 12,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "lens = Lens(model=credo_model, assessment_data=credo_data)\n",
            "metrics = ['precision_score', 'recall_score', 'equal_opportunity']\n",
            "lens.add(ModelFairness(metrics=metrics))\n",
            "lens.run()"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "673dbbc9",
         "metadata": {},
         "source": [
            "As you can notice, when adding _evaluators_ to lens, they need to be instantiated. If any extra arguments need to be passed to the evaluator (like metrics in this case), this is the time to do it."
         ]
      },
      {
         "cell_type": "markdown",
         "id": "2a5dcbe3",
         "metadata": {},
         "source": [
            "**Getting Evaluator Results**\n",
            "\n",
            "Afte the pipeline is run, the results become accessible via the method `get_results()`\n",
            "\n",
            "`lens.get_results()` provides a dictionary where the results of the evaluators are stored as values, and the keys correspond to the ids of the evaluators.  \n",
            "\n",
            "In the previous case we specified the id of the evaluator when we added `ModelFairness` to the pipeline, however `id` is an optional argument for the `add` method. If omitted, a random one will be generated."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 13,
         "id": "031d237a",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "[{'metadata': {'evaluator': 'ModelFairness',\n",
                     "   'sensitive_feature': 'SEX',\n",
                     "   'dataset_type': 'assessment_data'},\n",
                     "  'results': [                     type     value\n",
                     "   0       equal_opportunity  0.030618\n",
                     "   0  precision_score_parity  0.030256\n",
                     "   1     recall_score_parity  0.030618,\n",
                     "         SEX             type     value\n",
                     "   0  female  precision_score  0.267436\n",
                     "   1    male  precision_score  0.237179\n",
                     "   2  female     recall_score    0.7173\n",
                     "   3    male     recall_score  0.686681,\n",
                     "      true_label predicted_label     value sens_feat_group\n",
                     "   0           0               0  0.401457          female\n",
                     "   1           1               0  0.282700          female\n",
                     "   2           0               1  0.598543          female\n",
                     "   3           1               1  0.717300          female\n",
                     "   4           0               0  0.428370            male\n",
                     "   5           1               0  0.313319            male\n",
                     "   6           0               1  0.571630            male\n",
                     "   7           1               1  0.686681            male]}]"
                  ]
               },
               "execution_count": 13,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "lens.get_results()"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "ac3bfe1a",
         "metadata": {},
         "source": [
            "**Credo AI Governance Platform**\n",
            "\n",
            "For information on how to interact with the plaform, please look into: [Connecting with Governance App](https://credoai-lens.readthedocs.io/en/stable/notebooks/platform_integration.html) tutorial for directions.\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "133c3491",
         "metadata": {},
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "test_connect_branch",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.7.12"
      },
      "vscode": {
         "interpreter": {
            "hash": "5dda0b5e19c4eb10b9a41a2edc552fb23118f5913d13212a587f3692e14d741d"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}
