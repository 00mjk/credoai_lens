{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69463794",
   "metadata": {},
   "source": [
    "# Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d522d44",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "CredoAI Lens can be installed with pip from PyPI as follows:\n",
    "\n",
    "`pip install credoai-lens`\n",
    "\n",
    "Optional dependencies can be installed as follows:\n",
    "\n",
    "`pip install 'credoai-lens[extras]'`\n",
    "\n",
    "### Find the code\n",
    "This notebook can be found on [github](https://github.com/credo-ai/credoai_lens/blob/develop/docs/notebooks/quickstart.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5876b40e",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "CredoAI Lens is the assessment framework component of the broader CredoAI suite.\n",
    "It is usable as a standalone gateway to assessments or in combination\n",
    "with CredoAI's Governance Platform. \n",
    "\n",
    "Understanding how your AI systems are operating is the most important step in intervening upon your system. From the technically complicated questions of improving a system to the business-relevant questions of whether to deploy a system - everything is fundamentally founded upon good observability. Lens strives to make assessment comprehensive, easy, and adaptable.\n",
    "\n",
    "### Assessments\n",
    "\n",
    "CredoAI Lens is an entry point to assessments developed by CredoAI, as well as the broader ecosystem of open-source assessments. Custom analytics can also be folded in easily (see the `lens customization` notebook)\n",
    "\n",
    "AI system assessment starts with verifying standard performance metrics to an evolving set of assessments falling under the banner of *Responsible AI*. A non-exhaustive list includes\n",
    "\n",
    "* Fairness\n",
    "* Explainability\n",
    "* Performance\n",
    "* Robustness\n",
    "\n",
    "These different categories of assessment differ substantially based on whether one is \n",
    "evaluating datasets or models, what kind of model (e.g., tabular, NLP, computer vision), and the use-case. As the ecosystem develops, Lens will support assessing a broader range of AI systems. Currently, we are focused on Fairness.\n",
    "\n",
    "### Governance\n",
    "\n",
    "While Lens is a stand-alone assessment framework, its value is increased when combined with the CredoAI Governance Platform. The platform supports multi-stakeholder `Alignment` on how to assess your AI systems (e.g., what does good look like for this system?). It also supports translating assessment results into a Risk perspective that is scalable across your organization and understandable to diverse stakeholders.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bda618",
   "metadata": {},
   "source": [
    "## Lens in 5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9768c46d",
   "metadata": {},
   "source": [
    "Get your data and models together! Here we have a support vector machine trained on the UCI Credit Card Default Dataset. No train/test split needed for this tutorial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5635aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for example data and model training\n",
    "from credoai.data import fetch_creditdefault\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Base Lens imports\n",
    "import credoai.lens as cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dc297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_creditdefault()\n",
    "X = data['data'].drop(columns=['SEX'])\n",
    "y = data['target']\n",
    "sensitive_feature = data['data']['SEX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e26339",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1744f398",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using Lens\n",
    "\n",
    "Below is a basic example where our goal is to evaluate the fairness of the above model. We will rely on Lens defaults for this analysis.\n",
    "\n",
    "### Quick Reference\n",
    "Below are all the steps needed to assess a model using Lens. We'll break these down [below](#Breaking-Down-The-Steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac7f734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up moden and data artifacts\n",
    "credo_model = cl.CredoModel(name='credit_default_classifier',\n",
    "                            model=model)\n",
    "\n",
    "credo_data = cl.CredoData(name='UCI-credit-default',\n",
    "                          X=X, \n",
    "                          y=y.astype(int),\n",
    "                          sensitive_features=sensitive_feature)\n",
    "\n",
    "# specify the metrics that will be used by the FairnessBase assessment\n",
    "alignment_spec = {\n",
    "    'FairnessBase': {'metrics': ['precision_score']}\n",
    "}\n",
    "\n",
    "# run lens\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               spec=alignment_spec)\n",
    "results = lens.run_assessments()\n",
    "reports = lens.create_reports()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2b2886",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Breaking Down The Steps\n",
    "\n",
    "#### Preparing artifacts\n",
    "\n",
    "Lens interacts with Credo Artifacts which wrap models and datasets and standardizes them for use by different assessments.\n",
    "Below we create a `CredoModel` object, which automatically infers that the \"model\" object is from scikit-learn. We also create a `CredoData` object which is store X, y and sensitive features. Both of these objects are customizable. See `lens_customization.ipynb` for more information.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf80b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "credo_model = cl.CredoModel(name='credit_default_classifier',\n",
    "                            model=model)\n",
    "\n",
    "credo_data = cl.CredoData(name='UCI-credit-default',\n",
    "                          X=X, \n",
    "                          y=y.astype(int),\n",
    "                          sensitive_features=sensitive_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffd6212",
   "metadata": {
    "tags": []
   },
   "source": [
    "\\\n",
    "\\\n",
    "**Other ways to use CredoModel**\n",
    "\n",
    "The way a CredoModel works is by defining a \"config\" dictionary that outlines the models functionality.\n",
    "\n",
    "Above the CredoModel functionality was inferred from the fact that the model (GraidentBoostingClassifier) is a scikit-learn model. But under the hood all that happens was it defined a `config`.\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e0c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the config was inferred from the model passed to CredoModel\n",
    "credo_model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a735227a",
   "metadata": {},
   "source": [
    "\\\n",
    "\\\n",
    "*The Config is all that matters* \n",
    "\n",
    "Since the `config` is all that matters, CredoModels are just wrappers around functions. They can accomodate any modeling framework, or even completely custom functions\n",
    "\n",
    "For instance, we could have created the above `credo_model` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd2c21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'prob_fun': model.predict_proba,\n",
    "          'pred_fun':  model.predict}\n",
    "credo_model = cl.CredoModel(name='credit_default_classifier',\n",
    "                            model_config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95695061",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "*Using precomputed values*\n",
    "\n",
    "A common use case you may run into is wanting to assess *pre-computed* predictions. You don't need Lens to perform inference, just use the inferences you've already generated for assessment.\n",
    "\n",
    "Below is an example of such a case using the same example. Note that the `pred_fun` still needs to take in an `X` variable to maintain the appropriate function signature. In this case, however, X is completely ignored and the predictions are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfabe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precomputed predictions\n",
    "predictions = model.predict(X)\n",
    "probs = model.predict_proba(X)\n",
    "# light wrapping\n",
    "config = {'pred_fun': lambda X: predictions,\n",
    "          'prob_fun': lambda X: probs}\n",
    "credo_model = cl.CredoModel(name='credit_default_classifier',\n",
    "                            model_config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80199494",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               spec=alignment_spec)\n",
    "results = lens.run_assessments()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ba3367",
   "metadata": {},
   "source": [
    "### Assessments \n",
    "\n",
    "Lens uses the functionality of the above artifacts to automatically determine which assessments can be run. In this case the Dataset Assessment and FairnessBase Assessment can be run. You can see what assessments are runnable with the following function.\n",
    "\n",
    "Assessments can be chosen, rather than inferred. See the [customizing lens notebook](https://credoai-lens.readthedocs.io/en/latest/notebooks/lens_customization.html#Selecting-Assessments) for this functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97857bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.assessment import get_usable_assessments\n",
    "get_usable_assessments(credo_model, credo_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46b8907",
   "metadata": {},
   "source": [
    "You can also list all assesments, along with their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f142e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.assessment.utils import get_assessment_names\n",
    "get_assessment_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1e3de8-6d4a-4aed-82f3-c04fa701b949",
   "metadata": {},
   "source": [
    "#### Assessment Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8ed40f-578a-4586-b0a5-28c680db7686",
   "metadata": {},
   "source": [
    "Documentation for each assessment is a bit difficult to understand. Assessments wrap modules, which themselves have documentation. Normally, you don't have to worry about the module itself, except if you are creating your own assessments or want to use the modules directly. The main exception here is if you want to parameterize how you run the assessment\n",
    "\n",
    "For the assessments, you may be interested in what parameters you can pass to their initialization (which is passed using the `spec` parameter when running Lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2766315e-8fd7-4f7c-924a-649c33cedb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different aspects of documentation you may be interested in\n",
    "from credoai.assessment import FairnessBaseAssessment\n",
    "\n",
    "# what parameters can be passed to the initialization?\n",
    "FairnessBaseAssessment.init_module?\n",
    "\n",
    "# what requirements are needed? \n",
    "# (This is normally included in the assessments base documentation)\n",
    "assessment = FairnessBaseAssessment()\n",
    "assessment.get_requirements()\n",
    "\n",
    "# what does the module require? \n",
    "# This is often similar to the parameters passed to assessment initialization\n",
    "assessment.module?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453dc6d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Alignment Spec\n",
    "\n",
    "The Alignment Spec describes how the assessments should be run. Think about it is as the *parameterization* of the assessments Lens will run.\n",
    "\n",
    "If you use the Credo AI Governance Platform, the alignment spec is a principle artifact determined during the *Alignment Phase*. It is the output of multi-stakeholder collaboration. Lens will automatically download the Alignment Spec associated with your governance credentials (which uses another artifact: `CredoGovernance`)\n",
    "\n",
    "The Alignment Spec retrieved from the Governance Platform is a *subset* of all possible parameterizations of the different assessments you can run. Some parameters are too \"in the weeds\" for multistakeholder collaboration! However, you have full control of the assessments by manually defining the `spec`. Anything defined in the spec parameter will take precedence over the spec retrieved from the Governance Platform.\n",
    "\n",
    "**Setting up the Spec**\n",
    "\n",
    "The alignment spec is a set of {assessment_name: parameter} pairs. The assessment name must be the name of one of the assessments, as returned by `get_usable_assessments` (above). In general, the name will be the name of the method without the trailing \"assessment\". For example, FairnessBaseAssessment -> \"FairnessBase\". `get_assessment_names` will tell you the names you need.\n",
    "\n",
    "The spec's parameters are passed to each Assessments `init_module` function.\n",
    "\n",
    "Not all assessments *require* a spec, though many can be customized. In the case of \"FairnessBase\" a spec defining a list of metrics **is** required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a07908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the metrics that will be used by the FairnessBase assessment\n",
    "alignment_spec = {\n",
    "    'FairnessBase': {'metrics': ['precision_score']}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd0f234",
   "metadata": {},
   "source": [
    "### Run Lens\n",
    "\n",
    "Once we have the model and data artifacts, as well as the spec, we can run Lens. By default it will automatically infer which assessments to run, just as we manually did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8a54be",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               spec=alignment_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb122ce",
   "metadata": {},
   "source": [
    "**Getting Assessment Results**\n",
    "\n",
    "Lens has one primary method: `run_assessments`\n",
    "\n",
    "`run_assessments` outputs the results into a dictionary that can be used for further processing. If the `export` keyword is used, it can also export the data to a json or straight to Credo AI's governance platform (again, if you are using it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c82684",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = lens.run_assessments()\n",
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b24c759",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['FairnessBase']['fairness']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e7f95",
   "metadata": {},
   "source": [
    "**Creating Reports**\n",
    "\n",
    "Reports can also be easily created. Note - not all assessments support reports! But if a report can be created, it will be with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e8d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs = lens.create_reports()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b38e6a",
   "metadata": {},
   "source": [
    "Example export to `~/credoai_test_location`. Uncomment to test.\n",
    "\n",
    "For export options straight to Credo AI's governance platform, see the Governance Integration notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52424513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example export\n",
    "# import os\n",
    "# output_directory = os.path.join(os.path.expanduser('~'), 'credoai_test_location')\n",
    "# lens.run_assessments(export=output_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
