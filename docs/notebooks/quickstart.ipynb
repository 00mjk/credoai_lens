{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96b4792f",
   "metadata": {},
   "source": [
    "# Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b4794e",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "CredoAI Lens can be installed with pip from PyPI as follows:\n",
    "\n",
    "`pip install credoai`\n",
    "\n",
    "Optional dependencies can be installed as follows:\n",
    "\n",
    "`pip install credoai[extras]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4203f75c",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "CredoAI Lens is the assessment framework component of the broader CredoAI suite.\n",
    "It is usable as a standalone gateway to assessments or in combination\n",
    "with CredoAI's Governance Platform. \n",
    "\n",
    "Understanding how your AI systems are operating is the most important step in intervening upon your system. From the technically complicated questions of improving a system to the business-relevant questions of whether to deploy a system - everything is fundamentally founded upon good observability. Lens strives to make assessment comprehensive, easy, and adaptable.\n",
    "\n",
    "### Assessments\n",
    "\n",
    "CredoAI Lens is an entry point to assessments developed by CredoAI, as well as the broader ecosystem of open-source assessments. Custom analytics can also be folded in easily (see the `lens customization` notebook)\n",
    "\n",
    "AI system assessment starts with verifying standard performance metrics to an evolving set of assessments falling under the banner of *Responsible AI*. A non-exhaustive list includes\n",
    "\n",
    "* Fairness\n",
    "* Explainability\n",
    "* Performance\n",
    "* Robustness\n",
    "\n",
    "These different categories of assessment differ substantially based on whether one is \n",
    "evaluating datasets or models, what kind of model (e.g., tabular, NLP, computer vision), and the use-case. As the ecosystem develops, Lens will support assessing a broader range of AI systems. Currently, we are focused on Fairness.\n",
    "\n",
    "### Governance\n",
    "\n",
    "While Lens is a stand-alone assessment framework, its value is increased when combined with the CredoAI Governance Platform. The platform supports multi-stakeholder `Alignment` on how to assess your AI systems (e.g., what does good look like for this system?). It also supports translating assessment results into a Risk perspective that is scalable across your organization and understandable to diverse stakeholders.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a99124",
   "metadata": {},
   "source": [
    "## Lens in 5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726fc511",
   "metadata": {},
   "source": [
    "Get your data and models together! Here we have a support vector machine trained on the Iris Dataset. No train/test split needed for this tutorial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9a8fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for example data and model training\n",
    "from credoai.data import fetch_creditdefault\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Base Lens imports\n",
    "import credoai.lens as cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "508d0a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_creditdefault(as_frame=True)\n",
    "X = data['data'].drop(columns=['SEX'])\n",
    "y = data['target']\n",
    "sensitive_feature = data['data']['SEX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a2d5874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ee700",
   "metadata": {},
   "source": [
    "### Using Lens\n",
    "\n",
    "Below is a basic example where our goal is to evaluate the fairness of the above model. We will rely on Lens defaults for this analysis.\n",
    "\n",
    "#### Preparing artifacts\n",
    "\n",
    "Lens interacts with Credo Artifacts which wrap models and datasets and standardizes them for use by different assessments.\n",
    "Below we create a `CredoModel` object, which automatically infers that the \"model\" object is from scikit-learn. We also create a `CredoData` object which is store X, y and sensitive features. Both of these objects are customizable. See `lens_customization.ipynb` for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a0d06dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "credo_model = cl.CredoModel(name='credit_default_classifier',\n",
    "                            model=model)\n",
    "\n",
    "credo_data = cl.CredoData(name='UCI-credit-default',\n",
    "                          X=X, \n",
    "                          y=y.astype(int),\n",
    "                          sensitive_features=sensitive_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d761776a-d7eb-403d-a954-4d89393c41e0",
   "metadata": {},
   "source": [
    "The functionality of these artifacts determines which assessments can be run. Lens can automatically determine which assessments are runnable. In this case the Dataset Assessment and FairnessBase Assessment can be run. You can see what assessments are runnable with the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18c8eb6e-8e0a-4999-8dd1-5e45e42f1216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dataset': <credoai.assessment.assessments.DatasetAssessment at 0x16666e8b0>,\n",
       " 'FairnessBase': <credoai.assessment.assessments.FairnessBaseAssessment at 0x16666ebe0>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from credoai.assessment import get_usable_assessments\n",
    "get_usable_assessments(credo_model, credo_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88195b3-b501-499b-8429-d1093f16cdbb",
   "metadata": {},
   "source": [
    "You can also list all assesments, along with their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1000054a-e63b-4f5b-b4ff-19e5fa3e7b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DatasetAssessment': 'Dataset',\n",
       " 'FairnessBaseAssessment': 'FairnessBase',\n",
       " 'NLPEmbeddingBiasAssessment': 'NLPEmbeddingBias',\n",
       " 'NLPGeneratorAssessment': 'NLPGenerator'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from credoai.assessment.utils import get_assessment_names\n",
    "get_assessment_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd20b46",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Alignment Spec\n",
    "\n",
    "The Alignment Spec describes how the assessments should be run at a high level. It should be thought of as the output of a multi-stakeholder alignment session on \"how we should assess this AI system\". Another way to think about it is as the *parameterization* of the assessments Lens will run.\n",
    "\n",
    "If you use the Credo AI Governance Platform, the alignment spec is a principle artifact determined during the *Alignment Phase*. It is the output of multi-stakeholder collaboration. Lens will automatically download the Alignment Spec associated with your governance credentials (which uses another artifact: `CredoGovernance`)\n",
    "\n",
    "The Alignment Spec retrieved from the Governance Platform is a *subset* of all possible parameterizations of the different assessments you can run. Some parameters are too \"in the weeds\" for multistakeholder collaboration! However, you have full control of the assessments by manually defining the `spec`. Anything defined in the spec parameter will take precedence over the spec retrieved from the Governance Platform.\n",
    "\n",
    "**Setting up the Spec**\n",
    "\n",
    "The alignment spec is a set of {assessment_name: parameter} pairs. The assessment name must be the name of one of the assessments, as returned by `get_usable_assessments` (above). In general, the name will be the name of the method without the trailing \"assessment\". For example, FairnessBaseAssessment -> \"FairnessBase\". `get_assessment_names` will tell you the names you need.\n",
    "\n",
    "Not all assessments *require* a spec, though many can be customized. In the case of \"FairnessBase\" a spec defining a list of metrics **is** required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6abc926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the metrics that will be used by the FairnessBase assessment\n",
    "alignment_spec = {\n",
    "    'FairnessBase': {'metrics': ['precision_score']}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31051601",
   "metadata": {},
   "source": [
    "#### Run Lens\n",
    "\n",
    "Once we have the model and data artifacts, as well as the spec, we can run Lens. By default it will automatically infer which assessments to run, just as we manually did above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74680dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               spec=alignment_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d019e65",
   "metadata": {},
   "source": [
    "Lens has one primary method: `run_assessments`\n",
    "\n",
    "`run_assessments` outputs the results into a dictionary that can be used for further processing. If the `export` keyword is used, it can also export the data to a json or straight to Credo AI's governance platform (again, if you are using it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af115bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Dataset', 'FairnessBase'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = lens.run_assessments()\n",
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70acc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['FairnessBase']['fairness']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e7f880",
   "metadata": {},
   "source": [
    "Example export to `~/credoai_test_location`. Uncomment to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2202ad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example export\n",
    "# import os\n",
    "# output_directory = os.path.join(os.path.expanduser('~'), 'credoai_test_location')\n",
    "# lens.run_assessments(export=output_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
