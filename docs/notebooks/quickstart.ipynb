{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d608c2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Quickstart\n",
    "\n",
    "Get started here. We will assess a payment default prediction model for gender fairness using Lens, in 5 minutes. More in-depth information can be found in the [lens FAQ](https://credoai-lens.readthedocs.io/en/latest/notebooks/lens_faq.html#How-can-I-choose-which-assessments-to-run?)\n",
    "\n",
    "## Setup\n",
    "\n",
    "Setup instruction can be found on [readthedocs](https://credoai-lens.readthedocs.io/en/stable/setup.html)\n",
    "\n",
    "**Find the code**\n",
    "\n",
    "This notebook can be found on [github](https://github.com/credo-ai/credoai_lens/blob/develop/docs/notebooks/quickstart.ipynb).\n",
    "\n",
    "**Data + Model Preparation (before Lens)**\n",
    "\n",
    "Some quick setup. This script reflects all of your datascience work before assessment and integration with Credo AI.\n",
    "\n",
    "Here we have a gradient boosted classifier trained on the UCI Credit Card Default Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f914dde-164c-40e0-8dc3-f0f5e00121ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and df are defined by this script\n",
    "%run training_script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7557a63e-a7d8-4ade-9f71-71416d1abe2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1261860b-4ec3-42e1-872a-d357e22bd8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Lens imports\n",
    "import credoai.lens as cl\n",
    "# set default format for image displays. Change to 'png' if 'svg' is failing\n",
    "%config InlineBackend.figure_formats = ['svg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0797a51",
   "metadata": {},
   "source": [
    "## Lens in 5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce779fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "Below is a basic example where our goal is to evaluate the above model. We will rely on Lens defaults for this analysis, which will automatically determine the assessments that can be run.\n",
    "\n",
    "We'll break these down [below](#Breaking-Down-The-Steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4e6a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up model and data artifacts\n",
    "credo_model = cl.CredoModel(name='credit_default_classifier',\n",
    "                            model=model)\n",
    "\n",
    "credo_data = cl.CredoData(name='UCI-credit-default',\n",
    "                          data=df,\n",
    "                          sensitive_feature_keys=['SEX'],\n",
    "                          label_key='target'\n",
    "                          )\n",
    "\n",
    "# specify the metrics that will be used by the Fairness and Performance assessment\n",
    "metrics = ['precision_score', 'recall_score', 'equal_opportunity']\n",
    "assessment_plan = {'Fairness': {'metrics': metrics}}\n",
    "# run lens\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               assessment_plan=assessment_plan)\n",
    "               \n",
    "# first we run the assessments\n",
    "# note that we use method chaining to make code more readable\n",
    "lens.run_assessments().display_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b8dd11-bc17-42b4-b73f-a42324f31780",
   "metadata": {},
   "source": [
    "## In-Depth Overview\n",
    "\n",
    "CredoAI Lens is the assessment framework component of the broader CredoAI suite.\n",
    "It is usable as a standalone gateway to assessments or in combination\n",
    "with CredoAI's Governance App. \n",
    "\n",
    "Understanding how your AI systems are operating is the most important step in intervening upon your system. From the technically complicated questions of improving a system to the business-relevant questions of whether to deploy a system - everything is fundamentally founded upon good observability. Lens strives to make assessment comprehensive, easy, and adaptable. The primary outputs from Lens are **assessment results** in the form of various metrics. Lens also can visualize some of these results.\n",
    "\n",
    "### Assessments\n",
    "\n",
    "CredoAI Lens is an entry point to assessments developed by CredoAI, as well as the broader ecosystem of open-source assessments. Custom analytics can also be folded in easily (see the `lens customization` notebook)\n",
    "\n",
    "AI system assessment starts with verifying standard performance metrics to an evolving set of assessments falling under the banner of *Responsible AI*. A non-exhaustive list includes\n",
    "\n",
    "* Fairness\n",
    "* Explainability\n",
    "* Performance\n",
    "* Robustness\n",
    "\n",
    "These different categories of assessment differ substantially based on whether one is \n",
    "evaluating datasets or models, what kind of model (e.g., tabular, NLP, computer vision), and the use-case. As the ecosystem develops, Lens will support assessing a broader range of AI systems. Currently, we are focused on Fairness.\n",
    "\n",
    "### Governance\n",
    "\n",
    "While Lens is a stand-alone assessment framework, its value is increased when combined with the CredoAI Governance App. The app supports multi-stakeholder `Alignment` on how to assess your AI systems (e.g., what does good look like for this system?). It also supports translating assessment results into a Risk perspective that is scalable across your organization and understandable to diverse stakeholders.\n",
    "\n",
    "Check out the [Connecting with Governance App](https://credoai-lens.readthedocs.io/en/latest/notebooks/governance_integration.html) for information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c05dfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Breaking Down The Steps\n",
    "\n",
    "### Preparing artifacts\n",
    "\n",
    "Lens interacts with Credo Artifacts which wrap models and datasets and standardizes them for use by different assessments.\n",
    "Below we create a `CredoModel` object, which automatically infers that the \"model\" object is from scikit-learn. We also create a `CredoData` object which is store X, y and sensitive features. Both of these objects are customizable. See `lens_customization.ipynb` for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f1fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "credo_model = cl.CredoModel(name='credit_default_classifier',\n",
    "                            model=model)\n",
    "\n",
    "credo_data = cl.CredoData(name='UCI-credit-default',\n",
    "                          data=df,\n",
    "                          sensitive_feature_keys=['SEX'],\n",
    "                          label_key='target'\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114bd0bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### CredoModel\n",
    "\n",
    "CredoModel serves as an adapter between arbitrary models and the assessments in CredoLens. Assessments depend on CredoModel instantiating certain methods. In turn, the methods an instance of CredoModel defines informs Lens which assessment can be automatically run.\n",
    "\n",
    "The way a CredoModel works is by defining a \"config\" dictionary that outlines the models functionality.\n",
    "\n",
    "Above the CredoModel functionality was inferred from the fact that the model (GraidentBoostingClassifier) is a scikit-learn model. But under the hood all that happens was it defined a `config`.\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b180fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the config was inferred from the model passed to CredoModel\n",
    "credo_model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7fb09b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### CredoData\n",
    "\n",
    "Just as CredoModel is an adapter between arbitrary models and the Lens assessment framework, CredoData serves as an adapter between tabular datasets and the assessments in CredoLens.\n",
    "\n",
    "When you pass a dataframe to CredoData, CredoData separates it into an \"X\", \"y\", and, if applicable, \"sensitive_features\".\n",
    "\n",
    "You can pass CredoData to Lens as a training dataset or a validation dataset. If the former, it will not be used to assess the model. Instead, dataset assessments will be performed on the dataset (e.g., fairness assessment). The validation dataset will be assessed in the same way, but _also_ used to assess the model, if provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc23c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "credo_data = cl.CredoData(name='UCI-credit-default',\n",
    "                          data=df,\n",
    "                          sensitive_feature_keys=['SEX'],\n",
    "                          label_key='target'\n",
    "                          )\n",
    "credo_data.X.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4583249",
   "metadata": {},
   "source": [
    "### Assessments \n",
    "\n",
    "Lens uses the functionality of the above artifacts to automatically determine which assessments can be run. In this case the Dataset Assessment and Fairness Assessment can be run. You can see what assessments are runnable with the following function.\n",
    "\n",
    "Assessments can be chosen, rather than inferred. See the [lens FAQ](https://credoai-lens.readthedocs.io/en/latest/notebooks/lens_faq.html#How-can-I-choose-which-assessments-to-run?) for this functionality, and other information about assessments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add5bf3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Assessment Plan\n",
    "\n",
    "The Assessment Plan describes how the assessments should be run. Think about it is as the *parameterization* of the assessments Lens will run.\n",
    "\n",
    "If you use the Credo AI Governance App, the Assessment Plan is a principle artifact determined during the *Alignment Phase*. It is the output of multi-stakeholder collaboration. Lens will automatically download the Assessment Plan associated with your governance credentials (which uses another artifact: `CredoGovernance`)\n",
    "\n",
    "You can also define the plan in code. Anything defined in the `assessment_plan` parameter will take precedence over the Assessment Plan retrieved from the Governance App.\n",
    "\n",
    "**Setting up the Plan**\n",
    "\n",
    "The Assessment Plan is a set of {assessment_name: parameter} pairs. The assessment name must be the name of one of the assessments, as returned by `get_usable_assessments` (above). In general, the name will be the name of the method without the trailing \"assessment\". For example, FairnessAssessment -> \"Fairness\". `get_assessment_names` will tell you the names you need.\n",
    "\n",
    "The plan's parameters are passed to each Assessments `init_module` function.\n",
    "\n",
    "Not all assessments *require* a plan, though many can be customized. In the case of \"Performance\" and \"Fairness\", a plan defining a list of metrics should be supplied, though default metrics will be defined for regression/classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eed596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the metrics that will be used by the Fairness assessment\n",
    "assessment_plan = {\n",
    "    'Fairness': {'metrics': ['precision_score']},\n",
    "    'Performance': {'metrics': ['precision_score']}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb3c5b8",
   "metadata": {},
   "source": [
    "### Run Lens\n",
    "\n",
    "Once we have the model and data artifacts, as well as the spec, we can run Lens. By default it will automatically infer which assessments to run, just as we manually did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc76430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               assessment_plan=assessment_plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5dcbe3",
   "metadata": {},
   "source": [
    "**Getting Assessment Results**\n",
    "\n",
    "To run the assessments with Lens, call `run_assessments`\n",
    "\n",
    "`run_assessments` outputs the results into a dictionary that can be used for further processing. You can also export the data to a json or straight to Credo AI's Governance App by calling `lens.export()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27253691",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = lens.run_assessments().get_results()\n",
    "results['validation'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b82a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the fairness results, from the Fairness assessment, run on the validation dataset\n",
    "results['validation_model']['Fairness']['fairness']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73d0dc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualizing assessments\n",
    "\n",
    "Assessments aren't much if you can't visualize them. Lens allows you to visualize your results easily.\n",
    "\n",
    "**Displaying Plots**\n",
    "\n",
    "If you'd like to display the plots in your active jupyter notebook, set `display_results` to True. That's what we did at the top of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3bfe1a",
   "metadata": {},
   "source": [
    "**Exporting assessments To Credo AI's Governance App**\n",
    "\n",
    "Finally, the assessments can also be exported to Credo AI's Governance App. Check out the [Connecting with Governance App](https://credoai-lens.readthedocs.io/en/latest/notebooks/governance_integration.html) tutorial for directions.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d7eb8a87bb83596f4cd5aeb66d856dad2a9bb65fe804cea051250e36746a46f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
