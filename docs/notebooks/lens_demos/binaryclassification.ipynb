{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification with Lens\n",
    "\n",
    "### Find the code\n",
    "This notebook can be found on [github](https://github.com/credo-ai/credoai_lens/blob/develop/docs/notebooks/lens_demos/binaryclassification.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.lens import Lens, CredoModel, CredoData, CredoGovernance\n",
    "from credoai.assessment import FairnessBaseAssessment, DatasetAssessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [What is Covered](#What-is-Covered)\n",
    "1. [Introduction](#Introduction)\n",
    "1. [The UCI Credit-card Default Dataset](#The-UCI-Credit-Card-Default-Dataset)\n",
    "1. [Model training](#Train-the-model)\n",
    "1. [Binary Classification with Lens](#Running-Lens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Covered\n",
    "\n",
    "* **Domain:**\n",
    "  * Finance (loan decisions). The data is semisynthetic to create a simple example of disparities in FPR and FNR.\n",
    "\n",
    "* **ML task:**\n",
    "  * Binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we consider a scenario where algorithmic tools are deployed to predict the likelihood that an applicant will default on a credit card loan. The notebook emulates the problem presented in this [white paper](https://www.microsoft.com/en-us/research/uploads/prod/2020/09/Fairlearn-EY_WhitePaper-2020-09-22.pdf) in collaboration with EY.\n",
    "\n",
    "Due to data privacy, we do not use the data from the white paper. Instead, we use the [UCI Credit-card default dataset](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients), a toy dataset reflecting credit-card defaults in Taiwan, as a substitute dataset to replicate the desired workflow. To make this dataset applicable to our problem, we introduce a synthetic feature that is highly predictive for applicants defined as \"female\" in terms of the \"gender\" feature, but is uninformative for applicants defined as \"male\".\n",
    "\n",
    "We train an algorithm on this dataset and show the model has a higher false-positive rate as well as a higher false-negative rate for the \"male\" group than for the \"female\" group using the Credo Fairness Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pprint import pprint\n",
    "# Data processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import GradientBoostingClassifier as gbc\n",
    "\n",
    "# data\n",
    "from credoai.data import fetch_creditdefault"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "### The UCI Credit-Card Default Dataset\n",
    "\n",
    "The UCI dataset contains data on 30,000 clients and their credit card transactions at a bank in Taiwan. In addition to static client features, the dataset contains the history of credit card bill payments between April and September 2005, as well as the balance limit of the client's credit card. The target is whether the client will default on a card payment in the following month, October 2005. A model trained on this data could be used, in part, to determine whether a client is eligible for another loan or a credit increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_creditdefault()\n",
    "dataset = pd.concat([data['data'], data['target']], axis=1)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset columns:\n",
    "\n",
    "* `LIMIT_BAL`: credit card limit, will be replaced by a synthetic feature\n",
    "* `gender, EDUCATION, MARRIAGE, AGE`: client demographic features\n",
    "* `BILL_AMT[1-6]`: amount on bill statement for April-September\n",
    "* `PAY_AMT[1-6]`: payment amount for April-September\n",
    "* `default payment next month`: target, whether the client defaulted the following month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the sensitive feature\n",
    "gender = dataset[\"SEX\"]\n",
    "# Extract the target\n",
    "Y = dataset[\"default payment next month\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduce a Synthetic Feature\n",
    "\n",
    "We manipulate the balance-limit feature `LIMIT_BAL` to make it highly predictive for the \"female\" group but not for the \"male\" group. Specifically, we set this up, so that a lower credit limit indicates that a female client is less likely to default, but provides no information on a male client's probability of default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_scale = 0.5\n",
    "np.random.seed(12345)\n",
    "# Make 'LIMIT_BAL' informative of the target\n",
    "dataset['LIMIT_BAL'] = Y + np.random.normal(scale=dist_scale, size=dataset.shape[0])\n",
    "# But then make it uninformative for the male clients\n",
    "dataset.loc[gender==\"male\", 'LIMIT_BAL'] = np.random.normal(scale=dist_scale, size=dataset[gender==\"male\"].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['LIMIT_BAL'][(gender==1) & (Y==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "# Plot distribution of LIMIT_BAL for men\n",
    "dataset['LIMIT_BAL'][(gender==\"male\") & (Y==0)].plot(kind='kde', label=\"Payment on time\", ax=ax1, \n",
    "                                           title=\"LIMIT_BAL distribution for \\\"male\\\" group\")\n",
    "dataset['LIMIT_BAL'][(gender==\"male\") & (Y==1)].plot(kind='kde', label=\"Default\", ax=ax1)\n",
    "# Plot distribution of LIMIT_BAL for women\n",
    "dataset['LIMIT_BAL'][(gender==\"female\") & (Y==0)].plot(kind='kde', label=\"Payment on time\", ax=ax2, \n",
    "                                           legend=True, title=\"LIMIT_BAL distribution for \\\"female\\\" group\")\n",
    "dataset['LIMIT_BAL'][(gender==\"female\") & (Y==1)].plot(kind='kde', label=\"Default\", ax=ax2, \n",
    "                                           legend=True).legend(bbox_to_anchor=(1.6, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in the above figures that the new `LIMIT_BAL` feature is indeed highly predictive for the \"female\" group, but not for the \"male\" group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, Y_train, Y_test, gender_train, gender_test = train_test_split(\n",
    "    dataset.drop(columns=['SEX', 'default payment next month']), \n",
    "    Y, \n",
    "    gender, \n",
    "    test_size = 0.3, \n",
    "    random_state=12345,\n",
    "    stratify=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "This is a \"fairness unaware\" model. This means it is trained with no consideration to fairness or protected attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train an out-of-the-box `gradient boosted classifier` model on the modified data and assess several fairness metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gbc()\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores on test set\n",
    "test_scores = model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions (0 or 1) on test set\n",
    "test_preds = (test_scores >= np.mean(Y_train)) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.modules import FairnessModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = FairnessModule(['precision_score', 'roc_auc_score', 'recall_score', 'demographic_parity_ratio'],\n",
    "               gender_test, Y_test, test_preds, test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = mod.run().get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out['fairness']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Governance credentials allow Lens to connect directly to the Credo AI Governance Platform. One important function of this connection is to get the \"Alignment Spec\" - the set of instructions for how the model should be assessed. \n",
    "\n",
    "Another function is to export assessment results to be incorporated in your AI Governance operations.\n",
    "\n",
    "For this demo, we assume you do _not_ have access to Credo AI's governance platform. In this case, the Alignment Spec must be created manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# governance information\n",
    "# used to connect to Credo AI Alignment\n",
    "# uncomment and fill in to see how\n",
    "# governance -> automatic alignment spec\n",
    "# incorporation\n",
    "gov = CredoGovernance(\n",
    "    ai_solution_id = \"{example_id}\",\n",
    "    model_id = \"{example_id}\",\n",
    "    data_id = \"\"\n",
    ")\n",
    "\"\"\"\n",
    "alignment_spec = {\n",
    "    'FairnessBase': {'metrics': ['recall_score', 'demographic_parity_ratio']}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_data = CredoData(\n",
    "    name='UCI_dataset', \n",
    "    X=X_test, \n",
    "    y=Y_test, \n",
    "    sensitive_features=gender_test,\n",
    "    metadata={'stage': 'validation'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model config is a dictionary that defines the \n",
    "# functions that will be used for assessment. \n",
    "# These functions nclude functions that take in data,\n",
    "# as well as functions that reveal internal aspects\n",
    "# of the model (e.g., embeddings, feature importances)\n",
    "\n",
    "# the config is then passed to CredoModel,\n",
    "# a wrapper that instantiates the functionality\n",
    "# defined in the config\n",
    "model_config = {'pred_fun': model.predict,\n",
    "                'prob_fun': model.predict_proba}\n",
    "credo_model = CredoModel(name='model_test', \n",
    "                         model_config=model_config)\n",
    "\n",
    "# If the model config can be inferred directly from \n",
    "# the model (as in the second example above),\n",
    "# you can pass the model straight\n",
    "# to CredoModel as the model_config\n",
    "credo_model = CredoModel(name='test', \n",
    "                         model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = Lens(spec=alignment_spec,\n",
    "            assessments=[FairnessBaseAssessment()],\n",
    "            model=credo_model,\n",
    "            data=assessment_data\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_results = lens.run_assessments(export=False)\n",
    "fairness_results = assessment_results['FairnessBase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Fairness Assesssent returns two dataframes\n",
    "fairness_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_results['fairness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_results['disaggregated_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = lens.create_reports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d7eb8a87bb83596f4cd5aeb66d856dad2a9bb65fe804cea051250e36746a46f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
