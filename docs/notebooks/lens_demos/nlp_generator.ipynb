{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48b2cf0c-9a00-46e7-b59d-4591c2aeface",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Language Generation Models Assessment with Lens\n",
    "\n",
    "### Find the code\n",
    "This notebook can be found on [github](https://github.com/credo-ai/credoai_lens/blob/develop/docs/notebooks/lens_demos/nlp_generator_demo.ipynb).\n",
    "\n",
    "### Install required packages\n",
    "\n",
    "This package requires extra installations. Please run\n",
    "```\n",
    "pip install credoai-lens[extras]\n",
    "```\n",
    "\n",
    "## Overview\n",
    "Language generation models generate meaningful text when prompted with a sequence of words as context. They empower many modern downstream applications, such as chatbots.\n",
    "\n",
    "Language Generation Models Assessment tool enables the assessment of a generation model for a text attribute (toxicity, profanity, etc.) and the interaction of that text attribute with a sensitive attribute. \n",
    "\n",
    "It assesses the responses generated by the model to prompts and returns the text attribute levels across groups (e.g., Islam and Christianity). The tool has multiple prompts datasets and assessment models built in, but is also highly customizable and allows a user to use their own datasets and models when desired. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8423743-017a-4b8e-abb7-85bac0c96fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.lens import Lens, CredoModel, CredoData\n",
    "from credoai.assessment import NLPGeneratorAssessment\n",
    "from credoai.utils.nlp_utils import gpt1_text_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4514f6-7c55-4284-ae29-592e839e375a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Assessing GPT-1\n",
    "\n",
    "For this assessment the CredoModel must be supplied with a `generator_fun`. Here we use a built in function, which uses GPT-1 to generate text.\n",
    "\n",
    "The default parametrization of the assessment uses gpt2 as a comparison model. It runs on the `bold_religious_ideology` prompt dataset, which takes too long, so we are replacing it for this demo with the `realtoxicityprompts_challenging_20` dataset.\n",
    "\n",
    "\n",
    "\n",
    "The default assessment function is Credo AI's local toxicity assessment model. This model is <strong>basic and for demo purposes only</strong> -- a [logistic regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) pretrained on a dataset of nearly 30,000 human-labeled comments ([Davidson et al.](https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master/data) and [Zampieri et al.](https://sites.google.com/site/offensevalsharedtask/olid)). It uses [Sentence Transformers](https://www.sbert.net/docs/pretrained_models.html) for encoding. We encourage using Perspective API for your assessment purposes, which is supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6157f745-5625-4f23-b2f1-144fe1fe4d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different aspects of documentation you may be interested in\n",
    "from credoai.assessment import NLPGeneratorAssessment\n",
    "\n",
    "# what parameters can be passed to the initialization?\n",
    "NLPGeneratorAssessment.init_module?\n",
    "\n",
    "# what requirements are needed? \n",
    "# (This is normally included in the assessments base documentation)\n",
    "assessment = NLPGeneratorAssessment()\n",
    "assessment.get_requirements()\n",
    "\n",
    "# what does the module require? \n",
    "# This is often similar to the parameters passed to assessment initialization\n",
    "assessment.module?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c4e33b-24e0-415c-9b9f-054115e1c3ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running Lens\n",
    "\n",
    "Running Lens for assessing NLP generators is similar to running Lens for binary classification. The assesssment spec is different given the different assessment and the CredoModel requires a generator function. Give it a go!\n",
    "\n",
    "Note, this assessment takes substantially longer than the binary classification assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d16a70-1428-4d1e-ac81-f1d2be554395",
   "metadata": {},
   "outputs": [],
   "source": [
    "credo_model = CredoModel(name='gpt1',\n",
    "                         model_config = {'generator_fun': gpt1_text_generator})\n",
    "\n",
    "assessment_spec = {'NLPGenerator': {'prompts': 'realtoxicityprompts_challenging_20'}}\n",
    "lens = Lens(model = credo_model,\n",
    "            spec = assessment_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad656d-f8b8-4607-874a-e736e55a93af",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = lens.run_assessments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c031f5a-88b6-4a0b-8c1c-5848c4e860e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens.create_reports()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
