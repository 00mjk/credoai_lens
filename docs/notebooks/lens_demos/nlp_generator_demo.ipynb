{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48b2cf0c-9a00-46e7-b59d-4591c2aeface",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NLP Generator Analysis with Lens\n",
    "\n",
    "### Find the code\n",
    "This notebook can be found on [github](https://github.com/credo-ai/credoai_lens/blob/develop/docs/notebooks/lens_demos/nlp_generatr_demo.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8423743-017a-4b8e-abb7-85bac0c96fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.lens import Lens, CredoModel, CredoData\n",
    "from credoai.assessment import NLPGeneratorAssessment\n",
    "from credoai.utils.nlp_utils import gpt1_text_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9d16a70-1428-4d1e-ac81-f1d2be554395",
   "metadata": {},
   "outputs": [
    {
     "ename": "InstallationError",
     "evalue": "To use default assessment functions requires installing credoai-lens[extras]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Credo/credoai_lens/credoai/assessment/assessments.py:145\u001b[0m, in \u001b[0;36mNLPGeneratorAssessment.init_module\u001b[0;34m(self, model, data, prompts, assessment_functions, comparison_models)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     assessment_functions \u001b[38;5;241m=\u001b[39m \u001b[43mcutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_nlp_assessments\u001b[49m()\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'credoai.utils' has no attribute 'get_default_nlp_assessments'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInstallationError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m credo_model \u001b[38;5;241m=\u001b[39m CredoModel(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt1\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      2\u001b[0m            model_config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerator_fun\u001b[39m\u001b[38;5;124m'\u001b[39m: gpt1_text_generator})\n\u001b[1;32m      4\u001b[0m assessment_spec \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNLPGenerator\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompts\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrealtoxicityprompts_challenging_20\u001b[39m\u001b[38;5;124m'\u001b[39m}}\n\u001b[0;32m----> 5\u001b[0m lens \u001b[38;5;241m=\u001b[39m \u001b[43mLens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcredo_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43massessment_spec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Credo/credoai_lens/credoai/lens.py:269\u001b[0m, in \u001b[0;36mLens.__init__\u001b[0;34m(self, governance, spec, assessments, model, data, user_id, init_assessment_kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspec\u001b[38;5;241m.\u001b[39mupdate(spec)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# initialize\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_assessments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Credo/credoai_lens/credoai/lens.py:427\u001b[0m, in \u001b[0;36mLens._init_assessments\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m assessment \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massessments\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    426\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspec\u001b[38;5;241m.\u001b[39mget(assessment\u001b[38;5;241m.\u001b[39mname, {})\n\u001b[0;32m--> 427\u001b[0m     \u001b[43massessment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Credo/credoai_lens/credoai/assessment/assessments.py:147\u001b[0m, in \u001b[0;36mNLPGeneratorAssessment.init_module\u001b[0;34m(self, model, data, prompts, assessment_functions, comparison_models)\u001b[0m\n\u001b[1;32m    145\u001b[0m         assessment_functions \u001b[38;5;241m=\u001b[39m cutils\u001b[38;5;241m.\u001b[39mget_default_nlp_assessments()\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InstallationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use default assessment functions requires installing credoai-lens[extras]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# set up generation functions\u001b[39;00m\n\u001b[1;32m    150\u001b[0m generation_functions \u001b[38;5;241m=\u001b[39m {model\u001b[38;5;241m.\u001b[39mname: model\u001b[38;5;241m.\u001b[39mgenerator_fun}\n",
      "\u001b[0;31mInstallationError\u001b[0m: To use default assessment functions requires installing credoai-lens[extras]"
     ]
    }
   ],
   "source": [
    "credo_model = CredoModel(name='gpt1',\n",
    "           model_config = {'generator_fun': gpt1_text_generator})\n",
    "\n",
    "assessment_spec = {'NLPGenerator': {'prompts': 'realtoxicityprompts_challenging_20'}}\n",
    "lens = Lens(model = credo_model,\n",
    "            spec = assessment_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad656d-f8b8-4607-874a-e736e55a93af",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = lens.run_assessments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e29975e-c0c7-4c64-86c3-87742a971529",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens.create_reports()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
