{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [What is Covered](#What-is-Covered)\n",
    "1. [Introduction](#Introduction)\n",
    "1. [The UCI Credit-card Default Dataset](#The-UCI-Credit-Card-Default-Dataset)\n",
    "1. [Model training](#Train-the-model)\n",
    "1. [Credo Binary Classification Fairness Module](#Credo-Fairness-Module)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Covered\n",
    "\n",
    "* **Domain:**\n",
    "  * Finance (loan decisions). The data is semisynthetic to create a simple example of disparities in FPR and FNR.\n",
    "\n",
    "* **ML task:**\n",
    "  * Binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we consider a scenario where algorithmic tools are deployed to predict the likelihood that an applicant will default on a credit-card loan. The notebook emulates the problem presented in this [white paper](https://www.microsoft.com/en-us/research/uploads/prod/2020/09/Fairlearn-EY_WhitePaper-2020-09-22.pdf) in collaboration with EY.\n",
    "\n",
    "Due to data privacy, we do not use the data from the white paper. Instead, we use the [UCI Credit-card default dataset](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients), a toy dataset reflecting credit-card defaults in Taiwan, as a substitute dataset to replicate the desired workflow. To make this dataset applicable to our problem, we introduce a synthethic feature that is highly predictive for applicants defined as \"female\" in terms of the \"sex\" feature, but is uninformative for applicants defined as \"male\".\n",
    "\n",
    "We train an algorithm on this dataset and show the model has a higher false-positive rate as well as a higher false-negative rate for the \"male\" group than for the \"female\" group using the Credo Fairness Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pprint import pprint\n",
    "# Data processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import GradientBoostingClassifier as gbc\n",
    "\n",
    "# Credo Fairness Module\n",
    "from credoai.assessment.model_modules.fairness_base import FairnessModule, list_metrics\n",
    "# Fairness Reporting\n",
    "from credoai.reporting.fairness_binaryclassification import FairnessReport\n",
    "# get aligned metrics from governance platform\n",
    "from credoai.utils.credo_api_utils import get_aligned_metrics\n",
    "from credoai.data import fetch_creditdefault"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "### The UCI Credit-Card Default Dataset\n",
    "\n",
    "The UCI dataset contains data on 30,000 clients and their credit card transactions at a bank in Taiwan. In addition to static client features, the dataset contains the history of credit card bill payments between April and September 2005, as well as the balance limit of the client's credit card. The target is whether the client will default on a card payment in the following month, October 2005. A model trained on this data could be used, in part, to determine whether a client is eligible for another loan or a credit increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_creditdefault(as_frame=True)\n",
    "dataset = pd.concat([data['data'], data['target']], axis=1)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset columns:\n",
    "\n",
    "* `LIMIT_BAL`: credit card limit, will be replaced by a synthetic feature\n",
    "* `SEX, EDUCATION, MARRIAGE, AGE`: client demographic features\n",
    "* `BILL_AMT[1-6]`: amount on bill statement for April-September\n",
    "* `PAY_AMT[1-6]`: payment amount for April-September\n",
    "* `default payment next month`: target, whether the client defaulted the following month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the sensitive feature\n",
    "A = dataset[\"SEX\"]\n",
    "A_str = A.map({ 2:\"female\", 1:\"male\"})\n",
    "# Extract the target\n",
    "Y = dataset[\"default payment next month\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduce a Synthetic Feature\n",
    "\n",
    "We manipulate the balance-limit feature `LIMIT_BAL` to make it highly predictive for the \"female\" group but not for the \"male\" group. Specifically, we set this up, so that a lower credit limit indicates that a female client is less likely to default, but provides no information on a male client's probability of default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_scale = 0.5\n",
    "np.random.seed(12345)\n",
    "# Make 'LIMIT_BAL' informative of the target\n",
    "dataset['LIMIT_BAL'] = Y + np.random.normal(scale=dist_scale, size=dataset.shape[0])\n",
    "# But then make it uninformative for the male clients\n",
    "dataset.loc[A==1, 'LIMIT_BAL'] = np.random.normal(scale=dist_scale, size=dataset[A==1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "# Plot distribution of LIMIT_BAL for men\n",
    "dataset['LIMIT_BAL'][(A==1) & (Y==0)].plot(kind='kde', label=\"Payment on time\", ax=ax1, \n",
    "                                           title=\"LIMIT_BAL distribution for \\\"male\\\" group\")\n",
    "dataset['LIMIT_BAL'][(A==1) & (Y==1)].plot(kind='kde', label=\"Default\", ax=ax1)\n",
    "# Plot distribution of LIMIT_BAL for women\n",
    "dataset['LIMIT_BAL'][(A==2) & (Y==0)].plot(kind='kde', label=\"Payment on time\", ax=ax2, \n",
    "                                           legend=True, title=\"LIMIT_BAL distribution for \\\"female\\\" group\")\n",
    "dataset['LIMIT_BAL'][(A==2) & (Y==1)].plot(kind='kde', label=\"Default\", ax=ax2, \n",
    "                                           legend=True).legend(bbox_to_anchor=(1.6, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in the above figures that the new `LIMIT_BAL` feature is indeed highly predictive for the \"female\" group, but not for the \"male\" group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, Y_train, Y_test, A_train, A_test, A_str_train, A_str_test = train_test_split(\n",
    "    dataset.drop(columns=['SEX', 'default payment next month']), \n",
    "    Y, \n",
    "    A, \n",
    "    A_str,\n",
    "    test_size = 0.3, \n",
    "    random_state=12345,\n",
    "    stratify=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "This is a \"fairness unaware\" model. This means it is trained with no consideration to fairness or protected attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train an out-of-the-box `graident boosted classifier` model on the modified data and assess several fairness metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gbc()\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores on test set\n",
    "test_scores = model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions (0 or 1) on test set\n",
    "test_preds = (test_scores >= np.mean(Y_train)) * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credo Fairness Module\n",
    "\n",
    "The Credo Fairness Module simplifies moving from metric alignment to analysis. The module supports three main functions:\n",
    "\n",
    "* translates metrics into fairness analysis, including risk assessment based on thresholds\n",
    "* visualizing fairness metrics\n",
    "* sending metrics to Credo AI\n",
    "\n",
    "Many metrics are easily accessible in the module. `credo.fairness_base.list_metrics` gives all of them. You can also extend the functionality with [custom metrics](##Custom-metrics-and-incorporating-confidence-intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_metrics = ['equalized odds difference',\n",
    "                   'false positive rate',\n",
    "                   'false negative rate']\n",
    "fairness_bounds = {'equalized odds difference': (-.1, .1)}\n",
    "performance_bounds = {'false positive rate': (0, 0.15)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment = FairnessModule(aligned_metrics, \n",
    "                A_str_test,\n",
    "                Y_test,\n",
    "                test_preds,\n",
    "                test_scores,\n",
    "                fairness_bounds,\n",
    "                performance_bounds\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get performance metrics disaggregated across senstive features\n",
    "assessment.get_disaggregated_results(calculate_risk=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get fairness metrics which include metrics like \"average odds\"\n",
    "# and difference metrics for any performance metric supplied\n",
    "#\n",
    "# Note the \"method\" argument which defines how fairness metrics are calculated\n",
    "assessment.get_fairness_results(calculate_risk=True, method='between_groups')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting\n",
    "\n",
    "Reports are easily generated. These reports include plots focused on comparing metrics, as well as the creation of disaggregated infographics which summarize performance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = FairnessReporter(assessment, (4,4), size=4)\n",
    "report.create_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom metrics and incorporating confidence intervals\n",
    "\n",
    "Custom metrics can be incorporated by creating a `FairnessFunction`. The `FairnessFunction` is a lightweight wrapper class that defines a few characteristics of the custom function needed by the module. \n",
    "\n",
    "**Example: Confidence Intervals**\n",
    "\n",
    "Confidence intervals are not generically supported. However, they can be derived for metrics derived from a confusion matrix using the `wilson confidence interval`. A convenience function called `confusion_wilson` is supplied which returns an array: [lower, upper] bound for the metric. \n",
    "\n",
    "However, the module is not designed to accomodate metrics that return arrays. If you wish to incorporate CIs, we'd suggest creating a derived function that returns the lower bound, for example, and then wrapping it in a `FairnessFunction` as we mentioned for custom functions above. We show a case of that below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.utils.metrics import confusion_wilson\n",
    "from credoai.assessment.model_modules.fairness_base import FairnessFunction\n",
    "\n",
    "# define partial functions for the true positive rate lower bound\n",
    "def lower_bound_tpr(y_true, y_pred):\n",
    "    return confusion_wilson(y_true, y_pred, metric='tpr', confidence=0.95)[0]\n",
    "\n",
    "# and upper bound\n",
    "def upper_bound_tpr(y_true, y_pred):\n",
    "    return confusion_wilson(y_true, y_pred, metric='tpr', confidence=0.95)[1]\n",
    "\n",
    "# wrap the functions in fairness functions\n",
    "lower_metric = FairnessFunction(name = 'lower_bound_tpr', \n",
    "                          func = lower_bound_tpr, \n",
    "                          takes_sensitive_features = False, \n",
    "                          takes_prob = False)\n",
    "\n",
    "upper_metric = FairnessFunction(name = 'upper_bound_tpr', \n",
    "                          func = upper_bound_tpr, \n",
    "                          takes_sensitive_features = False, \n",
    "                          takes_prob = False)\n",
    "           \n",
    "# then proceed as normally\n",
    "custom_assessment = FairnessModule([lower_metric, \n",
    "                           'true_positive_rate', \n",
    "                           upper_metric],\n",
    "                A_str_test,\n",
    "                Y_test,\n",
    "                test_preds,\n",
    "                test_scores,\n",
    "                fairness_bounds\n",
    "               )\n",
    "\n",
    "custom_assessment.get_disaggregated_results(calculate_risk=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d7eb8a87bb83596f4cd5aeb66d856dad2a9bb65fe804cea051250e36746a46f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
