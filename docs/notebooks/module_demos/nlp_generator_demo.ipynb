{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f9dd59a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Language Generation Models Assessment with Lens\n",
    "Language generation models generate meaningful text when prompted with a sequence of words as context. They empower many modern downstream applications, such as chatbots.\n",
    "\n",
    "Language Generation Models Assessment tool enables the assessment of a generation model for a text attribute (toxicity, profanity, etc.) and disparate impact. It assesses the responses generated by the model to prompts and returns the text attribute levels across groups (e.g., Islam and Christianity). The tool has multiple prompts datasets and assessment models built in, but is also highly customizable and allows a user to use their own datasets and models when desired. \n",
    "\n",
    "<span style=\"color:blue\">This tutorial is currently outdated. Use [this version](https://github.com/credo-ai/credoai_lens/blob/4b44a3cdecdfd690d9bd7b49469c8bf1c9dd94ae/docs/notebooks/lens_demos/nlp_generator.ipynb) instead.</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5155d284",
   "metadata": {},
   "source": [
    "### Find the code\n",
    "This notebook can be found on [github](https://github.com/credo-ai/credoai_lens/blob/develop/docs/notebooks/module_demos/nlp_generator_demo.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6812ba6d",
   "metadata": {},
   "source": [
    "### Install required packages\n",
    "\n",
    "This package requires extra installations. Please run\n",
    "```\n",
    "pip install credoai-lens[extras]\n",
    "```\n",
    "or uncomment the below to install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b35a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade transformers\n",
    "# %pip install --upgrade sentence_transformers\n",
    "# %pip install --upgrade google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cc0ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from credoai.modules.model_modules.nlp_generator import NLPGeneratorAnalyzer\n",
    "from credoai.reporting.nlp_generator import NLPGeneratorAnalyzerReport\n",
    "from transformers import OpenAIGPTLMHeadModel, OpenAIGPTTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ee7811",
   "metadata": {},
   "source": [
    "### Define text generation function(s)\n",
    "[GPT-1](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) and [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) from OpenAI are assessed. Their pretrained versions available in Hugging Face [Transformers](https://huggingface.co/docs/transformers/index) are used. Feel free to add more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d7f510-c562-48ee-8cd6-a1836fb74967",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_gpt1 = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "model_gpt1 = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt', pad_token_id=tokenizer_gpt1.eos_token_id)\n",
    "def gpt1_text_generator(prompt):\n",
    "    inputs = tokenizer_gpt1.encode(prompt, return_tensors='pt')\n",
    "    outputs = model_gpt1.generate(inputs, max_length=max(30, len(inputs[0])+1), do_sample=True)\n",
    "    response = tokenizer_gpt1.decode(outputs[0], skip_special_tokens=True)[len(prompt):]\n",
    "    return response\n",
    "\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model_gpt2 = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer_gpt2.eos_token_id)\n",
    "def gpt2_text_generator(prompt):\n",
    "    inputs = tokenizer_gpt2.encode(prompt, return_tensors='pt')\n",
    "    outputs = model_gpt2.generate(inputs, max_length=max(30, len(inputs[0])+1), do_sample=True)\n",
    "    response = tokenizer_gpt2.decode(outputs[0], skip_special_tokens=True)[len(prompt):]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc8b5b0",
   "metadata": {},
   "source": [
    "## Two Example Use Cases\n",
    "The use of the tool is demonstrated on two examples. These two example are <strong>independent</strong> and you can run each on its own.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14afc176",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "In this example we use a local toxicity assessment model. This model is <strong>basic and for demo purposes only</strong> -- a [logistic regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) pretrained on a dataset of nearly 30,000 human-labeled comments ([Davidson et al.](https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master/data) and [Zampieri et al.](https://sites.google.com/site/offensevalsharedtask/olid)). It uses [Sentence Transformers](https://www.sbert.net/docs/pretrained_models.html) for encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.data import load_lr_toxicity\n",
    "\n",
    "loaded = load_lr_toxicity()\n",
    "lr_model = loaded['model']\n",
    "st_encoder = loaded['encoder']\n",
    "\n",
    "def lr_assessment_fun(txt):\n",
    "    txt_embedding = st_encoder.encode([txt])\n",
    "    ypred = lr_model.predict_proba(txt_embedding)\n",
    "    score = ypred[0][1]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ea7d0b",
   "metadata": {},
   "source": [
    "#### Configurations\n",
    "Here we use the builtin `realtoxicityprompts_challenging_20` prompts dataset (20 real-world challenging prompts from [Gehman et al. 2020](https://arxiv.org/pdf/2009.11462.pdf)). You can also use your own custom prompts dataset. Configure it as a csv file with three columns of 'group', 'subgroup', and 'prompt' and assign its file path to the `prompts` parameter below. If your prompts do not have any grouping, just set 'group' and 'subgroup' values to a dummy string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d21b134",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_ex1 = 'realtoxicityprompts_challenging_20'\n",
    "generation_functions_ex1 = {'gpt1': gpt1_text_generator, 'gpt2':gpt2_text_generator}\n",
    "assessment_functions_ex1 = {'toxicity': lr_assessment_fun}\n",
    "\n",
    "analyzer_ex1 = NLPGeneratorAnalyzer(\n",
    "    prompts=prompts_ex1,\n",
    "    generation_functions=generation_functions_ex1,\n",
    "    assessment_functions=assessment_functions_ex1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5279de6-9434-4647-ac96-b21cb71957d9",
   "metadata": {},
   "source": [
    "#### Perform the Assessment\n",
    "It takes about 2 minutes to run. Consider using larger prompts dataset and higher iterations for more reliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2369132-d689-4c15-83a8-e0a9e353249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_ex1.run(n_iterations=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30e1cb8",
   "metadata": {},
   "source": [
    "#### Create a Report\n",
    "`create_report()` creates summary visualizations. You can optionally pass a file path to its `filepath` to create a pdf report or/and pass a directory path to its its `dirpath` to have all the graphs as wel as tables written to your disk as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae8c890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "analyzer_ex1.prepare_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce1707",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_ex1 = NLPGeneratorAnalyzerReporter(analyzer_ex1)\n",
    "report_ex1.create_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90922780",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "In this example we use a powerful assessment tool called [Perspective API](https://www.perspectiveapi.com/). It is a text classification service from a collaborative research effort by Google. \n",
    "\n",
    "Perspective API is free, can assess text for a wide variety of attributes, and is supports many different languages.\n",
    "\n",
    "The codes needed for using this service are built into Lens, but you need to obtain a Perspective API key (instructions available [here](https://developers.perspectiveapi.com/s/docs-get-started)). The default quota is 60 requests per minute (`rpm_limit`), but you can submit a request for an increase [here](https://developers.perspectiveapi.com/s/request-quota-increase) if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b742c9e",
   "metadata": {},
   "source": [
    "#### Configuration\n",
    "Here we use the builtin `bold_religious_ideology` prompts dataset (640 real-world prompts from [Dhamala et al. 2021](https://arxiv.org/pdf/2101.11718.pdf)). Since the prompts are grouped by religious ideology, we can also gain insights into potential impact disparities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be884d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_ex2 = 'bold_religious_ideology'\n",
    "generation_functions_ex2 = {'gpt1': gpt1_text_generator, 'gpt2':gpt2_text_generator}\n",
    "assessment_functions_ex2 = {'profanity':'perspective_profanity', 'toxicity':'perspective_toxicity', 'threat':'perspective_threat', 'insult':'perspective_insult'}\n",
    "perspective_config = {\n",
    "    'api_key': 'my_api_key', \n",
    "    'rpm_limit': 60,\n",
    "    }\n",
    "\n",
    "analyzer_ex2 = NLPGeneratorAnalyzer(\n",
    "    prompts=prompts_ex2,\n",
    "    generation_functions=generation_functions_ex2,\n",
    "    assessment_functions=assessment_functions_ex2,\n",
    "    perspective_config=perspective_config\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50200560",
   "metadata": {},
   "source": [
    "#### Perform the Assessment\n",
    "With this larger dataset, this run takes about 1 hour. Consider increasing the iterations for a more reliable assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df18981",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_ex2.run(n_iterations=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c492f94f",
   "metadata": {},
   "source": [
    "#### Create a Report\n",
    "`create_report()` creates summary visualizations. You can optionally pass a file path to its `filepath` to create a pdf report or/and pass a directory path to its its `dirpath` to have all the graphs as wel as tables written to your disk as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fda3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "analyzer_ex2.prepare_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c071c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_ex2 = NLPGeneratorAnalyzerReporter(analyzer_ex2)\n",
    "report_ex2.create_report()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
