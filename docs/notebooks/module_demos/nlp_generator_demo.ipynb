{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ac2c883",
   "metadata": {},
   "source": [
    "# Language Generation Models Assessment Tool Demo\n",
    "Language generation models generate meaningful text when prompted with a sequence of words as context. These models now empower\n",
    "many downstream applications from conversation bots to automatic storytelling ([Dhamala et al. 2021](https://arxiv.org/pdf/2101.11718.pdf)). \n",
    "\n",
    "Language Generation Models Assessment tool enables the assessment of a generation model for a text attribute (toxicity, profanity, etc.) and disparate impact. It assesses the responses generated by the model to prompts and returns the text attribute levels across groups (e.g., islam and christianity). The tool has a variety of prompts datasets and text assessment models built in, but is also highly flexible and allows a user to provide their own datasets and models when desired. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f704198",
   "metadata": {},
   "source": [
    "### Install required packages\n",
    "\n",
    "This package requires extra insllations. Please run\n",
    "```\n",
    "pip install credoai-lens[extras]\n",
    "```\n",
    "or uncomment the below to install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbc7840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade transformers\n",
    "# %pip install --upgrade sentence_transformers\n",
    "# %pip install --upgrade google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a944f2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "from credoai.modules.model_assessments.nlp_generator import NLPGeneratorAnalyzer\n",
    "from transformers import OpenAIGPTLMHeadModel, OpenAIGPTTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33a3f20",
   "metadata": {},
   "source": [
    "### Define text generation function(s)\n",
    "[GPT-1](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) and [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) from OpenAI are assessed now. Their pretrained versions available in Hugging Face [Transformers](https://huggingface.co/docs/transformers/index) are used. Feel free to add more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b09b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_gpt1 = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "model_gpt1 = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt', pad_token_id=tokenizer_gpt1.eos_token_id)\n",
    "def gpt1_text_generator(prompt):\n",
    "    inputs = tokenizer_gpt1.encode(prompt, return_tensors='pt')\n",
    "    outputs = model_gpt1.generate(inputs, max_length=max(30, len(inputs[0])+1), do_sample=True)\n",
    "    response = tokenizer_gpt1.decode(outputs[0], skip_special_tokens=True)[len(prompt):]\n",
    "    return response\n",
    "\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model_gpt2 = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer_gpt2.eos_token_id)\n",
    "def gpt2_text_generator(prompt):\n",
    "    inputs = tokenizer_gpt2.encode(prompt, return_tensors='pt')\n",
    "    outputs = model_gpt2.generate(inputs, max_length=max(30, len(inputs[0])+1), do_sample=True)\n",
    "    response = tokenizer_gpt2.decode(outputs[0], skip_special_tokens=True)[len(prompt):]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06069ae4",
   "metadata": {},
   "source": [
    "## Two Example Use Cases\n",
    "The use of the tool is demonstrated on two examples. These two example are <strong>independent</strong> and you can run each on its own.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0429b0d",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "In this example we use a local toxicity assessment model. This model is <strong>basic and for demo purposes only</strong> -- a [logistic regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) pretrained on a dataset of nearly 30,000 human-labeled comments ([Davidson et al.](https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master/data) and [Zampieri et al.](https://sites.google.com/site/offensevalsharedtask/olid)). It uses \"multi-qa-distilbert-cos-v1\" model from [Sentence Transformers](https://www.sbert.net/docs/pretrained_models.html) as the encoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a675eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.data import load_lr_toxicity\n",
    "\n",
    "loaded = load_lr_toxicity()\n",
    "lr_model = loaded['model']\n",
    "st_encoder = loaded['encoder']\n",
    "\n",
    "def lr_assessment_fun(txt):\n",
    "    txt_embedding = st_encoder.encode([txt])\n",
    "    ypred = lr_model.predict_proba(txt_embedding)\n",
    "    score = ypred[0][1]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5981a72e",
   "metadata": {},
   "source": [
    "#### Configurations\n",
    "Here we use the builtin `realtoxicityprompts_challenging_100` prompts dataset (100 real-world challenging prompts extracted from [Gehman et al. 2020](https://arxiv.org/pdf/2009.11462.pdf)). You can also use your own custom prompts dataset. Configure it as a csv file with three columns of 'group', 'subgroup', and 'prompt' and assign its file path to the `prompts_dataset` key below. If your prompts do not have any grouping, just set 'group' and 'subgroup' values to a dummy string like \"none\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ad7e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_config_ex1 = {'assessment_functions': {'toxicity': lr_assessment_fun}, \n",
    "                         'prompts_dataset': 'realtoxicityprompts_challenging_100'}\n",
    "\n",
    "generation_config_ex1 = {'gpt1': gpt1_text_generator, 'gpt2':gpt2_text_generator}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1427191",
   "metadata": {},
   "source": [
    "#### Perform the Assessment\n",
    "It takes about 10 minutes to run due to the GPT time complexity. You can use `realtoxicityprompts_challenging_20` or lower the `n_iterations` for a faster, but not as conclusive, run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46969454",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_ex1 = NLPGeneratorAnalyzer(generation_config=generation_config_ex1, \n",
    "                                    assessment_config=assessment_config_ex1)\n",
    "\n",
    "results_ex1 = analyzer_ex1.run(n_iterations=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc21ac65",
   "metadata": {},
   "source": [
    "#### Create a Report\n",
    "`create_reports()` creates summary visualizations and tables. You can optionally pass a directory path to its `save_dir` parameter to have all the graphs and tables written to your disk as well.\n",
    "\n",
    "If you want to have the summary statistics table below as a dataframe, call `analyzer_ex1.prepare_rsults()`. The complete assessment results dataframe is available through `analyzer_ex1.raw_results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0a909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_ex1.create_reports()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326e4a9",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "In this example we use a powerful assessment tool called [Perspective API](https://www.perspectiveapi.com/). It is a text classification service from a collaborative research effort by Google. \n",
    "\n",
    "#### Perspective API Setup\n",
    "Perspective API is free, can assess text for a wide variety of attributes, and is supports many different languages.\n",
    "\n",
    "The codes needed for using this service are built into Lens, but you need to obtain a Perspective API key (instructions available [here](https://developers.perspectiveapi.com/s/docs-get-started)). The default quota is 60 requests per minute (`rpm_limit`), but you can submit a request for an increase [here](https://developers.perspectiveapi.com/s/request-quota-increase) if needed.\n",
    "\n",
    "Breaking down the steps, you will have to:\n",
    "* Create a google cloud account\n",
    "* Create a project\n",
    "* [Request API Access](https://developers.perspectiveapi.com/s/docs-get-started)\n",
    "* [Then enable the API](https://developers.perspectiveapi.com/s/docs-enable-the-api)\n",
    "\n",
    "After setup, the below cells should run. If they don't, test your Perspective client by running this [sample request](https://developers.perspectiveapi.com/s/docs-sample-requests)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7639df2",
   "metadata": {},
   "source": [
    "#### Configuration\n",
    "Here we use the builtin `bold_religious_ideology` prompts dataset (640 real-world prompts from [Dhamala et al. 2021](https://arxiv.org/pdf/2101.11718.pdf)). Since the prompts are grouped by religious ideology, we can also gain insights into potential impact disparities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955b19d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'copy-your-api-key-here'\n",
    "rpm_limit = 60\n",
    "\n",
    "assessment_config_ex2 = {'assess_with_builtin_models': 'perspective', \n",
    "                         'api_key': API_KEY, \n",
    "                         'rpm_limit': rpm_limit, \n",
    "                         'prompts_dataset': 'bold_religious_ideology'}\n",
    "\n",
    "generation_config_ex2 = {'gpt1': gpt1_text_generator, \n",
    "                         'gpt2':gpt2_text_generator}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bab1e9b",
   "metadata": {},
   "source": [
    "#### Perform the Assessment\n",
    "With this larger dataset, more iterations, and the GPT time complexity, this run takes about 3 hours. Feel free to run with a small subset and fewer iterations for a faster run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b745be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_ex2 = NLPGeneratorAnalyzer(generation_config=generation_config_ex2, \n",
    "                                    assessment_config=assessment_config_ex2)\n",
    "\n",
    "results_ex2 = analyzer_ex2.run(n_iterations=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b1a948",
   "metadata": {},
   "source": [
    "#### Create a Report\n",
    "`create_reports()` creates summary visualizations and tables. You can optionally pass a directory path to its `save_dir` parameter to have all the graphs and tables written to your disk as well.\n",
    "\n",
    "If you want to have the summary statistics table below as a dataframe, call `analyzer_ex1.prepare_rsults()`. The complete assessment results dataframe is available through `analyzer_ex1.raw_results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd82daf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_ex2.create_reports()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
