{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f9dd59a",
   "metadata": {},
   "source": [
    "# Text Generation Model Assessment Tool Demo\n",
    "Text generation models generate cohesive open-ended text when prompted with a sequence of words as context. These models now empower\n",
    "many downstream applications from conversation bots to automatic storytelling ([Dhamala et al. 2021](https://arxiv.org/pdf/2101.11718.pdf)). \n",
    "\n",
    "Text Generation Model Assessment tool enables the assessment of a text generation model for a text attribute (e.g., toxicity) and bias. It assesses the responses generated by the model to categorized prompts and returns the text attribute levels overall and across groups (e.g., islam and christianity) of a protected attribute (e.g., religion)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6812ba6d",
   "metadata": {},
   "source": [
    "### Install required packages\n",
    "\n",
    "This package requires extra insllations. Please run\n",
    "```\n",
    "pip install credoai-lens[extras]\n",
    "```\n",
    "or uncomment the below to install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b35a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade transformers\n",
    "# %pip install --upgrade tensorflow-hub\n",
    "# %pip install --upgrade google-api-python-client\n",
    "# %pip install --upgrade seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ee7811",
   "metadata": {},
   "source": [
    "### Define your text generation function\n",
    "We use GPT-2 by [OpenAI](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) here to demonstrate the assessment module. GPT-2  is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. The pretrained version of this model from [Hugging Face](https://huggingface.co/docs/transformers/model_doc/gpt2) is used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d7f510-c562-48ee-8cd6-a1836fb74967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Initialize tokenizer and model from pretrained GPT2 model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "def gpt2_text_generator(prompt):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(inputs, max_length=max(20, len(inputs[0])+1), do_sample=True)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14afc176",
   "metadata": {},
   "source": [
    "### Define your text assessment function\n",
    "An `assessment_fun` needs to be provided to score the responses generated by the `generation_fun` for a particualr attribute of text (`assessment_attribute`).\n",
    "\n",
    "Lens includes a pretrained toxicity assessment model that we use here. It is a basic [logistic regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) pretrained on a dataset of nearly 30,000 human-labeled comments from [Davidson et al.](https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master/data) and [Zampieri et al.](https://sites.google.com/site/offensevalsharedtask/olid) and uses [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/4) as the encoder. \n",
    "\n",
    "Later in this demo we will use a much more powerful text assessment service from Google. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6a8059a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained toxicity assessment model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/lens/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.0 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from credoai.data import load_lr_toxicity\n",
    "\n",
    "loaded = load_lr_toxicity()\n",
    "lr_model = loaded['model']\n",
    "use_encoder = loaded['encoder']\n",
    "\n",
    "def lr_assessment_fun(txt):\n",
    "    txt_embedding = use_encoder([txt])\n",
    "    ypred = lr_model.predict_proba(txt_embedding)\n",
    "    score = ypred[0][1]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24487823",
   "metadata": {},
   "source": [
    "Let's test our basic toxicity assessment model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f178ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = '\"What a beautiful day!\"'\n",
    "print(txt + ' is ' + \"{:.0%}\".format(lr_assessment_fun(txt)) + ' toxic.')\n",
    "txt = '\"Shut up and listen to me!\"'\n",
    "print(txt + ' is ' + \"{:.0%}\".format(lr_assessment_fun(txt)) + ' toxic.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5279de6-9434-4647-ac96-b21cb71957d9",
   "metadata": {},
   "source": [
    "### Perform the Assessment\n",
    "Assessment is now done for the `gpt2_text_generator` generation model for toxicity and using `lr_assessment_attribute` assessment function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2369132-d689-4c15-83a8-e0a9e353249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.modules.model_assessments.fairness_nlp import NLPGeneratorAnalyzer\n",
    "\n",
    "lr_assessment_attribute = 'TOXICITY'\n",
    "\n",
    "analyzer = NLPGeneratorAnalyzer(\n",
    "    generation_fun=gpt2_text_generator,\n",
    "    assessment_fun=lr_assessment_fun,\n",
    "    assessment_attribute=lr_assessment_attribute\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4898d85e",
   "metadata": {},
   "source": [
    "Analyzer is run for the protected attribute of religion. `n_iterations` is the number of times to generate responses for a same prompt to enable a more robust assessment of stochastic text generation models. Groups can be chosen from ['judaism', 'christianity', 'islam', 'hinduism', 'buddhism', 'sikhism', 'atheism'] or be set to 'all' to include them all. Other supported `protected_attribute` values are: 'gender', 'politics', 'profession', and 'race'. The prompts are from [BOLD dataset](https://github.com/amazon-research/bold).\n",
    "\n",
    "Every iteration takes around a few minutes to complete due to the GPT-2's time complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df18981",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.run(\n",
    "    protected_attribute='religion',\n",
    "    n_iterations=2,\n",
    "    groups=['judaism', 'christianity', 'islam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450e22c3-a56e-4b57-ab73-1414595f495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = analyzer.prepare_results()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9b7d01",
   "metadata": {},
   "source": [
    "Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec14108-1650-4a46-96ad-75faacfe98b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rc(\"figure\", dpi=200)\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "fig = plt.figure(figsize=(8,4), dpi=300)\n",
    "fig.patch.set_facecolor('white')\n",
    "ax = sns.barplot(x=\"value\", y=\"group\", data=results, color='rebeccapurple')\n",
    "ax.set_frame_on(False)\n",
    "ax.xaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "ax.tick_params(axis='x', colors='grey')\n",
    "ax.yaxis.set_ticks_position('none') \n",
    "plt.xlabel('average response ' + lr_assessment_attribute.lower())\n",
    "plt.ylabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9bbb52-f6b2-429c-ac2c-f8bee225ba8d",
   "metadata": {},
   "source": [
    "### Assessment with a custom rating function\n",
    "Let's replace the basic logistic regression toxicity scoring model with an advanced custom one -- [Perspective API](https://www.perspectiveapi.com/). It is a toxic comment identification product from a collaborative research effort by Jigsaw and Googleâ€™s Counter Abuse Technology team. \n",
    "\n",
    "Perspective API is free and available to use in Arabic, Chinese, Czech, Dutch, English, French, German, Hindi, Hinglish, Indonesian, Italian, Japanese, Korean, Polish, Portuguese, Russian, and Spanish\n",
    "\n",
    "In addition to toxicity (`TOXICITY`), Perspective API can also provide scores for the other following text attributes (more details [here](https://developers.perspectiveapi.com/s/about-the-api-attributes-and-languages)):\n",
    "* Severe Toxicity (`SEVERE_TOXICITY`)\n",
    "* Insult (`INSULT`)\n",
    "* Profanity (`PROFANITY`)\n",
    "* Identity attack (`IDENTITY_ATTACK`)\n",
    "* Threat (`THREAT`)\n",
    "\n",
    "Instructions on how to obtain a Perspective API Key are available [here](https://developers.perspectiveapi.com/s/docs-get-started). API requests frequency quota is limited (default is 1 per second). Set the `pause_duration` according to your account's limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d29f5ae-e0be-4fa6-b0eb-8f7a476b06fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from time import sleep\n",
    "\n",
    "perspective_assessment_attribute = 'TOXICITY'\n",
    "\n",
    "API_KEY = ''\n",
    "pause_duration = 1.0/60  # seconds\n",
    "\n",
    "client = discovery.build(\n",
    "    \"commentanalyzer\",\n",
    "    \"v1alpha1\",\n",
    "    developerKey=API_KEY,\n",
    "    discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "    cache_discovery=False\n",
    ")\n",
    "\n",
    "def perspective_assessment_fun(txt):\n",
    "    sleep(pause_duration)\n",
    "    analyze_request = {'comment': { 'text': txt }, 'requestedAttributes': {perspective_assessment_attribute: {}}, 'languages': ['en']}\n",
    "    response = client.comments().analyze(body=analyze_request).execute()\n",
    "    return response['attributeScores'][perspective_assessment_attribute]['summaryScore']['value']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc598fb7",
   "metadata": {},
   "source": [
    "Let's give this assessment service a quick try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec58625",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = '\"What a beautiful day!\"'\n",
    "print(txt + ' is ' + \"{:.0%}\".format(perspective_assessment_fun(txt)) + ' toxic.')\n",
    "txt = '\"Shut up and listen to me!\"'\n",
    "print(txt + ' is ' + \"{:.0%}\".format(perspective_assessment_fun(txt)) + ' toxic.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df7a30-e4a1-4ce5-9955-8eb2c88f2999",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_perspective = NLPGeneratorAnalyzer(\n",
    "    generation_fun=gpt2_text_generator,\n",
    "    assessment_fun=perspective_assessment_fun,\n",
    "    assessment_attribute=perspective_assessment_attribute)\n",
    "\n",
    "\n",
    "res_perspective = analyzer_perspective.run(\n",
    "    protected_attribute='religion',\n",
    "    n_iterations=5,\n",
    "    groups='all'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbd09e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_perspective = analyzer_perspective.prepare_results()\n",
    "results_perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfb38b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4), dpi=300)\n",
    "fig.patch.set_facecolor('white')\n",
    "ax = sns.barplot(x=\"value\", y=\"group\", data=results_perspective, color='rebeccapurple')\n",
    "ax.set_frame_on(False)\n",
    "ax.xaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "ax.tick_params(axis='x', colors='grey')\n",
    "ax.yaxis.set_ticks_position('none') \n",
    "plt.xlabel('average response ' + perspective_assessment_attribute.lower())\n",
    "plt.ylabel('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
