{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6f2215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.integration import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f78408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2050e80",
   "metadata": {},
   "source": [
    "# Create an example ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8260edd9",
   "metadata": {},
   "source": [
    "### Load data and train model\n",
    "\n",
    "For the purpose of this demonstration, we will be classifying digits after a large amount of noise has been added to each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595c6a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# add noise\n",
    "digits.data += np.random.rand(*digits.data.shape)*16\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n",
    "\n",
    "# create and fit model\n",
    "clf = SVC(probability=True)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89b9890",
   "metadata": {},
   "source": [
    "### Visualize example images along with predicted label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058f276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_plot = plt.figure()\n",
    "for i in range(8):\n",
    "    image_data = X_test[i,:]\n",
    "    prediction = digits.target_names[clf.predict(image_data[None,:])[0]]\n",
    "    label = f'Pred: \"{prediction}\"'\n",
    "    # plot\n",
    "    ax = plt.subplot(2,4,i+1)\n",
    "    ax.imshow(image_data.reshape(8,8), cmap='gray')\n",
    "    ax.set_title(label)\n",
    "    ax.tick_params(labelbottom=False, labelleft=False, length=0)\n",
    "plt.suptitle('Example Images and Predictions', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff7e690",
   "metadata": {},
   "source": [
    "### Calculate performance metrics and visualize\n",
    "\n",
    "As a multiclassification problem, we can calculate metrics per class, or overall. We record overall metrics, but include figures for individual class performance breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff83614",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = classification_report(y_test, clf.predict(X_test), output_dict=True)\n",
    "overall_metrics = metrics['macro avg']\n",
    "del overall_metrics['support']\n",
    "pprint(overall_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4981663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = clf.predict_proba(X_test)\n",
    "pr_curves = plt.figure(figsize=(8,6))\n",
    "# plot PR curve sper digit\n",
    "for digit in digits.target_names:\n",
    "    y_true = y_test == digit\n",
    "    y_prob = probs[:,digit]\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    plt.plot(recalls, precisions, lw=3, label=f'Digit: {digit}')\n",
    "plt.xlabel('Recall', fontsize=16)\n",
    "plt.ylabel('Precision', fontsize=16)\n",
    "\n",
    "# plot iso lines\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    label = label='ISO f1 curves' if f_score==f_scores[0] else ''\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2, label=label)\n",
    "# final touches\n",
    "plt.xlim([0.5, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.title('PR Curves per Digit', fontsize=20)\n",
    "plt.legend(loc='lower left', fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4830b81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "confusion_plot = plt.figure(figsize=(6,6))\n",
    "plot_confusion_matrix(clf, X_test, y_test, \\\n",
    "                      normalize='true', ax=plt.gca(), colorbar=False)\n",
    "plt.tick_params(labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae38bd09",
   "metadata": {},
   "source": [
    "# Integration example \n",
    "## Sending model information to Credo AI\n",
    "For governance, Credo AI needs some information about your model.\n",
    "* Metrics related to performance, fairness, or other governance considerations\n",
    "* Input features\n",
    "* Outputs (what does the model produce?)\n",
    "\n",
    "In addition, figures are often produced that help communicate metrics better, understand the model, or other contextualize the AI system. Credo can ingest those as well.\n",
    "\n",
    "**Which metrics to record?**\n",
    "\n",
    "Ideally you will have decided on the most important metrics before building the model. We refer to this stage as `Metric Alignment`. This is the phase where you explicitly determine how you will measure whether your model can be safely deployed. \n",
    "\n",
    "You may want to record more metrics than those explicitly determined during `Metric Alignment`.\n",
    "\n",
    "For instance, in this example let's say that during `Metric Alignment`, the _F1 Score_ is the primary metric used to evaluate model performance. However, we have decided that recall and precision would be helpful supporting. So we will send those three metrics.\n",
    "\n",
    "\n",
    "To reiterate: You are always free to send more metrics - Credo AI will ingest them. It is you and your team's decision which metrics are tracked specifically for governance purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7128c975",
   "metadata": {},
   "source": [
    "## Quick reference\n",
    "\n",
    "Below is all the code needed to record a set of metrics, figures, inputs and outputs and wrap them all up in a model. We will unpack each part below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2e526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'SVC_1.0'\n",
    "dataset_name = 'test'\n",
    "\n",
    "# metrics\n",
    "metric_records = record_metrics(overall_metrics, \n",
    "                                model=model_name,\n",
    "                                dataset=dataset_name, \n",
    "                                user_id='Ada Lovelace')\n",
    "\n",
    "#figures\n",
    "example_figure_record = Figure(examples_plot._suptitle.get_text(), examples_plot)\n",
    "confusion_figure_record = Figure(confusion_plot.axes[0].get_title(), confusion_plot)\n",
    "\n",
    "pr_curve_caption=\"\"\"Precision-recall curves are shown for each digit separately.\n",
    "These are calculated by treating each class as a separate\n",
    "binary classification problem. The grey lines are \n",
    "ISO f1 curves - all points on each curve have identical\n",
    "f1 scores.\n",
    "\"\"\"\n",
    "pr_curve_figure_record = Figure(pr_curves.axes[0].get_title(),\n",
    "                                figure=pr_curves,\n",
    "                                caption=pr_curve_caption)\n",
    "figure_records = [example_figure_record, confusion_figure_record, pr_curve_figure_record]\n",
    "\n",
    "# inputs\n",
    "input_description=\"\"\"8x8 digit image. The original images had 4-bit pixels.\n",
    "Noise (uniformly sampled from real numbers 0-16) was\n",
    "added to each pixel.\n",
    "\"\"\"\n",
    "input_feature_record = Input(name='8x8 noisy digit image',\n",
    "                      example=digits.data[0],\n",
    "                      description=input_description)\n",
    "\n",
    "# outputs\n",
    "output_description=\"\"\"Class of the digit corresponding to the highest\n",
    "probability digit according to the model. \n",
    "The probability of each class is also output from the model.\"\"\"\n",
    "output_record = Output(name='Digit class',\n",
    "                example=digits.target_names[0],\n",
    "                description=output_description)\n",
    "\n",
    "# model\n",
    "model_record = Model(metric_records, figure_records, [input_feature_record, output_record])\n",
    "\n",
    "# export to file\n",
    "# export_record(model_record, 'model_record.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32292657",
   "metadata": {},
   "source": [
    "## Metric Record\n",
    "\n",
    "To record a metric you can either record each one manually or ingest a dictionary of metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd891a46",
   "metadata": {},
   "source": [
    "### Manually entering individual metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2010c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_description = \"\"\"Harmonic mean of precision and recall scores.\n",
    "Ranges from 0-1, with 1 being perfect performance.\"\"\"\n",
    "f1_record = Metric(name='f1', \n",
    "                   value=overall_metrics['f1-score'],\n",
    "                   model = model_name, \n",
    "                   dataset=dataset_name,\n",
    "                   user_id = 'Ada Lovelace',\n",
    "                   description=f1_description)\n",
    "\n",
    "precision_record = Metric(name='precision',\n",
    "                          value=overall_metrics['precision'],\n",
    "                          model = model_name, \n",
    "                          dataset=dataset_name,\n",
    "                          user_id = 'Marvin Minsky'\n",
    "                          )\n",
    "\n",
    "recall_record = Metric(name='recall', \n",
    "                       value=overall_metrics['recall'],\n",
    "                       model = model_name, \n",
    "                       dataset = dataset_name)\n",
    "metrics = [f1_record, precision_record, recall_record]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1dfe1d",
   "metadata": {},
   "source": [
    "### Convenience to record multiple metrics\n",
    "\n",
    "*** Note *** You cannot customize each metric's model, dataset or user ID and cannot edit the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f8161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_records = record_metrics(overall_metrics, \n",
    "                                model=model_name,\n",
    "                                dataset=dataset_name, \n",
    "                                user_id='Ada Lovelace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f356f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in metric_records:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d1835e",
   "metadata": {},
   "source": [
    "## Record figures\n",
    "\n",
    "Credo can accept a path to an image file or a matplotlib figure. Matplotlib figures are converted to PNG images and saved.\n",
    "\n",
    "\n",
    "A caption can be included for futher description. Included a caption is recommended when the image is not self-explanatory, which is most of the time! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b356e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_figure_record = Figure(examples_plot._suptitle.get_text(), examples_plot)\n",
    "confusion_figure_record = Figure(confusion_plot.axes[0].get_title(), confusion_plot)\n",
    "\n",
    "pr_curve_caption=\"\"\"Precision-recall curves are shown for each digit separately.\n",
    "These are calculated by treating each class as a separate\n",
    "binary classification problem. The grey lines are \n",
    "ISO f1 curves - all points on each curve have identical\n",
    "f1 scores.\n",
    "\"\"\"\n",
    "pr_curve_figure_record = Figure(pr_curves.axes[0].get_title(),\n",
    "                                figure=pr_curves,\n",
    "                                caption=pr_curve_caption)\n",
    "figure_records = [example_figure_record, confusion_figure_record, pr_curve_figure_record]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836272fd",
   "metadata": {},
   "source": [
    "## Input Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b517d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_description=\"\"\"8x8 digit image. The original images had 4-bit pixels.\n",
    "Noise (uniformly sampled from real numbers 0-16) was\n",
    "added to each pixel.\n",
    "\"\"\"\n",
    "input_feature_record = Input(name='8x8 noisy digit image',\n",
    "                      example=digits.data[0],\n",
    "                      description=input_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370d3a14",
   "metadata": {},
   "source": [
    "If many features need to be recorded, the convenience function `create_feature_records` can be used. For instance, the code below records each pixel as a separate input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb177333",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_args = {'name': digits.feature_names,\n",
    "                'example': digits.data[0]}\n",
    "inputs_exhaustive = create_input_records(input_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3549f97d",
   "metadata": {},
   "source": [
    "## Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_description=\"\"\"Class of the digit corresponding to the highest\n",
    "probability digit according to the model. \n",
    "The probability of each class is also output from the model.\"\"\"\n",
    "output_record = Output(name='Digit class',\n",
    "                example=digits.target_names[3],\n",
    "                description=output_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2e48ff",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "To send all the information, we wrap the records in a model record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10939ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_record = Model(metric_records, figure_records, [input_feature_record, output_record])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a717e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Export to Credo AI\n",
    "\n",
    "The json object of the model record can be created by calling `model_record.jsonify()`. The convenience function `export_record` can be called to export the json record to a file. This file can then be uploaded to Credo AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f5bcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename is the location to save the json object of the model record\n",
    "filename=\"XXX.json\"\n",
    "export_record(model_record, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab33dbb8",
   "metadata": {},
   "source": [
    "All Credo Toolkits also have the ability to send Model Records directly to Credo AI's Governance Platform. Though you shouldn't need to access these functions directly in general, we illustrate how this could be completed below. Note you must know the Model ID associated with the model on the Governance Platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a42213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.utils.credo_api_utils import patch_metrics\n",
    "model_id = \"XXX\"\n",
    "patch_metrics(model_id, model_record)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
