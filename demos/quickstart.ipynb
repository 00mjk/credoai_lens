{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bacb6cfb",
   "metadata": {},
   "source": [
    "# Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef97ab0c",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "CredoAI Lens can be installed with pip from PyPI as follows:\n",
    "\n",
    "`pip install credoai`\n",
    "\n",
    "Optional dependencies can be installed as follows:\n",
    "\n",
    "`pip install credoai[extras]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05e9ec9",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "CredoAI Lens is the assessment framework component of the broader CredoAI suite.\n",
    "It is usable as a standalone gateway to assessments or in combination\n",
    "with CredoAI's Governance Platform. \n",
    "\n",
    "Understanding how your AI systems are operating is the most important step in intervening upon your system. From the technically compliated questions of improving a system to the business-relevant questions of whether to deploy a system - everything is fundamentally founded upon good observability. Lens strives to make assessment comprehensive, easy, and adaptable.\n",
    "\n",
    "### Assessments\n",
    "\n",
    "CredoAI Lens is an entrypoint to asssessments developed by CredoAI, as well as the broader ecosystem of open source assessments. Custom analytics can also be folded in easily (see the `lens customization` notebook)\n",
    "\n",
    "AI system assessment starts with verifying standard performance metrics to an evolving set of assessments falling under the banner of *Responsible AI*. A non-exhaustive list includes\n",
    "\n",
    "* Fairness\n",
    "* Explainability\n",
    "* Performance\n",
    "* Robustness\n",
    "\n",
    "These different categories of assessment differ substantially based on whether one is \n",
    "evaluating datasets or models, what kind of model (e.g., tabular, NLP, computer vision), and the use-case. As the ecosystem develops, Lens will support assessing a broader range of AI systems. Currently, we are focused on Fairness.\n",
    "\n",
    "### Governance\n",
    "\n",
    "While Lens is a stand alone assessment framework, it's value is increased when combined with the CredoAI Governance Platform. The platform supports multi-stakeholder `Alignment` on how to assess your AI systems (e.g., what does good look like for this system?). It also supports translating assessment results into a Risk perspective that is scalable across your organization and understandable to diverse stakeholders.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22055d1",
   "metadata": {},
   "source": [
    "## Lens in 5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14bf2fb",
   "metadata": {},
   "source": [
    "Get your data and models together! Here we have an support vector machine trained on the Iris Dataset. No train/test split needed for this tutorial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38bebb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.data import fetch_creditdefault\n",
    "from credoai.utils.model_utils import get_gradient_boost_model\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7f687e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_creditdefault(as_frame=True)\n",
    "X = data['data'].drop(columns=['SEX'])\n",
    "y = data['target']\n",
    "sensitive_feature = data['data']['SEX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eab5cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5348a0",
   "metadata": {},
   "source": [
    "### Using Lens\n",
    "\n",
    "Lens interacts with Credo Artifacts which wrap models and datasets and standardizes them for use by different assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4556df79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.lens import Lens, CredoModel, CredoData\n",
    "from credoai.assessment.assessments import FairnessAssessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bcc3eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "credo_model = CredoModel(name='credit_default_classifier',\n",
    "                         model=model)\n",
    "\n",
    "# no sensitive feature! need to correct. Better dataset needed\n",
    "credo_data = CredoData(name='UCI-credit-default',\n",
    "                       X=X, \n",
    "                       y=y.astype(int),\n",
    "                       sensitive_features=sensitive_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38e3fed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scope = {'metrics': ['precision_score']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a2b72e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = Lens(model=credo_model,\n",
    "            data=credo_data,\n",
    "            assessments=[FairnessAssessment()],\n",
    "            scope=scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce9cf100",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = lens.run_assessments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34ddd2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>risk</th>\n",
       "      <th>acceptable</th>\n",
       "      <th>bound_lower</th>\n",
       "      <th>bound_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision_score</th>\n",
       "      <td>0.000543</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>-inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    value  risk  acceptable  bound_lower  bound_upper\n",
       "precision_score  0.000543   NaN        True         -inf          inf"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['FairnessBase']['fairness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49897251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
