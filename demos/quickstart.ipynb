{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96b4792f",
   "metadata": {},
   "source": [
    "# Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b4794e",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "CredoAI Lens can be installed with pip from PyPI as follows:\n",
    "\n",
    "`pip install credoai`\n",
    "\n",
    "Optional dependencies can be installed as follows:\n",
    "\n",
    "`pip install credoai[extras]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4203f75c",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "CredoAI Lens is the assessment framework component of the broader CredoAI suite.\n",
    "It is usable as a standalone gateway to assessments or in combination\n",
    "with CredoAI's Governance Platform. \n",
    "\n",
    "Understanding how your AI systems are operating is the most important step in intervening upon your system. From the technically complicated questions of improving a system to the business-relevant questions of whether to deploy a system - everything is fundamentally founded upon good observability. Lens strives to make assessment comprehensive, easy, and adaptable.\n",
    "\n",
    "### Assessments\n",
    "\n",
    "CredoAI Lens is an entry point to assessments developed by CredoAI, as well as the broader ecosystem of open-source assessments. Custom analytics can also be folded in easily (see the `lens customization` notebook)\n",
    "\n",
    "AI system assessment starts with verifying standard performance metrics to an evolving set of assessments falling under the banner of *Responsible AI*. A non-exhaustive list includes\n",
    "\n",
    "* Fairness\n",
    "* Explainability\n",
    "* Performance\n",
    "* Robustness\n",
    "\n",
    "These different categories of assessment differ substantially based on whether one is \n",
    "evaluating datasets or models, what kind of model (e.g., tabular, NLP, computer vision), and the use-case. As the ecosystem develops, Lens will support assessing a broader range of AI systems. Currently, we are focused on Fairness.\n",
    "\n",
    "### Governance\n",
    "\n",
    "While Lens is a stand-alone assessment framework, its value is increased when combined with the CredoAI Governance Platform. The platform supports multi-stakeholder `Alignment` on how to assess your AI systems (e.g., what does good look like for this system?). It also supports translating assessment results into a Risk perspective that is scalable across your organization and understandable to diverse stakeholders.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a99124",
   "metadata": {},
   "source": [
    "## Lens in 5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726fc511",
   "metadata": {},
   "source": [
    "Get your data and models together! Here we have a support vector machine trained on the Iris Dataset. No train/test split needed for this tutorial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9a8fc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Failed to import tensorflow. Please note that tensorflow is not installed by default when you install tensorflow_hub. This is so that users can decide which tensorflow package to use. To use tensorflow_hub, please install a current version of tensorflow by following the instructions at https://tensorflow.org/install and https://tensorflow.org/hub/installation.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imports for example data and model training\n",
    "from credoai.data import fetch_creditdefault\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Base Lens imports\n",
    "from credoai.lens import Lens, CredoModel, CredoData\n",
    "# importing a particular assessment\n",
    "from credoai.assessment.assessments import FairnessAssessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "508d0a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_creditdefault(as_frame=True)\n",
    "X = data['data'].drop(columns=['SEX'])\n",
    "y = data['target']\n",
    "sensitive_feature = data['data']['SEX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a2d5874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ee700",
   "metadata": {},
   "source": [
    "### Using Lens\n",
    "\n",
    "Below is a basic example where our goal is to evaluate the fairness of the above model. We will rely on Lens defaults for this analysis.\n",
    "\n",
    "#### Preparing artifacts\n",
    "\n",
    "Lens interacts with Credo Artifacts which wrap models and datasets and standardizes them for use by different assessments.\n",
    "Below we create a `CredoModel` object, which automatically infers that the \"model\" object is from scikit-learn. We also create a `CredoData` object which is store X, y and sensitive features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a0d06dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "credo_model = CredoModel(name='credit_default_classifier',\n",
    "                         model=model)\n",
    "\n",
    "# no sensitive feature! need to correct. Better dataset needed\n",
    "credo_data = CredoData(name='UCI-credit-default',\n",
    "                       X=X, \n",
    "                       y=y.astype(int),\n",
    "                       sensitive_features=sensitive_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd20b46",
   "metadata": {},
   "source": [
    "#### Alignment Manifest\n",
    "\n",
    "The Alignment Manifest describes how the assessments should be run at a high level. It should be thought of as the output of a multi-stakeholder alignment session on \"how we should assess this AI system\". Another way to think about it is as the *paramterization* of the assessments Lens will run.\n",
    "\n",
    "The Alignment Manifest is a *subset* of all possible parameterizations of the different assessments you will run. You have more programmatic control of the assessments by making use of the `init_assessment_kwargs` (passed to Lens when initializied) or the `assessment_kwargs`, which are passed to the `run_assessments` function.\n",
    "\n",
    "If you use the Credo AI Governance Platform, the alignment manifest is a principle artifact determined during the *Alignment Phase*. It is the output of multi-stakeholder collaboration. Lens will automatically download the Alignment Manifest associated with your governance credentials (which uses another artifact: `CredoGovernance`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6abc926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the metrics that will be used by the FairnessBase assessment\n",
    "alignment_manifest = {'metrics': ['precision_score']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31051601",
   "metadata": {},
   "source": [
    "#### Run Lens\n",
    "\n",
    "Once we have the model and data artifacts, as well as the manifest, we can run Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74680dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = Lens(model=credo_model,\n",
    "            data=credo_data,\n",
    "            assessments=[FairnessAssessment()],\n",
    "            manifest=alignment_manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d019e65",
   "metadata": {},
   "source": [
    "Lens has one primary method: `run_assessments`\n",
    "\n",
    "`run_assessments` outputs the results into a dictionary that can be used for further processing. If the `export` keyword is used, it can also export the data to a json or straight to Credo AI's governance platform (again, if you are using it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af115bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = lens.run_assessments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d70acc89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision_score</th>\n",
       "      <td>0.000543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    value\n",
       "precision_score  0.000543"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['FairnessBase']['fairness']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e7f880",
   "metadata": {},
   "source": [
    "Example export to `~/credoai_test_location`. Uncomment to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2202ad0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FairnessBase': {'fairness':                     value\n",
       "  precision_score  0.000543,\n",
       "  'disaggregated_results':          precision_score\n",
       "  SEX                     \n",
       "  1.0             0.695954\n",
       "  2.0             0.695411\n",
       "  overall         0.695652}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # example export\n",
    "# import os\n",
    "# output_directory = os.path.join(os.path.expanduser('~'), 'credoai_test_location')\n",
    "# lens.run_assessments(export=output_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
