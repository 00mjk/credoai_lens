{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dbbfa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.utils.credo_api_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec3ca62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'A7JCD7y3RjCVp9v7Q45jpB',\n",
       "  'type': 'ai_scope_metrics',\n",
       "  'lower_threshold': 0.0,\n",
       "  'model_id': 'Yppp7ETdViNFfDJXUSpBLE',\n",
       "  'name': 'recall score',\n",
       "  'upper_threshold': 1.0},\n",
       " {'id': 'e8ZXC3D5WKCsVG3nPW2cki',\n",
       "  'type': 'ai_scope_metrics',\n",
       "  'lower_threshold': -0.1,\n",
       "  'model_id': 'Yppp7ETdViNFfDJXUSpBLE',\n",
       "  'name': 'equalized odds',\n",
       "  'upper_threshold': 0.1},\n",
       " {'id': 'Hz4bUeseP7vvqwKWhkcA9A',\n",
       "  'type': 'ai_scope_metrics',\n",
       "  'lower_threshold': 0.0,\n",
       "  'model_id': 'Yppp7ETdViNFfDJXUSpBLE',\n",
       "  'name': 'precision_score',\n",
       "  'upper_threshold': 1.0},\n",
       " {'id': 'h8pQbEWxj8EySG2hqNbLKm',\n",
       "  'type': 'ai_scope_metrics',\n",
       "  'lower_threshold': 0.0,\n",
       "  'model_id': 'Vw2dUofxXJan3NBcECWnLk',\n",
       "  'name': 'equal odds',\n",
       "  'upper_threshold': 1.0},\n",
       " {'id': 'N4LSTVjeXqFVFhdmMNS7iX',\n",
       "  'type': 'ai_scope_metrics',\n",
       "  'lower_threshold': 0.0,\n",
       "  'model_id': 'Yppp7ETdViNFfDJXUSpBLE',\n",
       "  'name': 'tpr',\n",
       "  'upper_threshold': 1.0}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scope = get_solution_manifest('ZESsXtfyDRWzftnRZt3QZS')\n",
    "scope['metrics']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2715fa5d",
   "metadata": {},
   "source": [
    "# Customizing Lens\n",
    "\n",
    "Lens "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9862658",
   "metadata": {},
   "source": [
    "## Parameterizing Assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df55e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89471349",
   "metadata": {},
   "source": [
    "## Creating New Modules & Assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1654cf64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b79376ad",
   "metadata": {},
   "source": [
    "## Custom Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea76881",
   "metadata": {},
   "source": [
    "### incorporating confidence intervals\n",
    "\n",
    "Custom metrics can be incorporated by creating a `FairnessFunction`. The `FairnessFunction` is a lightweight wrapper class that defines a few characteristics of the custom function needed by the module. \n",
    "\n",
    "**Example: Confidence Intervals**\n",
    "\n",
    "Confidence intervals are not generically supported. However, they can be derived for metrics derived from a confusion matrix using the `wilson confidence interval`. A convenience function called `confusion_wilson` is supplied which returns an array: [lower, upper] bound for the metric. \n",
    "\n",
    "However, the module is not designed to accomodate metrics that return arrays. If you wish to incorporate CIs, we'd suggest creating a derived function that returns the lower bound, for example, and then wrapping it in a `FairnessFunction` as we mentioned for custom functions above. We show a case of that below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddf8645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.utils.metrics import confusion_wilson\n",
    "from credoai.assessment.model_assessments.fairness_base import FairnessFunction\n",
    "\n",
    "# define partial functions for the true positive rate lower bound\n",
    "def lower_bound_tpr(y_true, y_pred):\n",
    "    return confusion_wilson(y_true, y_pred, metric='tpr', confidence=0.95)[0]\n",
    "\n",
    "# and upper bound\n",
    "def upper_bound_tpr(y_true, y_pred):\n",
    "    return confusion_wilson(y_true, y_pred, metric='tpr', confidence=0.95)[1]\n",
    "\n",
    "# wrap the functions in fairness functions\n",
    "lower_metric = FairnessFunction(name = 'lower_bound_tpr', \n",
    "                          func = lower_bound_tpr, \n",
    "                          takes_sensitive_attributes = False, \n",
    "                          takes_prob = False)\n",
    "\n",
    "upper_metric = FairnessFunction(name = 'upper_bound_tpr', \n",
    "                          func = upper_bound_tpr, \n",
    "                          takes_sensitive_attributes = False, \n",
    "                          takes_prob = False)\n",
    "           \n",
    "# then proceed as normally\n",
    "custom_assessment = FairnessModule([lower_metric, \n",
    "                           'true_positive_rate', \n",
    "                           upper_metric],\n",
    "                A_str_test,\n",
    "                Y_test,\n",
    "                test_preds,\n",
    "                test_scores,\n",
    "                fairness_bounds\n",
    "               )\n",
    "\n",
    "custom_assessment.get_disaggregated_results(calculate_risk=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
