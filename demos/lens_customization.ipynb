{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9ee9afc-b73e-4cc3-9a1a-74501a1b7572",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Customizing Lens\n",
    "\n",
    "Lens strives to give you sensible defaults and automatically do the proper assessments whenever possible. However, there are many times where you'll want to change, select, or extend the functionality. Lens is intended to be an _extensible framework_ that can accomodate your own analysis plan.\n",
    "\n",
    "In this document we will describe a number of ways you can customize Lens:\n",
    "* Selecting which assessments to run\n",
    "* Parameterizing assessments\n",
    "* Creating new modules and assessments\n",
    "* Incorporating new metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4de1ab2a-9620-4305-8a58-38d032a19208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import credoai.lens as cl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bb7169-de98-456d-9efa-42d09a2b4cab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup\n",
    "\n",
    "We'll set up data and a model to run through some customization options. See `quickstart` for more information on this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90c666f2-a8b9-4609-becb-5efdd65719a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports for example data and model training\n",
    "from credoai.data import fetch_creditdefault\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# data\n",
    "data = fetch_creditdefault(as_frame=True)\n",
    "X = data['data'].drop(columns=['SEX'])\n",
    "y = data['target']\n",
    "sensitive_feature = data['data']['SEX']\n",
    "# model\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a0d06dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "credo_model = cl.CredoModel(name='credit_default_classifier',\n",
    "                            model=model)\n",
    "\n",
    "credo_data = cl.CredoData(name='UCI-credit-default',\n",
    "                          X=X, \n",
    "                          y=y.astype(int),\n",
    "                          sensitive_features=sensitive_feature)\n",
    "alignment_manifest = {'metrics': ['precision_score']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d752c278-28aa-4cf2-bc1a-771a623d6b73",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Selecting Assessments\n",
    "\n",
    "Lens has a number of assessments available, each of which works with different kinds of models or datasets. By default, Lens will automatically run every assessment that has its prerequesites met. The prerequesities can be queried by calling the `get_requirements` function on an assessment. These indicate the set of features or functions your model and data must instantiate in order for the assessment to be run. See `credoai.assessments.credo_assessment.AssessmentRequirements` for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "896d9916-bae8-47ef-8433-0efb92dcd85f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_requirements': [('prob_fun', 'pred_fun')],\n",
       " 'data_requirements': ['X', 'y', 'sensitive_features']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from credoai.assessment.assessments import FairnessBaseAssessment\n",
    "assessment = FairnessBaseAssessment()\n",
    "assessment.get_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937022f9-0042-451c-a8a4-9a43bbf8fd98",
   "metadata": {},
   "source": [
    "But what if you don't want all of those assessments? No problem! Just pass the assessments you do want to Lens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c3d67db-c456-4578-9d99-765cdf7f26c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Remember to instantiate the assessment!\n",
    "selected_assessments = [FairnessBaseAssessment()]\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               assessments=selected_assessments,\n",
    "               manifest=alignment_manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9862658",
   "metadata": {},
   "source": [
    "## Parameterizing Assessments\n",
    "\n",
    "Now that we can select assessments, how about customizing them? There are two places where assessments can be customized: (1) when their underlying module is initialized and (2) when they are ran (which runs the underlying module).\n",
    "\n",
    "### Customizing initialization\n",
    "Below we can see how to customize the initialization of the assessment. In this case we want to run an additional metric that isn't part of the `alignment manifest`. The parameters that can be passed at this stage are the same parameters passed to the assessment's `init_module` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2df55e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Remember to instantiate the assessment!\n",
    "selected_assessments = [FairnessBaseAssessment()]\n",
    "init_kwargs = {'FairnessBase': {'additional_metrics': ['recall_score']}}\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               assessments=selected_assessments,\n",
    "               manifest=alignment_manifest,\n",
    "               init_assessment_kwargs=init_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bef27b3-16bc-428f-9a8f-8101765ad085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision_score</th>\n",
       "      <td>0.000543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_score</th>\n",
       "      <td>0.01676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    value\n",
       "precision_score  0.000543\n",
       "recall_score      0.01676"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens.run_assessments()['FairnessBase']['fairness']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db0c62c-7eed-448b-a2a7-effa71fcea55",
   "metadata": {},
   "source": [
    "### Customizing running assessments \n",
    "\n",
    "The other way of parameterizing the assessments is by passing arguments to the assessment's `run` function. These kwargs are passed to `lens.run_assessments`, which are, in turn passed to the assessment's initialized module.\n",
    "\n",
    "For instance, the `FairnessBase` assessment initializes `mod.FairnessBase`, whose `run` argument can take a `method` parameter which controls how fairness scores are calculated. The default is \"between_groups\", but we can change it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff377b0d-5e3f-43c1-8f94-f035bf472707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision_score</th>\n",
       "      <td>0.000302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_score</th>\n",
       "      <td>0.009504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    value\n",
       "precision_score  0.000302\n",
       "recall_score     0.009504"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_kwargs = {'FairnessBase': {'method': 'to_overall'}}\n",
    "lens.run_assessments(assessment_kwargs = run_kwargs)['FairnessBase']['fairness']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89471349",
   "metadata": {},
   "source": [
    "## Creating New Modules & Assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1654cf64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b79376ad",
   "metadata": {},
   "source": [
    "## Custom Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea76881",
   "metadata": {},
   "source": [
    "### incorporating confidence intervals\n",
    "\n",
    "Custom metrics can be incorporated by creating a `FairnessFunction`. The `FairnessFunction` is a lightweight wrapper class that defines a few characteristics of the custom function needed by the module. \n",
    "\n",
    "**Example: Confidence Intervals**\n",
    "\n",
    "Confidence intervals are not generically supported. However, they can be derived for metrics derived from a confusion matrix using the `wilson confidence interval`. A convenience function called `confusion_wilson` is supplied which returns an array: [lower, upper] bound for the metric. \n",
    "\n",
    "However, the module is not designed to accomodate metrics that return arrays. If you wish to incorporate CIs, we'd suggest creating a derived function that returns the lower bound, for example, and then wrapping it in a `FairnessFunction` as we mentioned for custom functions above. We show a case of that below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddf8645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.utils.metrics import confusion_wilson\n",
    "from credoai.assessment.model_assessments.fairness_base import FairnessFunction\n",
    "\n",
    "# define partial functions for the true positive rate lower bound\n",
    "def lower_bound_tpr(y_true, y_pred):\n",
    "    return confusion_wilson(y_true, y_pred, metric='tpr', confidence=0.95)[0]\n",
    "\n",
    "# and upper bound\n",
    "def upper_bound_tpr(y_true, y_pred):\n",
    "    return confusion_wilson(y_true, y_pred, metric='tpr', confidence=0.95)[1]\n",
    "\n",
    "# wrap the functions in fairness functions\n",
    "lower_metric = FairnessFunction(name = 'lower_bound_tpr', \n",
    "                          func = lower_bound_tpr, \n",
    "                          takes_sensitive_attributes = False, \n",
    "                          takes_prob = False)\n",
    "\n",
    "upper_metric = FairnessFunction(name = 'upper_bound_tpr', \n",
    "                          func = upper_bound_tpr, \n",
    "                          takes_sensitive_attributes = False, \n",
    "                          takes_prob = False)\n",
    "           \n",
    "# then proceed as normally\n",
    "custom_assessment = FairnessModule([lower_metric, \n",
    "                           'true_positive_rate', \n",
    "                           upper_metric],\n",
    "                A_str_test,\n",
    "                Y_test,\n",
    "                test_preds,\n",
    "                test_scores,\n",
    "                fairness_bounds\n",
    "               )\n",
    "\n",
    "custom_assessment.get_disaggregated_results(calculate_risk=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
