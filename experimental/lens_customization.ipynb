{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af84a3a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Customizing Lens\n",
    "\n",
    "Lens strives to give you sensible defaults and automatically do the proper assessments whenever possible. However, there are many times where you'll want to change, select, or extend the functionality. Lens is intended to be an _extensible framework_ that can accomodate your own analysis plan.\n",
    "\n",
    "In this document we will describe a number of ways you can customize Lens:\n",
    "* Setting up CredoModels\n",
    "* Selecting which assessments to run\n",
    "* Parameterizing assessments\n",
    "* Incorporating new metrics\n",
    "\n",
    "### Find the code\n",
    "This notebook can be found on [github](https://github.com/credo-ai/credoai_lens/blob/develop/docs/notebooks/lens_customization.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415b71ce-6b7a-4dbc-a10d-c30dcb256e18",
   "metadata": {},
   "source": [
    "### Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b3080e2-2ace-4267-b948-44c25ce9c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and df are defined by this script\n",
    "%run training_script.py\n",
    "import credoai.lens as cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e215660-dc70-4c56-b44e-bb27fba1ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model and data artifacts\n",
    "credo_model = cl.CredoModel(name='credit_default_classifier',\n",
    "                            model=model)\n",
    "\n",
    "credo_data = cl.CredoData(name='UCI-credit-default',\n",
    "                          data=df,\n",
    "                          sensitive_feature_keys=['SEX'],\n",
    "                          label_key='target'\n",
    "                          )\n",
    "\n",
    "# specify the metrics that will be used by the Fairness and Performance assessment\n",
    "metrics = ['precision_score', 'recall_score', 'equal_opportunity']\n",
    "assessment_plan = {'Fairness': {'metrics': metrics},\n",
    "                   'Performance': {'metrics': metrics}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aadea100-a24a-4475-9ae6-75c64a3465a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Lens imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d9dfb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import credoai.lens as cl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc258d5-bb22-452e-9cc8-c16408f7358f",
   "metadata": {},
   "source": [
    "## Setting up CredoModel\n",
    "\n",
    "The first step in using Lens is creating a CredoModel. Most kinds of models can be wrapped in a CredoModel for assessment. Let's see how to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811e3d51",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Selecting Assessments\n",
    "Lens has a number of assessments available, each of which works with different kinds of models or datasets. By default, Lens will automatically run every assessment that has its prerequesites met. However, you can instead specify a list of assessments and Lens will only use those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bfda748-45ca-4243-96bc-18b37afaa26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.assessment import *\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               assessment_plan=assessment_plan,\n",
    "               assessments=[FairnessAssessment, # <- new argument\n",
    "                            DatasetFairnessAssessment] \n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d681e1-fe0d-44e3-aca1-5b9ed0eb03a2",
   "metadata": {},
   "source": [
    "Even when you select assessments, Lens will only run the ones that work with your model and data. You are setting the assessments that Lens has access to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409b510e-42c6-4ff7-aeca-7eaccc0fcf3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parameterizing Assessments\n",
    "\n",
    "Now that we can select assessments, how about customizing them? There are two places where assessments can be customized: \n",
    "1. when their underlying module is initialized \n",
    "2. when they are ran (which runs the underlying module).\n",
    "\n",
    "### Customizing initialization\n",
    "To customize the module at initialization we use something we've already seen before - the `alignment spec`! The parameters that can be passed at this stage are the same parameters passed to the assessment's `init_module` function. \n",
    "\n",
    "The only one we've seen so far are the \"metrics\" argument that can be passed to the PerformanceAssessment and FairnessAssessment, but other assessments may be parameterized in different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a575093-f318-4c79-9b70-a50f54fd4343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0massessment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_sensitive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Initializes the performance module\n",
       "\n",
       "Parameters\n",
       "------------\n",
       "model : CredoModel, optional\n",
       "data : CredoData, optional\n",
       "metrics : List-like\n",
       "    list of metric names as string or list of Metrics (credoai.metrics.Metric).\n",
       "    Metric strings should in list returned by credoai.metrics.list_metrics.\n",
       "    Note for performance parity metrics like \n",
       "    \"false negative rate parity\" just list \"false negative rate\". Parity metrics\n",
       "    are calculated automatically if the performance metric is supplied\n",
       "ignore_sensitive : bool\n",
       "    Whether to ignore the sensitive_feature of CredoData (thus preventing calculation\n",
       "    of disaggregated performance). Generally used when Lens is also running \n",
       "    Fairness Assessment, which also calculates disaggregated performance.\n",
       "\n",
       "Example\n",
       "---------\n",
       "def build(self, ...):\n",
       "    y_pred = CredoModel.predict(CredoData.X)\n",
       "    y = CredoData.y\n",
       "    self.initialized_module = self.module(y_pred, y)\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Credo/credoai_lens/credoai/assessment/assessments.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assessment = PerformanceAssessment()\n",
    "assessment.init_module?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cc1a24-b8ce-4608-9942-9ce025622b32",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Customizing how assessments are run\n",
    "\n",
    "The other way of parameterizing the assessments is by passing arguments to the assessment's `run` function. These kwargs are passed to `lens.run_assessments`, which are, in turn passed to the assessment's initialized module.\n",
    "\n",
    "For instance, the `Fairness` assessment initializes `mod.Fairness`, whose `run` argument can take a `method` parameter which controls how fairness scores are calculated. The default is \"between_groups\", but we can change it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a920f75d-27da-4b2f-a0fc-03dc2c45c0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Running assessment-DatasetFairness\n",
      "INFO:absl:Running assessment-Fairness\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<credoai.lens.Lens at 0x29a46fd60>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_kwargs = {'Fairness': {'method': 'to_overall'}}\n",
    "lens.run_assessments(assessment_kwargs = run_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc23b24-96d2-44b7-b048-b3d6ca66c0e3",
   "metadata": {},
   "source": [
    "## Module Specific Customization - Custom Metrics for Fairness Base\n",
    "\n",
    "Each module has different parameterization options, as discused above. The FairnessModule takes a set of metrics to calculate on the model and data. Many metrics are supported out-of-the-box. These metrics can be referenced by string. However, custom metrics can be created as well. Doing so will allow you to calculate any metric that takes in a `y_true` and some kind of prediction\n",
    "\n",
    "Custom metrics can be incorporated by creating a `Metric` object. `Metrics` are lightweight wrapper classes that defines a few characteristics of the custom function needed by Lens. \n",
    "\n",
    "**Example: Confidence Intervals**\n",
    "\n",
    "We will create custom metrics that reflect the lower and upper 95th percentile confidence bound on the true positive rate.\n",
    "\n",
    "Confidence intervals are not generically supported. However, they can be derived for metrics derived from a confusion matrix using the `wilson confidence interval`. A convenience function called `confusion_wilson` is supplied which returns an array: [lower, upper] bound for the metric. \n",
    "\n",
    "Wrapping the confusion wilson function in a `Metric` allows us to pass it as a metric to the FairnessModule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d83ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.metrics.credoai_metrics import confusion_wilson\n",
    "from credoai.metrics import Metric\n",
    "\n",
    "# define partial functions for the true positive rate lower bound\n",
    "def lower_bound_tpr(y_true, y_pred):\n",
    "    return confusion_wilson(y_true, y_pred, metric='true_positive_rate', confidence=0.95)[0]\n",
    "\n",
    "# and upper bound\n",
    "def upper_bound_tpr(y_true, y_pred):\n",
    "    return confusion_wilson(y_true, y_pred, metric='true_positive_rate', confidence=0.95)[1]\n",
    "\n",
    "# wrap the functions in fairness functions\n",
    "lower_metric = Metric(name = 'lower_bound_tpr', \n",
    "                      metric_category = \"binary_classification\",\n",
    "                      fun = lower_bound_tpr)\n",
    "\n",
    "upper_metric = Metric(name = 'upper_bound_tpr', \n",
    "                      metric_category = \"binary_classification\",\n",
    "                      fun = upper_bound_tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a44df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Remember to instantiate the assessment!\n",
    "selected_assessments = [FairnessAssessment()]\n",
    "init_kwargs = deepcopy(alignment_spec)\n",
    "init_kwargs['Fairness']['metrics'] = [lower_metric, 'tpr', upper_metric]\n",
    "custom_lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               assessments=selected_assessments,\n",
    "               assessment_plan=init_kwargs)\n",
    "custom_lens.run_assessments().get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db56359b-b7dd-4756-9830-207ae460f617",
   "metadata": {},
   "source": [
    "## Understanding assessment requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26309500-48b4-46c5-abcd-ec30862c312b",
   "metadata": {},
   "source": [
    "The prerequesities can be queried by calling the `get_requirements` function on an assessment. These indicate the set of features or functions your model and data must instantiate in order for the assessment to be run. \n",
    "\n",
    "These requirements are specific for the model and the data. Each requirement is either a single requirements or a tuple. If a tuple, only one of the requirements within the tuple must be met. For instance, the FairnessAssessment needs *either* `predict_proba` OR `predict`. See `credoai.assessments.credo_assessment.AssessmentRequirements` for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7137eee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_requirements': [('predict_proba', 'predict')],\n",
       " 'data_requirements': ['X', 'y', 'sensitive_features']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from credoai.assessment import FairnessAssessment\n",
    "assessment = FairnessAssessment()\n",
    "assessment.get_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ad1643",
   "metadata": {},
   "source": [
    "You can also get the requirements for all assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3c803c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DatasetFairnessAssessment': {'model_requirements': [],\n",
       "  'data_requirements': ['X', 'y', 'sensitive_features']},\n",
       " 'FairnessAssessment': {'model_requirements': [('predict_proba', 'predict')],\n",
       "  'data_requirements': ['X', 'y', 'sensitive_features']},\n",
       " 'NLPEmbeddingBiasAssessment': {'model_requirements': ['embedding_fun'],\n",
       "  'data_requirements': []},\n",
       " 'NLPGeneratorAssessment': {'model_requirements': ['generator_fun'],\n",
       "  'data_requirements': []},\n",
       " 'PerformanceAssessment': {'model_requirements': [('predict_proba', 'predict')],\n",
       "  'data_requirements': ['X', 'y']}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from credoai.assessment import get_assessment_requirements\n",
    "get_assessment_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eb7213",
   "metadata": {},
   "source": [
    "## Creating New Modules & Assessments\n",
    "\n",
    "WIP section!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d7eb8a87bb83596f4cd5aeb66d856dad2a9bb65fe804cea051250e36746a46f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
